Title,Abstract,Discussion
Machine learning techniques for the diagnosis of Alzheimer's disease: A review,"Alzheimer’s disease is an incurable neurodegenerative disease primarily affecting the elderly population. Efficient automated techniques are needed for early diagnosis of Alzheimer’s. Many novel approaches are proposed by researchers for classification of Alzheimer’s disease. However, to develop more efficient learning techniques, better understanding of the work done on Alzheimer’s is needed. Here, we provide a review on 165 papers from 2005 to 2019, using various feature extraction and machine learning techniques. The machine learning techniques are surveyed under three main categories: support vector machine (SVM), artificial neural network (ANN), and deep learning (DL) and ensemble methods. We present a detailed review on these three approaches for Alzheimer’s with possible future directions.","In the classification of dementia related data, there are various categories or targets. One classification target is MCI vs AD, which is one of the most important targets for early diagnosis of AD. It can be observed in fig. 6(b), 7(b), and 9(b) that most of the work has been done in classification of CN vs AD and CN vs MCI. Moreover, classifications like MCI vs AD are very less. This needs to be addressed in future research for early detection of AD. Other categories like MCIc vs MCInc, and MCIc vs MCIs are also addressed in very few papers. Therefore, researchers can focus on these particular problems for early detection of dementia caused by Alzheimer’s disease. SVM (83%) TWSVM (3%) Ensemble (5%) Others (7%) LSTSVM (2%) (a) CN vs AD CN vs MCI MCI vs AD Within MCI Within AD Others 0 10 20 30 40 50 60 Number of classifications (b) Fig. 6. Plot showing usage of (a) different variants of SVM and (b) different target groups for Alzheimers. In AD diagnosis, an important focus point for research is the development of individual specific diagnosis models. For this, multimodal clinical data can be utilized as per the population. Moreover, novel learning techniques need to be developed for small datasets, since in real world scenarios the sample size from some population may not be large for training of model. Further, the data collection for Alzheimers includes noise from various sources. So, noise insensitive techniques must be applied for AD classification. In the next subsections, we suggest some future directions specific to the different machine learning approaches used for Alzheimers. 6.1 SVM As discussed in previous sections, different variants of SVM have been employed for classification of Alzheimers. The usage of different types of SVM in our survey is shown in fig. 6(a). One can notice that among the different variants of SVM, 83% of the papers used standard SVM. This shows the popularity and robustness of SVM in the classification of Manuscript submitted to ACM 18 M. Tanveer, et al. MRI data [112]. In 3% of the papers, TWSVM is used [5, 192], whereas LSTSVM [183] is used in only 1 paper. CSVM is used in [45, 158]. Some papers used ensemble of SVMs to classify Alzheimer’s data [7, 65]. However, it can be observed that only 7 % of the papers are in the others category. This category involves the algorithms based on SVM which are modified especially for Alzheimers. One can observe that very few variants of SVM have been applied for AD. This shows that research is needed in application of other variants of SVM for Alzheimers. Moreover, other than the existing models, some novel variants of SVM also need to be developed for Alzheimer’s disease as was done in [33]. In this paper, the spatial regularization on MRI image is included with SVM using graph Laplacian approach. Also, one can develop and use novel kernel functions for diagnosis of AD using SVM. Such kind of novel models can increase the classification performance of SVM. Plots for application of ANN on different (a) modalities (b) classification tasks (c) feature extraction methods, and (d) cross validation methods 6.2 ANN, TL and MKL techniques Techniques like transfer learning and multikernel learning can be used on multimodal data. We can see in fig. 7(a) that T1-weighted structural MRI is the most used modality. For feature extraction from MR images, atlas based methods have been mostly used by the researchers as shown in fig. 7(c). Atlas based methods employ some type of atlas to parcellate the brain regions from which different types of features are extracted. In future, other feature selection techniques can be utilized with ANN. For validating the model performance, 10-fold cross validation was the mostly used technique in fig. 7(d). However, one can also use methods like LOOCV, since it is the mostly used cross-validation method for SVM based algorithms in fig. 3(b). The usage of different architecture of ANNs are shown in fig. 8(a). Mostly ANNs with backpropagation were used. Fig. 8(b) shows different learning paradigms used by researchers. Most of the effort has been given on selection of better and more informative features. More informative features alone are insufficient for improving performance on classification of AD. Thus, more effort needs to be given on producing novel classifiers specifically designed for handling neuroimaging data. Also, more efforts need to be given on approaches like TL and MTL as they can aid in dealing with the problem of small sample size and high dimensionality. Pie charts showing usage of (a) variants of ANN and (b) different learning paradigms. It can be seen from fig. 7(a) that less efforts have been given on utilizing data from multiple modalities as compared to data from single modality. Hence, more efforts should be given on developing models that can leverage multimodal data. Ensemble (13.33%) LDA (6.67%) DL (35%) LR (10%) RF (11.67%) Others (23.33%) (a) CN vs AD CN vs MCI MCI vs AD Within MCI Others Group combinations 0 10 20 30 40 50 Number of classifications (b) Fig. 9. (a) Plot showing usage of different types of machine learning algorithms, and (b) targets used for classification of AD data. 6.3 DL and ensemble methods In most of the papers using DL and ensemble techniques, T1-weighted structural MRI is used for classification of AD as shown in Table 3. Most of the researchers used ROI based features to classify or diagnose AD. DL techniques based on CNN architectures perform better for classification of AD. Moreover, 10-fold cross-validation is mostly used for validation of learning algorithms as shown in fig. 5(b). The usage of DL, ensemble, and other classification techniques in our survey is shown in fig. 9(a). Among the different techniques, 35% of the papers used DL, 10% of the papers used LR, 11.67% of the papers used RF, 6.67% of papers used LDA, and 13.33% of papers used ensemble methods, and about 23.33% of the papers used other methods. The DL, ensemble and other techniques reviewed in our paper have issues related to model interpretability for routine use by clinicians. It can be observed in fig. 5(a) that most of the machine learning algorithms use MRI scans. Hence, more focus should be given on leveraging data from various other modalities. In future, DL models can be trained on large sized Alzheimers datasets for better classification performance"
A new machine learning method for identifying Alzheimer's disease,"Most of the studies on Alzheimer's disease (AD) have been carried out using medical images. However, the acquisition of medical images data is difficult. The identification based on the patient's speech data can effectively reduce the medical cost, and the speech data can be collected in a non-invasive manner so that the patient's data can be collected in real-time and accurately. This paper proposes a new method that uses the spectrogram features extracted from speech data to identify AD, which can help families to understand the disease development of patients in an earlier stage, so that they can take measures in advance to delay the disease development. We use the speech data collected from the elderly that express the speech features displayed in the speech and used the machine learning methods for identifying AD. During the simulation and experiment, we collect a new speech dataset, which includes Alzheimer's disease patients and healthy control subjects. Then, we compare with the speech data made available by the Dem@Care project. Among the tested models, LogisticgressionCV model exhibited the best performance. It is shown that this method using extracted spectrogram features from speech data to identify AD is feasible. The credibility of the new dataset and feasibility of the used methods in this paper are demonstrated.","From the experimental results, LogisticRegressionCV obtained the best performance compared with other classification methods. Among them, F1-Score reached 86.9% on the VBSD dataset and 89.4% on the Dem@Care dataset. LinearSVC and MLP models perform relatively well that their accuracy reached above 70%. However, DecisionTree and Bagging obtained poor performance because DecisionTree is so easily producing a complex model that makes the generalization ability perform not well. Moreover, it also has the disadvantage of instability. A small change of data may lead to different tree generation. In this simulation experiment, the training sets and testing sets are in constant change, and thus DecisionTree classifier performs poorly in this simulation experiment. The base classifier of Bagging chooses DecisionTree classifier, and the performance of Bagging mainly depends on the stability of the base classifier. As the result, the classification performance of Bagging is also poor. The differences in language and pronunciation between countries are not considered in combining the two datasets in simulation experiment.

Through the above simulation experiments, the best performing method is the LogisticRegressionCV method. In order to further optimize this method, different parameter values for PCA dimension reduction experiments are carried out to achieve higher identification performance. Fig. 9 shows the performance metrics with different dimensions of the respective 450-dim reduced dimension 20-dim. As it can be seen that when n_component = 300 (the number of feature dimensions), accuracy achieves the best identification accuracy, the results are an accuracy of 86.1%, precision of 87.5%, recall of 91.3%, and f1-score of 89.4%, respectively. The identification results are different when the dimensions are different. Therefore, when conducting experiments, the parameter adjustment is needed, which can optimize the method to get better results."
"Machine learning techniques for diagnosis of alzheimer disease, mild cognitive disorder, and other types of dementia","Alzheimer’s disease (AD) is one of the most common form of dementia which mostly affects elderly people. AD identification in early stages is a difficult task in medical practice and there is still no biomarker known to be precise in detection of AD in early stages. Also, AD is not a curable disease at this time and there is a high failure rate in clinical trials for AD drugs. Researchers are making efforts to find ways in early detection of AD to help in slowing down its progression. This paper reviews the state-of-the-art research on machine learning techniques used for detection and classification of AD with a focus on neuroimaging and primarily journal articles published since 2016. These techniques include Support Vector Machine, Random forest, Convolutional Neural Network, K-means, among others. This review suggests that there is no single best approach; however, deep learning techniques such as Convolutional Neural Networks appear to be promising for diagnosis of AD, especially considering that they can leverage transfer learning which overcomes the limitations of availability of a large number of medical images. Research is still on-going to provide an accurate and efficient approach for diagnosis and prediction of AD. In recent years, a number of new and powerful supervised machine learning and classification algorithms have been developed such as the Enhanced Probabilistic Neural Network, Neural Dynamic Classification algorithm, Dynamic Ensemble Learning Algorithm, and Finite Element Machine for fast learning. Applications of these algorithms for diagnosis of AD have yet to be explored.","This paper presented a state-of the-art review of studies that used machine learning (ML) techniques for diagnosis of AD since 2016. Different types of data and modalities are used in various studies such as neuroimaging, protein sequence, speech data, EEG and MEG signals, as well as other information such as medical history and genetics. With the advances made in computer vision, imaging technologies have received significant attention in recent decades and are extensively used in AD studies. Among the neuroimaging techniques, Magnetic Resonance Imaging (MRI) is more frequently used than other imaging technologies. MRI is a non-invasive and radiation free imaging modality which provides detailed information about the soft tissue and has been reported as a promising tool in detection of diseases including AD.

Several supervised and unsupervised machine learning techniques are discussed in this paper such as support vector machine, random forest, k-means, convolutional neural network, fuzzy c-means, density and spectral based clustering, and Bayesian techniques. In terms of classification, deep learning techniques, specifically convolutional neural networks, are most frequently used in AD studies, and they have been reported to be more accurate than other ML techniques. In order to have a network with high accuracy, a large dataset of medical images is required for training purpose. However, such a large dataset is always not feasible in medical applications. Thus, most of the AD studies leveraged transfer learning and used one of the pioneering CNN architectures pretrained on some of the available large datasets such as ImageNet, and then fine-tuned the network to adapt to the AD data. For medical imaging dataset, ADNI is the most frequently-used dataset. ADNI is a public dataset with the primary goal of testing whether MRI, positron emission tomography (PET), and other imaging modalities, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early AD. In this review, although some techniques have been reported to be more accurate than other techniques, due to lack of consistency in the implementation which means that techniques are not implemented with the same criteria such as number of data samples, modalities, preprocessing techniques, database, etc., one could not judge on the accuracy rate reliably. Comparison in accuracy would be more valid if the techniques were developed under the same conditions. Also, there is still no technique to be considered the best and most accurate for AD diagnosis; deep learning techniques such as convolutional neural networks appear to be promising in diagnosis of AD, especially considering that they can leverage transfer learning which overcomes the limitations of availability of a large number of medical images. Research is still on-going to provide an accurate and efficient approach for diagnosis and prediction of AD. In recent years, a number of new and powerful supervised machine learning and classification algorithms have been developed such as the Enhanced Probabilistic Neural Network[6], Neural Dynamic Classification algorithm[141], Dynamic Ensemble Learning Algorithm[7], and Finite Element Machine for fast learning[136]. Applications of these algorithms for diagnosis of AD have yet to be explored."
Hippocampal shape analysis of Alzheimer disease based on machine learning methods,"BACKGROUND AND PURPOSE: Alzheimer disease (AD) is a neurodegenerative disease characterized by progressive dementia. The hippocampus is particularly vulnerable to damage at the very earliest stages of AD. This article seeks to evaluate critical AD-associated regional changes in the hippocampus using machine learning methods.

MATERIALS AND METHODS: High-resolution MR images were acquired from 19 patients with AD and 20 age- and sex-matched healthy control subjects. Regional changes of bilateral hippocampi were characterized using computational anatomic mapping methods. A feature selection method for support vector machine and leave-1-out cross-validation was introduced to determine regional shape differences that minimized the error rate in the datasets.

RESULTS: Patients with AD showed significant deformations in the CA1 region of bilateral hippocampi, as well as the subiculum of the left hippocampus. There were also some changes in the CA2–4 subregions of the left hippocampus among patients with AD. Moreover, the left hippocampal surface showed greater variations than the right compared with those in healthy control subjects. The accuracies of leave-1-out cross-validation and 3-fold cross-validation experiments for assessing the reliability of these subregions were more than 80% in bilateral hippocampi.

CONCLUSION: Subtle and spatially complex deformation patterns of hippocampus between patients with AD and healthy control subjects can be detected by machine learning methods.","The main purpose of this study was to present an integrated method for identifying specific subregions of the hippocampus that were prone to structural changes, significant for discriminating between patients with AD and healthy control subjects, and to build effective classifiers based on the regional changes. This was accomplished by characterizing surface deformations using surface-based measures based on the parameterization of each hippocampus. A feature selection method based on SVM RFE and LOOCV was used to identify critical changes in the 2 subject groups, that is, patients with AD versus healthy control subjects. These changes were used to construct effective classifiers to assess the reliability of the selected features. Using this method, high classification accuracies (Table 2) were obtained for the left and right hippocampi using 2 cross-validation experiments. Subregions of the hippocampus where shape changes are prominent between patients with AD and healthy control subjects are highlighted in Fig 4. Our approach was generally applicable to shape-based analysis and classification of other brain structures.

Pathologic Implications of the Selected Features
As shown in Fig 4, the most significant deformations in patients with AD were located in the CA1 region of bilateral hippocampi, as well as in the subiculum of the left hippocampus. These results were consistent with previous neuropathologic findings.32–36 Given that AD is a progressive disease, AD-mediated neuronal impairments appear in a hierarchical formation.33 Hippocampal degeneration within the CA1 subfield and subiculum appeared to be more severe compared with other components of the hippocampal formation in the early stages of AD.33–36 The main pyramidal cell layers of the hippocampus are the CA1–4 regions (primarily CA1 and CA3) and the dentate gyrus. The perforant path is the major input to the hippocampus. The axons of the perforant path arise principally in layers II and III of the entorhinal cortex, with minor contributions from the deeper layers IV and V. Axons from layers II/IV project to the granule cells of the dentate gyrus and pyramidal cells of the CA3 region, whereas those from layers III/V project to the pyramidal cells of the CA1 and the subiculum. Pathologic findings37–40 in patients with AD suggested that severe degeneration of the perforant pathway was a characteristic feature of AD. Our results showing the significant changes in the hippocampal surface were a probable consequence of this phenomenon.

Reliability of These Selected Shape Features
We assessed the reliability of these selected shape features using the permutation test. For each cross-validation experiment, we randomly selected some features from all of the features to construct a classifier to evaluate the classification accuracy between patients with AD and healthy control subjects. This process was repeated 10,000 times. We observed that in each experiment the percentage that the classification accuracies were greater than our experimental results was less than 5%. In other words, all of the P values in our cross-validation experiments were less than .05. This result indicated that our findings of the between-group differences in the shape features of the hippocampus were statistically significant.

Comparison with Related Work
Methodology.
Recently, increasing attention has focused on characterizing AD-related changes in the hippocampus using computational approaches. One popular method was to analyze the gray matter concentration of the hippocampus using VBM. Busatto et al13 found gray matter abnormalities over the entire extension of the temporal lobe in AD. However, Frisoni et al14 found that more regional changes corresponded with all parts of the left and right hippocampi. A second method6–9 was to investigate the hippocampal volume changes in AD using the region of interest method. Good et al15 compared the VBM results with region of interest measurements of temporal lobe structures and found that region of interest analyses appeared more sensitive to volume loss in the amygdalae, whereas VBM analyses appeared more sensitive to right middle temporal gyrus and regional hippocampal volume loss in patients with AD. Similarly, Testa et al41 compared the accuracy of VBM and region of interest–based hippocampal volumetry and suggested that VBM was more accurate, but the combination of both methods provided the highest accuracy for detection of hippocampal atrophy in patients with AD. A third method used the surface modeling methods to map hippocampal shape abnormality in AD. Such a surface-based method17,19,20,42,43 allows us to detect more subtle changes in the hippocampus in comparison with the VBM and region of interest–based methods. Using a 3D parametric mesh model, Thompson et al17 found that the hippocampal atrophic rates were faster in patients with AD than in the control subjects. Csernansky et al19,20 used the high-dimensional brain mapping methods to detect an AD-specific pattern in the hippocampus that was not found through the volume methods. In this study, we used surface modeling to characterize the hippocampal shape changes in AD. Compared with previous surface modeling approaches, our method provided a more specific hippocampal shape modeling, because it was particularly designed to banana-like objects, which might reduce the complexity of shape modeling. Our results also suggested that patients with AD showed significant deformations in the CA1 of bilateral hippocampi, as well as the subiculum of the left hippocampus (Fig 4), which were consistent with previous structural neuroimaging studies.18,20 In addition, in the present study, we used machine learning methods to find the abnormal subregions of the hippocampus in AD. The classification methods over the univariate statistical methods applied in previous AD-related hippocampal shape studies17,18 are that the discriminative analysis based on the classifier function can detect subtle differences between populations. The result of the analysis is a classifier function that can be used for assigning new examples and making a map over the original features indicating the extent to which each feature participates in estimating the label for any given example.22 In this study, we did not provide direct comparisons on the sensitivity of these surface-based methods applied in the present study and previous studies, because they involved different parameter selections and statistical analysis approaches.

Results.
In our study, significant deformations in AD were located in the CA1 and subiculum of the left hippocampus and the CA1 of the right hippocampus. There were also some changes in the CA2–4 subregions of the left hippocampus in AD. These results were consistent with those found in the literature.18,20 Wang et al18 found that inward deformation of the hippocampal surface in the proximity of the CA1 subfield and subiculum can be used to distinguish subjects with very mild AD from nondemented subjects. Similarly, Csernansky et al20 suggested that inward deformation of the lateral zone of the left hippocampal surface was an early predictor of the onset of AD in nondemented elderly subjects. In addition, as shown in Table 2, the selected shape features of the left hippocampus in different experiments obtain higher classification accuracies than those of the right hippocampus. Our results clearly showed that in discriminating patients with AD from healthy control subjects, an assessment of the left hippocampus proved more effective than of the right. Previous studies have alluded to this differential atrophy between the left and right hippocampi.44,45 In these studies, it was reported that bilateral hippocampal atrophy occurred to a greater extent on the left side than on the right, as evidenced in patients with AD. Moreover, several VBM studies,16,46 sulcal-warping studies,47,48 and single-photon emission CT and positron-emission tomography studies49,50 supported a laterality trend of the atrophic process and left more than right hemispheric involvement in AD.

Limitations of the Present Method
Some limitations of the study need to be emphasized. First, the subjects with AD were not categorized clinically as early stage or advanced stage for this experiment. At this point, it should be emphasized that there is a need to detect AD as early as possible. In the near future, the methods presented in this article could potentially be applied to discriminate patients with MCI or mild AD from healthy control subjects. Second, because of the limited spatial resolution of MR imaging in our study, the segmented gray matter in the images may have contained some subcortical white matter, leading to potential contamination with respect to measurements of the hippocampus. A higher magnetic field scanner should be adopted to acquire higher spatial resolution images. Finally, a shape analysis approach for evaluating the hippocampus may not be regarded as the only tool necessary for discriminating between patients with AD and healthy control subjects. A combination of various quantitative MR techniques that measure the anatomic, biochemical, microstructural, functional, and blood flow changes may provide useful markers for early diagnosis of AD. In addition, Kantarci and Jack51 suggested that these approaches of directly imaging the pathologic substrate would need to undergo a validation process with longitudinal studies to prove their usefulness as surrogate markers in AD. In the future, our method will be validated by tracking the progression of patients with MCI. The combination of the valuable biomarkers and the results of our hippocampal shape analysis are expected to form a more effective computer-aided diagnostic tool for AD."
Machine learning for modeling the progression of Alzheimer disease dementia using clinical data: a systematic literature review,"Objective
Alzheimer disease (AD) is the most common cause of dementia, a syndrome characterized by cognitive impairment severe enough to interfere with activities of daily life. We aimed to conduct a systematic literature review (SLR) of studies that applied machine learning (ML) methods to clinical data derived from electronic health records in order to model risk for progression of AD dementia.

Materials and Methods
We searched for articles published between January 1, 2010, and May 31, 2020, in PubMed, Scopus, ScienceDirect, IEEE Explore Digital Library, Association for Computing Machinery Digital Library, and arXiv. We used predefined criteria to select relevant articles and summarized them according to key components of ML analysis such as data characteristics, computational algorithms, and research focus.

Results
There has been a considerable rise over the past 5 years in the number of research papers using ML-based analysis for AD dementia modeling. We reviewed 64 relevant articles in our SLR. The results suggest that majority of existing research has focused on predicting progression of AD dementia using publicly available datasets containing both neuroimaging and clinical data (neurobehavioral status exam scores, patient demographics, neuroimaging data, and laboratory test values).

Discussion
Identifying individuals at risk for progression of AD dementia could potentially help to personalize disease management to plan future care. Clinical data consisting of both structured data tables and clinical notes can be effectively used in ML-based approaches to model risk for AD dementia progression. Data sharing and reproducibility of results can enhance the impact, adaptation, and generalizability of this research.","AD is the most common cause of dementia, which is a syndrome of impaired memory and/or thinking that interferes with activities of daily life. We performed an SLR of studies applying ML to clinical data to identify factors that predict risk for progression of AD dementia. There has been an exponential increase in such paper applying ML for AD over the past 3 years. We reviewed the selected articles according to key components of ML analysis such as data characteristics, computational algorithms, and research focus; in the process we identified gaps in the existing literature and potential opportunities for future research. Our review suggests that most of the articles focus on predicting the progression of the disease based on standardized publicly available multimodal datasets which include both neuroimaging and some nonimaging clinical data. Most commonly used nonimaging clinical features for predictive modeling include neurobehavioral status exam scores, patient demographics, neuroimaging data, and laboratory test values.

Clinical databases which are collected for specific research purposes (eg, data in clinical registries) or cleaned and curated to enhance data reusability (eg, MIMIC database87) are often relatively well-structured, standardized, and clean, even though they may still have a few missing values and outliers. Hence, many researchers focus on utilizing these relatively clean datasets for their experiments and methodological innovations. However, as we noted previously, clinical data from local sources like institutional EHRs, which are primarily used to track patient care but can also be used secondarily for clinical research and automated disease surveillance, have great potential for use in modeling AD dementia progression.88 Data from such raw EHR data sources often have data quality issues and require significant effort for data preprocessing and feature engineering. However, they are a rich source of historical clinical data containing patient-level elements which can be effectively leveraged using ML-based computational techniques for longitudinal analyses of their preclinical phase to identify prognostic clinical phenotypes, thus representing an opportunity to employ precision medicine paradigms in disease states where the current evidence-base precludes such an approach.

The basic criteria for selecting articles for our review was inclusion of clinical data excluding imaging with/or without other features and/or data types. We observed that a significant portion of articles employed neuroimaging features from structural and functional MRI as well as fluorodeoxyglucose (FDG) and amyloid positron emission tomography (PET) for predictive modeling; this indicates that most researchers use clinical features as part of multidimensional datasets containing both clinical and imaging features. As evident from the relationship between the modality and accessibility of the datasets, multimodal features are mostly derived from the category of publicly accessible, standardized, and well-curated datasets.

Identifying individuals with early AD brain pathological changes could enable therapeutic interventions to delay the disease progression over time and can be helpful for tailoring disease management and planning future care. Multiple failed drug trials for AD dementia show that in the later stages of the disease course, when the patient already has significant neuronal degeneration, treatment is unlikely to be helpful.89 Hence, many drug trials are now enrolling patients with either preclinical AD (cognitively normal individuals with AD-related brain pathology) or very early AD dementia.90,91 Most studies reported that their proposed methodology can identify individuals at risk for progression to AD dementia approximately 24–48 months before the diagnosis of AD dementia.

Nearly all of the reviewed articles used supervised learning in the proposed models. Unsupervised or semi-supervised learning can also be an effective tool for handling multidimensional longitudinal patient data where clinical outcomes are not known a priori. Unsupervised clustering algorithms can be helpful for identifying novel subphenotypes with distinct disease trajectories and the associations between them.89,92

EHR-derived data for patients who are screened for risk of AD dementia not only include structured data in the form of labs, medications, and procedures, but also clinical notes, which are textual descriptions of physician–patient encounters and records of their follow-up visits.93 We observed from the summary statistics that information from clinical notes are not often included for developing the predictive modeling pipelines. However, these notes often consist of additional clinical information that are usually unavailable in the structured data sources, offering a rich source of information for clinical decision-making. Most of the notes are free-text narratives lacking a standardized structure and they cannot be processed by conventional ML algorithms like SVM, decision trees, regression, etc. NLP is a field of computational techniques that offers a viable solution for effectively processing clinical notes. In recent years, deep learning-based NLP models like recurrent neural networks and long short-term memory networks have been shown to outperform the conventional word-embedding-based NLP techniques for extracting relevant information from clinical notes.94

Data sharing and reproducibility of results can also enhance the impact, adaptation, and generalizability of research. Ideally, measures such as use of standardized publicly available EHR-derived datasets and specification of implementation details can help ensure reproducibility of the published methods and results. However, siloed data between different academic and corporate institutions and inconsistent data formats often make data sharing difficult and therefore remains an open area of research and innovation.95 A solution to this problem is developing a culture of data sharing among different institutions, potentially utilizing common data models like the OHDSI (Observational Health Data Sciences and Informatics) data sharing initiative. OHDSI produces tools like the OMOP (Observational Medical Outcomes Partnership) Common Data Model, which transforms data within disparate observational databases into a common representation (terminologies, vocabularies, and coding schemes) so that they can be analyzed using standardized analytics tools.96

Despite these limitations, there have been significant advancements in the application of ML on EHR-derived data for predicting AD dementia progression over the last 2–3 years. Advancements in technology and computational tools provide an opportunity for researchers to develop deep learning-based computational hypotheses that can inform clinical decision-making. Deep learning and other analytic approaches in ML can define clinical patterns and generate insights beyond human capabilities. This not only reduces the burden on clinicians in making their diagnoses but leads to improved quality, safety, and outcomes of care planning and delivery.18

Limitations of SLR
Many relevant research works might be published in only conferences and workshops proceedings and not indexed in bibliographic databases. Similarly, some studies from arXiv/BioarXiv were not included since they were not yet peer-reviewed. Thus, identification of only peer-reviewed studies from bibliographic databases might lead to selection bias during the initial inclusion stage. This can potentially impact the results presented as it may not truly represent the growing interest in the domain of using ML models for AD prediction using clinical data."
Classification of Alzheimer's Disease using Machine Learning Techniques,"Alzheimer's disease (AD) is a commonly known and widespread neurodegenerative disease which causes
cognitive impairment. Although in medicine and healthcare areas, it is one of the frequently studied diseases
of the nervous system despite that it has no cure or any way to slow or stop its progression. However, there
are different options (drug or non-drug options) that may help to treat symptoms of the AD at its different
stages to improve the patient’s quality of life. As the AD progresses with time, the patients at its different
stages need to be treated differently. For that purpose, the early detection and classification of the stages of
the AD can be very helpful for the treatment of symptoms of the disease. On the other hand, the use of
computing resources in healthcare departments is continuously increasing and it is becoming the norm to
record the patient’ data electronically that was traditionally recorded on paper-based forms. This yield
increased access to a large number of electronic health records (EHRs). Machine learning, and data mining
techniques can be applied to these EHRs to enhance the quality and productivity of medicine and healthcare
centers. In this paper, six different machine learning and data mining algorithms including k-nearest
neighbours (k-NN), decision tree (DT), rule induction, Naive Bayes, generalized linear model (GLM) and
deep learning algorithm are applied on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset in
order to classify the five different stages of the AD and to identify the most distinguishing attribute for each
stage of the AD among ADNI dataset. The results of the study revealed that the GLM can efficiently classify
the stages of the AD with an accuracy of 88.24% on the test dataset. The results also revealed these techniques
can be successfully used in medicine and healthcare for the early detection and diagnosis of the disease.","The results analysis indicates that the generalized
linear model outperforms other classifiers and gives
an accuracy of 88.24% during the test period.
Reasonable accuracies are obtained for deep learning
and Naive Bayes algorithms i.e. 78.32% and 74.65%
respectively. It can also be observed that the results
obtained on test data are quite close to the results
obtained from cross-validation period. This shows
that the developed models are not overfitted during
their training period. From the decision tree and rule
induction models, it has been observed that the most
distinguishing attribute for the five stages of the AD
is the CDRSB cognitive test, as it appears at the top
of the decision tree. The most distinguishing clinical
assessment attribute is the volume of the whole brain,
whereas the most distinguishing demographic
attribute is the age of the patient (see Appendices A
and B). The detailed results obtained by applying the
generalized linear algorithm during the test period is
illustrated in Table 7.
It can be observed that generalized linear model
correctly classified most of the unseen instances of
AD, CN, EMCI and LMCI classes, with the class
recall of 100.00%, 94.44%, 90.62%, and 86.01%
respectively, and class precision of 100.00%, 89.16%,
98.12%, and 79.45% respectively. The results of the
SMC class show misclassification of testing
instances. It can be observed from the above table that
25 out of 93 instances of SMC class have been
misclassified with the CN class. This is because the
patients belonging to SMC class have very similar
values of clinical assessment attributes as those who
belong to CN class. As well as, 28 out of 243
instances of the LMCI class have been misclassified
as EMCI. This is because the attribute values of
EMCI and LMCI classes overlap with each other."
Diagnosis of alzheimer's disease using machine learning,"Machine learning is being widely used in various medical fields. Advances in medical technologies have given access to better data for identifying symptoms of various diseases in early stages. Alzheimer's disease is chronic condition that leads to degeneration of brain cells leading at memory enervation. Patients with cognitive mental problems such as confusion and forgetfulness, also other symptoms including behavioral and psychological problems are further suggested having CT, MRI, PET, EEG, and other neuroimaging techniques. The aim of this paper is making use of machine learning algorithms to process this data obtained by neuroimaging technologies for detection of Alzheimer's in its primitive stage.","In order to detect Alzheimer’s Subjects and analyze images
Alzheimer Disease related regions of brain, we use MRI
images and process them to get numeric data which in turn is
processed using machine learning algorithms. Neural
Network and Random Forest have much better performs in
accuracy, then other used methods. The implementation of
this method will give instant and accurate results. Support
Vector Machine and Gradient boosting are also powerful
algorithms for classification problems and it works well with
the problem at hand. At a premature stage Alzheimer disease
can be detected and necessary treatment done at these early
states it will minimize the possibility of creating further
complications of Alzheimer disease patients."
A comparative analysis of machine learning algorithms to predict alzheimer's disease,"Alzheimer’s disease has been one of the major concerns recently. Around 45 million people are suffering from this disease. Alzheimer’s is a degenerative brain disease with an unspecified cause and pathogenesis which primarily affects older people. The main cause of Alzheimer’s disease is Dementia, which progressively damages the brain cells. People lost their thinking ability, reading ability, and many more from this disease. A machine learning system can reduce this problem by predicting the disease. The main aim is to recognize Dementia among various patients. This paper represents the result and analysis regarding detecting Dementia from various machine learning models. The Open Access Series of Imaging Studies (OASIS) dataset has been used for the development of the system. The dataset is small, but it has some significant values. The dataset has been analyzed and applied in several machine learning models. Support vector machine, logistic regression, decision tree, and random forest have been used for prediction. First, the system has been run without fine-tuning and then with fine-tuning. Comparing the results, it is found that the support vector machine provides the best results among the models. It has the best accuracy in detecting Dementia among numerous patients. The system is simple and can easily help people by detecting Dementia among them.","The main aim of the system is to predict Alzheimer’s disease. For predicting Alzheimer’s disease or Dementia in adult patients, the “MRI and Alzheimer’s” dataset has been used, which has been provided by the Open Access Series of Imaging Studies (OASIS) project. The dataset has been visualized and filled in the missing values. Data has been preprocessed by removing some unnecessary features. The values were standardized to make sure that they easily fit in the ML models. Then the dataset has been used to train SVM, logistic regression, decision tree, and random forest models. For evaluation metrics, accuracy, recall, AUC, and confusion matrix have been used. To improve the system result, the grid search method has been used to fine-tune all developed models. For this particular dataset, the system got the best result using SVM. A more complex model like the random forest classifier suffered from an overfitting issue. For deployment, the SVM model has been used for the best results among all the models. In the future, the system models could be improved by using a larger dataset and more ML models such as AdaBoost, KNN, Majority Voting, and Bagging. This will increase reliability and enhance the performance of the system. The ML system can help the general public get an idea about the possibility of Dementia in adult patients by simply inputting MRI data. Hopefully, it will help patients to get early treatment for Dementia and improve their life."
Machine learning for comprehensive forecasting of Alzheimer's Disease progression,"Most approaches to machine learning from electronic health data can only predict a single endpoint. The ability to simultaneously simulate dozens of patient characteristics is a crucial step towards personalized medicine for Alzheimer’s Disease. Here, we use an unsupervised machine learning model called a Conditional Restricted Boltzmann Machine (CRBM) to simulate detailed patient trajectories. We use data comprising 18-month trajectories of 44 clinical variables from 1909 patients with Mild Cognitive Impairment or Alzheimer’s Disease to train a model for personalized forecasting of disease progression. We simulate synthetic patient data including the evolution of each sub-component of cognitive exams, laboratory tests, and their associations with baseline clinical characteristics. Synthetic patient data generated by the CRBM accurately reflect the means, standard deviations, and correlations of each variable over time to the extent that synthetic data cannot be distinguished from actual data by a logistic regression. Moreover, our unsupervised model predicts changes in total ADAS-Cog scores with the same accuracy as specifically trained supervised models, additionally capturing the correlation structure in the components of ADAS-Cog, and identifies sub-components associated with word recall as predictive of progression.","The ability to simulate the stochastic disease progression of individual patients in high resolution could have a transformative impact on patient care by enabling personalized data-driven medicine. Each patient with a given diagnosis has unique risks and a unique response to therapy. Due to this heterogeneity, predictive models cannot currently make individual-level forecasts with a high degree of confidence. Therefore, it is critical that data-driven approaches to personalized medicine and clinical decision support provide estimates of variance in addition to expected outcomes.

Previous efforts for modeling disease progression in AD have focused on predicting changes in predefined outcomes such as the ADAS-Cog11 score or the probability of conversion from MCI to AD17,18,19,20,21,22,23,24,25,27,28,29,30,31. Here, we have demonstrated that an approach based on unsupervised machine learning can create stochastic simulations of entire patient trajectories that achieve the same level of performance on individual prediction tasks as specific models while also accurately capturing correlations between variables. Machine learning-based generative models provide much more information than specific models, thereby enabling a simultaneous and detailed assessment of different risks.

Our approach to modeling patient trajectories in AD overcomes many of the limitations of previous applications of machine learning to clinical data3,8,9,11. CRBMs can directly integrate multimodal data with both continuous and discrete variables, and time-dependent and static variables, within a single model. In addition, bidirectional models like CRBMs can easily handle missing observations in the training set by performing automated imputation during training. Combined, these factors dramatically reduce the amount of data preprocessing steps needed to train a generative model to produce synthetic clinical data. We found that a single time-lagged connection was sufficient for explaining temporal correlations in AD; additional connections may be required for diseases with more complex temporal evolution.

The utility of cognitive scores as a measure of disease activity for patients with AD has been called into question numerous times48. Here, we found that the components of the ADAS-Cog and MMSE scores were only weakly correlated with other clinical variables. One possible explanation is that the observed stochasticity may simply reflect heterogeneity in performance on the cognitive exam that cannot be predicted from any baseline measurements. However, we did find that some of the individual components of the baseline cognitive scores are predictive of progression. Specifically, patients with poor performance on word recall tests tend to progress more rapidly than other patients, even after controlling for the ADAS-Cog11 score.

There are a number of improvements to our dataset and methodology that are important steps for future research. Here, we limited ourselves to modeling 44 variables that are commonly measured in AD clinical trials. We excluded some interesting covariates such as Leukocyte populations because they were not measured in the majority of patients in our dataset constructed from the CAMD database. We also lack data from neuroimaging studies and tests for levels of amyloid-β. Incorporating additional data into our model development will be a crucial next step, especially as surrogate biomarkers become a standard part of clinical trials."
Alzheimer disease prediction using machine learning algorithms,"Alzheimer disease is the one amongst neurodegenerative disorders. Though the symptoms are benign initially, they become more severe over time. Alzheimer's disease is a prevalent sort of dementia. This disease is challenging one because there is no treatment for the disease. Diagnosis of the disease is done but that too at the later stage only. Thus if the disease is predicted earlier, the progression or the symptoms of the disease can be slow down. This paper uses machine learning algorithms to predict the Alzheimer disease using psychological parameters like age, number of visit, MMSE and education.","The dataset has various parameters but only the significant parameters that greatly help in predicting the disease like MMSE score, age, number of visits, and education of the patients were used. When the machine learning algorithms like Support Vector Machine, Decision Tree were used, they predict the disease with different accuracies. Each algorithm is trained with 70% training dataset and tested with 30% test dataset.

The conduct of the algorithms is compared based on their accuracy. Then the dataset is partitioned according to that ratio and when the algorithms are compared the best one is selected and can be used for next stage of prediction.

Machine learning approach to predict the Alzheimer disease using machine learning algorithms is successfully implemented and gives greater prediction accuracy results. The model predicts the disease in the patient and also distinguishes between the cognitive impairment.

The future work can be done by combining both brain MRI scans and the psychological parameters to predict the disease with higher accuracy using machine learning algorithms. When they are combined, the disease could be predicted with a higher accuracy in the earlier stage itself."
Improvement of machine learning models' performances based on ensemble learning for the detection of Alzheimer disease,"Failure to early detection of Alzheimer’s disease (AD) can lead memory deterioration. Therefore, early detection of AD is essential affecting the points of the brain that control vital functions. Various early AD detection approaches have been employed using machine learning. In literature, most of the early detection of AD approaches has been developed using single machine learning methods. Due to the importance of early detection of AD, the goal of this study is to improve the classification performance of the previous studies for early detection of AD applying ensemble learning methods including bagging, boosting and stacking. ADNI clinical dataset was used in this study with three target classes: Normal (CN), Mild Cognitive Impairment (MCI) and Alzheimer’s disease (AD). The proposed ensemble learning methods provided better classification performance compared to single machine learning methods. Besides, the best classification performance from the ensemble methods is obtained through the boosting (AdaBoost) ensemble (92.7%). This study revealed that the classification rate increased up to between 3.2% and 7.2% compared to single based machine learning approaches through the AdaBoost ensemble method.","Table IV presents the performance results of previous ensemble studies and the proposed ensemble model in terms of early detection of AD. Due to the using of clinical data in our study, the accuracy results of the previous approaches using the clinical data are presented in Table IV. In [10], a stacking ensemble model achieved to provide 83.33% accuracy using DT-kNN with logistic regression. Also, a boosting ensemble model provides 90.1% accuracy based on AdaBoost with J48 in [23]. On the other hand, a stacking (heterogeneous) ensemble feature selection approach proposed by [6]. RelifF, Gain Ratio, Chi-Square and FCBF feature selection approaches have been used to obtain an optimal future subset. Then, bagging ensemble (random forest) approach is used based on the optimal subset. 91% accuracy has been obtained through the proposed bagging ensemble. As it can be seen from Table IV, the proposed boosting ensemble approach provided 92.7% accuracy, and we believe that the AdaBoost ensemble method plays an important role in early detection of AD.Ideally, the stacking ensemble model should provide better performance compared to the others. The reason behind this is that stacking ensemble combined four different models with the logistic regression as a meta classifier. However, boosting ensemble model provided the best performance in our study. Thus, it can be inferred from this result is that stacking ensemble model does not always provide better performance compared to the bagging and boosting ensemble. Also, it should be noted that different stacking algorithms can be used to improve the performance of it in future (i.e. super learner etc.).
Even if the AdaBoost ensemble algorithm has the best performance compared to the bagging and boosting ensemble models, there are a few differences between them in terms of performances as it is seen in Fig. 3. In this case, these ensemble models can be used in different fields behind the health science including sport science to classify football players’ performances, agricultural science to detection of pathogen (salmonella or E. colt) in agricultural surface waters, and education science to detect and classify students etc."
Speech processing for early Alzheimer disease diagnosis: machine learning based approach,"Alzheimer's disease (AD) is a neurodegenerative disease characterized by the insidious onset of cognitive, emotional and language disorders. These attacks are sufficiently intense to affect the daily social and professional lives of patients. Today, in the absence of a reliable diagnosis and effective curative treatments, fighting this disease is becoming a real public health issue, prompting research to consider non-drug techniques. Among these techniques, speech processing is proving to be a relevant and innovative field of investigation. Several Machine Learning algorithms achieved promising results in distinguishing AD from healthy control subjects. Alternatively, many other factors such as feature extraction, the number of attributes for feature selection, used classifiers, may affect the prediction accuracy evaluation. To surmount these weaknesses, a model is suggested which include a feature extraction step followed by imperative attribute selection and classification is achieved using a machine learning classifiers. The current findings show that the proposed model can be strongly recommended for classifying Alzheimer's patient from healthy individuals with an accuracy of 79%.","
From the results of the classifiers presented in Table 4, it is clear that the performance for all classifiers was augmented by the feature selection methods. For the NN the global precision increased from 58% to 69%. In similar way, DT increased from 53% to 67%. Amelioration in the results was showed in the SVM too, by the overall precision increased from 60% to 79%. Nevertheless, SVM performed better than NN and DT in both cases (with and without the feature selection methods).

Interestingly the three ML classifiers demonstrated a better precision, using the top 11 features selected by the KNN method.

According to the output from our study, we could prove that using linguistic features extracted from verbal utterances of AD patients could be effective biomarkers of Alzheimer and the related dementia diseases.

However, compared to Orimaye [24], our study identifies more detailed and representative linguistic features. Also contrary to other study, we did not include the MMSE score and age, which considered as the most informative features that could improve the accuracy of ML classifier; our approach is properly based in speech samples only.

Eventhough, the use of the CHAT transcription format has been an available tool for analyzing speech data; it is still not universally used for speech transcription. Therefore, we consider that the use of CHAT symbols for the extraction of the lexical features could be a limitation for this work.

SECTION V.Conclusion
As the population continues to age, interest in accurate diagnosis of dementia is increasing.

Nowadays, clinical diagnosis may require much time from caregivers, patients, and medical personnel. These demands are expected to increase; and therefore, an early detection for AD would be a valuable tool.

This study applies a Machine learning models for the early diagnosis of Alzheimer's disease, based on linguistic features indicative of language impairment extracted from the verbal utterances of individuals affected and non-affected with AD.

We believe that the proposed model will help improving the prediction performance in detecting AD and cover the limitations discussed in the previous researches.

Other features will be discussed in our future research, such as the acoustical features (prosodic, temporal, emotion) that can be used to AD diagnosis and to emotion responses analysis. Other classifiers may be also used, to better evaluate our features and distinguish between AD and HC classes."
Magnetic resonance imaging biomarkers for the early diagnosis of Alzheimer's disease: a machine learning approach,"Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as to lessen the time and cost of clinical trials. Magnetic Resonance (MR)-related biomarkers have been recently identified by the use of machine learning methods for the in vivo differential diagnosis of AD. However, the vast majority of neuroimaging papers investigating this topic are focused on the difference between AD and patients with mild cognitive impairment (MCI), not considering the impact of MCI patients who will (MCIc) or not convert (MCInc) to AD. Morphological T1-weighted MRIs of 137 AD, 76 MCIc, 134 MCInc, and 162 healthy controls (CN) selected from the Alzheimer's disease neuroimaging initiative (ADNI) cohort, were used by an optimized machine learning algorithm. Voxels influencing the classification between these AD-related pre-clinical phases involved hippocampus, entorhinal cortex, basal ganglia, gyrus rectus, precuneus, and cerebellum, all critical regions known to be strongly involved in the pathophysiological mechanisms of AD. Classification accuracy was 76% AD vs. CN, 72% MCIc vs. CN, 66% MCIc vs. MCInc (nested 20-fold cross validation). Our data encourage the application of computer-based diagnosis in clinical practice of AD opening new prospective in the early management of AD patients.","The localization and spatial extent of the anatomical features identified in our study are in line with previous research showing the precedence of pathologic changes in the temporal and parietal cortex (Braak and Braak, 1991; Schroeter et al., 2009). In fact, a recent neuroimaging meta-analysis (Schroeter et al., 2009) aimed at characterizing the prototypical neural substrates of AD and its prodromal stage amnestic MCI reported the presence of:

(a) Reduction of glucose utilization and perfusion in the inferior parietal lobules and the posterior cingulate cortex and precuneus; hypometabolism was detected in the left anterior superior insula; whereas gray matter atrophy was found in the left temporal pole/anterior superior temporal sulcus, right amygdala, and gyrus rectus when 525 MCI patients were compared with 1097 healthy controls.

(b) Reductions in glucose utilization and perfusion coincided in the inferior parietal lobules, posterior superior temporal sulcus, precuneus, posterior cingulate cortex, anterior medial frontal cortex, anterior cingulate gyrus and right inferior temporal sulcus; hypometabolism was in the right frontal pole, left posterior middle frontal gyrus and left hippocampal head; whereas gray matter atrophy was found in the both amygdalae, both anterior hippocampal formations, entorhinal areas, medial thalamus, posterior insula, and left middle temporal gyrus/superior temporal sulcus when 826 AD patients were compared with 1097 healthy controls.

The only brain region revealed by our pattern recognition analysis not typically related to AD-like atrophy was the cerebellum. The cerebellum is a region generally rather neglected in AD research. Atrophy of this region has been sparsely reported in neuroimaging studies (Thomann et al., 2008a; Nigro et al., 2014), although there is considerable number of histo-pathological studies that demonstrated the presence of degenerative changes (Li et al., 1999; Wegiel et al., 2000; Wang et al., 2002). These alterations mainly comprise reduced Purkinje cell density, atrophy of the molecular and granular cell layer as well as a large number of amyloid plaques in the cerebellar cortex of AD compared to controls. Moreover the fact that we detected only anatomical changes in the posterior lobule of the cerebellum corroborated our findings, since cognitive performance in AD patients was found to be significantly correlated with volumes of posterior cerebellar lobes (Thomann et al., 2008b).

The development of computer-based automatic methods for the accurate classification of patients in early phase of AD from imaging data has attracted strong interest from the clinical community in the last few years, since its possible critical impact on clinical management and practice (i.e., identification of new biomarkers). Many of these classification methods are based on SVM, a set of algorithms that uses supervised learning of pattern recognition in a training set to build a classifier able to predict the category to which a new example belongs. One of the most important challenging in this field of study is to define automated methods to discriminate MCI patients progressing later to AD from patients who will not (Schroeter et al., 2009). For this reason, this study was aimed at assessing the powerful of machine learning methods in discriminating MCI at a risk state of AD.

In our work we used nested CV to measure the performance of our classifier. Nested CV avoids optimistically biased estimates of performance that may arise from the use of the same CV for parameter estimation and performance evaluation. Specifically, when model parameters are estimated by means of the performance evaluation criterion, then these estimates depend on (1) improvements in generalization performance and (2) statistical features of the particular dataset on which the performance are evaluated. This may result in under-estimates of the CV error. Moreover, in ordinary CV, parameter estimation is performed prior to model building, which could lead to an optimistic evaluation of the performance of the classifier. On the other side, in nested CV parameter estimation is performed simultaneously to performance evaluation (Cawley and Talbot, 2010).

Performances of our classification algorithm evaluated by nested 20-fold CV were 0.76 for AD vs. CN, 0.72 for MCIc vs. CN, and 0.66 for MCIc vs. MCInc. In their published study, Cuingnet et al. (2011) evaluated the performance of ten different machine learning methods (28 algorithm configurations) by using the same group of ADNI subjects employed in our work, splitting datasets in two equal sample groups and using one group to estimate the optimal value of hyperparameters and the other group to evaluate the performance of the classifier. Performances reached by our algorithm for the three classifications (AD vs. CN, MCIc vs. CN, and MCIc vs. MCInc) are better than 27/28 algorithm configurations, since 27 algorithms have a Balanced Accuracy lower than 0.66 for the MCIc vs. MCInc comparison.

The use of our classifier is limited to the early diagnosis of AD. Notwithstanding the vast majority of brain regions identified by our multivariate pattern recognition analysis have been also described to be involved in neurodegenerative processes underlying other dementia disorders (e.g., Fronto-Temporal Lobar Degeneration), machine learning has been also found accurate when applied to MR images for the differential diagnosis of AD (e.g., Klöppel et al., 2008). The clinical use of such a machine learning approach (early and differential diagnosis of AD) should require the training of a multicategory classifier (Beom Choi et al., 2014) on MR images from CN and different dementia patients (e.g., MCIc, MCInc, AD, FTD).

The main innovative result of our work was the extraction of MR-related biomarkers for the early diagnosis of AD by means of machine learning. We assessed the relevance of each brain voxel with respect to the classification analysis, thus allowing regions critically involved in the pathophysiological mechanisms of AD to be identified. Notably, the vast majority of brain regions allowing to perform the best discrimination between AD and CN, as well as between MCIc and CN, were the same regions allowing the discrimination between the two critical forms of MCI, i.e., MCIc and MCInc. In other words, the AD-like atrophy patterns characterized by combined pathological changes within the temporal cortex, hippocampus, entorhinal cortex, thalamus, insular cortex, anterior cingulate cortex, orbitofrontal cortex, and precuneus, allowed distinguishing clinically- and cognitively-matched MCI patients progressing to AD from those who will not.

In conclusion, we demonstrated that an advanced neuroimaging approach based on machine learning is able to accurately classify patients who will or will not develop AD by means of structural MRI data and to extract MR-related biomarkers of AD. Moreover, our advanced neuroimaging study allows us to perform a challenging reflection. Due to the similarity between AD-like atrophy patterns with those detected in MCI who will convert in AD, we can derive that the machine learning approach impacts on the sensitivity of AD-related features rather than specificity. This would suggest that the problem of how to perform diagnosis of AD at a very early stage by MRI seems to be a matter of increasing the MRI detectability of structural biomarkers. For this reason, both current generation MRI systems combined with advanced images processing algorithms and future generation MRI systems with improved sensitivity (e.g., increased MRI resolution and better S/N ratio) will –definitely– move MRI diagnostic role from clinical to pre-clinical stage of AD.
"
Alzheimer's disease risk assessment using large-scale machine learning methods,"The goal of this work is to introduce new metrics to assess risk of Alzheimer's disease (AD) which we call AD Pattern Similarity (AD-PS) scores. These metrics are the conditional probabilities modeled by large-scale regularized logistic regression. The AD-PS scores derived from structural MRI and cognitive test data were tested across different situations using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. The scores were computed across groups of participants stratified by cognitive status, age and functional status. Cox proportional hazards regression was used to evaluate associations with the distribution of conversion times from mild cognitive impairment to AD. The performances of classifiers developed using data from different types of brain tissue were systematically characterized across cognitive status groups. We also explored the performance of anatomical and cognitive-anatomical composite scores generated by combining the outputs of classifiers developed using different types of data. In addition, we provide the AD-PS scores performance relative to other metrics used in the field including the Spatial Pattern of Abnormalities for Recognition of Early AD (SPARE-AD) index and total hippocampal volume for the variables examined.","The main goal of this analysis was to introduce and test new metrics for assessment of AD risk. Similar metrics, such as the SPARE-AD and STAND score [3], [5], have been proposed previously to detect AD-like abnormalities using structural MRI data. Both are based on the use of SVMs combined with severe dimension reduction measures. Alternatively, our AD-PS scores are based on the solution of classification problems of very large size via the use of logistic regression with sparsity regularization. We used the conditional probabilities modeled by large-scale regularized logistic regression as metrics, describing the similarity of the anatomical patterns found in a given individual to those found in AD patients. Despite the high dimensionality of the voxel space, our approach is relatively fast. Furthermore, due to the elastic net regularization, it produces voxel-based and sparse discriminative maps indicating the brain areas more relevant to prediction. In addition, we have extended the approach to cognitive data. The AD-PS cognitive scores are composite scores that detect AD-like anomalies based on cognitive scores taken from several memory tests in ADNI. Previously, composite cognitive scores to assess AD risk have been proposed by others [7], [8], but using a different rationale. While we used the conditional probabilities generated by regularized logistic regression, they used RF and psychometric theory methods to generate composite scores. Our scores, instead of providing a measure of the cognitive function in a more classical sense, are similarity measures of the cognitive patterns found in a given individual to those found in AD patients. The AD-PS cognitive scores often did not capture significant differences across age groups within a given cognitive status, which could be a consequence of the cognitive information being used to generate the cognitive groups. In Table 4, AD-PS cognitive scores tended to be close to zero and close to one for CN and AD participants, respectively. However, we still found significant correlations of the ncMCI, cMCI and AD participants' AD-PS cognitive scores with their corresponding THV and SPARE-AD scores (see Tables S1–S2 in File S1) and very strong associations with time of MCI to AD conversion. AD-PS cognitive scores also indicated significantly greater risk of AD in ncMCI participants with more impaired functional status. We expect that longitudinal follow-up may provide a better reflection of age-related cognitive change. The approach is not limited in any way to the 25 cognitive outcome measures we selected here to illustrate the concept. We will incorporate other cognitive tests (e.g. executive function) available through ADNI in the future.
We systematically evaluated the relevance of different types of brain tissue when discriminating cognitive status groups in the voxel space. If we take as a reference recent work in the ADNI literature [19], [33], our results for GM compare very well to those reported for SVM methods. Our approach did less well when discriminating ncMCI from cMCI, but other sMRI-based methods did not do much better in the studies noted above. In fact, classification methods based on the voxel space in [19] performed worse than our RLR approach, but the ROI-based ones did better in terms of sensitivity and specificity. This is a situation of great clinical importance that very likely requires the inclusion of other types of data (e.g. PET, amyloid biomarkers, etc.) for better discrimination, combined with dimension reduction such as using ROI data [34] and principal component analysis [35] or much larger sample sizes for methods based on voxel space like ours. The comparisons we made to previous work must be interpreted with caution, because CV procedures and sample sizes were different from ours. Meaningful statements about relative performance only can be made when the methods are tested under the same conditions, as done by Cuingnet and colleagues [19]. Tables 8–9 contain information about relative performance of different methods in the literature together with details about sample size, CV technique and normalization method. These tables highlight the great variety of conditions based on which metrics of classifier performance were estimated. Table 10 reports results related to detection of differences between ncMCI and cMCI participants of the ADNI dataset using machine learning generated metrics and statistical testing. The results reported by Hinrichs and colleagues differ in terms of types of data and statistical testing, sample sizes, and time of conversion from MCI to AD, which makes their results difficult to compare directly to ours. Our approach generates discriminative maps at a voxel level which uncover brain regions that have been associated with AD before (e.g. hippocampus, parahippocampal gyrus, etc.). They are similar in interpretation to those generated by a linear SVM [18] but while those are dense, ours due to the elastic net regularization are sparse pinpointing brain regions relevant to classification. Exploratory voxel-wise analyses (not presented) showed that the blue areas correspond to brain regions with significant decreases of tissue volume (tissue atrophy). The interpretation of the red areas is more subtle. Two sample SPM t-tests in GM produced significant results mostly in areas located in the boundary of GM and CSF, which are known to be challenging for segmentation algorithms [36]. Vemuri et al. 2008 suggested that the presence of these areas is the result of noise in the data due to partial volume effects, segmentation and registration errors, etc. This issue requires further study.
There is a growing body of literature indicating increasing interest in the role of white matter in AD [37]–[41]. Several studies have identified volume loss in various portions of the corpus callosum as related to AD [42]. The callosal white matter loss has been related to Wallerian degeneration, receiving axons from the temporo-parietal regions involved in AD. In the field of imaging genetics, interest in WM is also growing. Several groups are beginning to report associations of apolipoprotein E and other genetic markers with WM tissue integrity and atrophy [43]–[45]. However, often machine learning studies in the literature have focused on the role of GM, whole brain or ROI, and the roles of WM and CSF have been less investigated. In our study, similar to previous reports, we found GM to be more discriminative when classifying CN versus AD subjects. However, relative performance of WM with respect to GM increased when the CN group was compared to a group with less severe cognitive status than AD. When discriminating MCI subjects with stable cognitive status from CN subjects, GM and WM classifiers' performance is similar. This suggests that WM could play a more important role in early stages of AD than previously thought. Interestingly, for CN participants the WM AD-PS scores were greater than their corresponding GM counterparts (see Table 4), a trend that still can be observed in ncMCI participants but not in cMCI or AD participants, whose AD-like anomalies are much greater in GM than in WM. Similar observations were made for CN subjects across age groups. WM AD-PS scores showed more significant differences between age groups than the corresponding GM AD-PS scores (see Table 6). Interestingly, our discriminative maps show adjacency of WM and GM patterns of atrophy, especially in the temporal lobe, a brain region believed to be affected early by AD. Thus, although AD has been traditionally thought to be predominantly a disease of GM tissue, our findings support previous reports that suggest a role of the WM in early stages of the disease [46], [47].
We proposed here metrics for AD risk assessments which integrate information from different sources by combining probabilities generated by classifiers. Similar ideas have been used previously in face and voice recognition based on Bayesian theory [48]. The probability hypercube concept that we have introduced can be interpreted as a geometrical representation of the output of a set of generative classifiers, each one estimated with different types of data. It is intuitive, it provides a natural environment to generate multimodal metrics for AD detection, and it can be a powerful paradigm to visualize information in a clinical AD database such as ADNI. Two- or three-dimensional graphics of the AD-PS scores can offer researchers and clinicians a quick intuitive understanding of how the participants are located according to given biomarkers indicating AD-like abnormalities, and also to locate groups of participants whose assigned cognitive status does not correspond to the estimated risk.
Here the AD-PS scores were estimated using cognitive and structural data independently. We generated combinations of the scores to seek a composite cognitive-anatomical metric for AD risk assessment with increased performance compared with metrics based on a specific source of information. While many different composite metrics could be devised, in this work we used the sum of the scores across sources of information to illustrate the concept. In some situations, our composite metrics improved detection of differences of distributions between clinical groups, such as ncMCI from cMCI participants, but very often it did not. This could suggest that collapsing all the multimodal information in a single score may not always be useful and/or the non-optimality of the composite metric used here.
The AD-PS and SPARE-AD scores very often detected more significant differences between groups than THV. Although in general the AD-PS scores often produced more significant results, in several situations they were outperformed by the SPARE-AD scores and THV. These relative results do not represent a rigorous comparison of these three metrics. For example, the AD-PS and SPARE-AD scores are estimated using different image processing approaches, sample sizes and smoothing kernels. On the other hand, the THV used here were based on FreeSurfer estimates; there are other estimators available that were not tested here that could be more accurate. The SPARE-AD and THV performances were instead provided as a reference to help assess and validate the AD-PS scores. Additional information about correlations of the three metrics across cognitive groups can be found in Tables S1 and S2 in File S1.
Our study has several limitations. A potential confounding factor here is the quality of the brain tissue segmentation. Although we made an effort to generate masks covering each type of tissue, there could be overlap among areas. We centered our analyses on the use of only sMRI and cognitive data because these were available for most ADNI-1 participants at baseline. In the future, we will estimate the AD-PS scores for amyloid PET imaging, amyloid and tau levels in CSF, etc. Our composite cognitive scores included only a portion of the cognitive data available in ADNI: memory scores. We chose these parameters because of their well-documented association with AD and their use in previous work by other researchers. We will include additional cognitive information in the future. High performance of cognitive data-based classifiers is in large part the result of these data being used to define cognitive groups in advance, which gives these classifiers an unfair advantage. Our sMRI AD-PS scores were based on images normalized using DARTEL; although this is a method easy to use and less time-consuming than other methods in the field, it may not be the best option. We expect significant improvements of AD-PS anatomical scores by using more sophisticated normalization methods. It is very likely the AD-PS scores will benefit from increasing the sample size, which could be implemented by integrating the ADNI databases to be available worldwide in the coming years [1]. The sum composite metric chosen here assigns similar weights to different modalities, which is very likely non-optimal. We will evaluate in the future different metrics defined within the probability hypercube. Also, we did not adjust for multiple comparisons in our analyses, but we often observed the expected trends in the values of the scores across clinical severity and groups of participants ordered by higher risk. To evaluate performance of the scores, we used the Kolmogorov-Smirnov two-sample test, which is only one of several possible choices. Censored ncMCI cognitive data were only considered in the survival analyses. If some of those censored ncMCI participants converted to AD within 36 months, other ncMCI versus cMCI discrimination results are very likely slightly worse than they should be. Finally, one of the regularization parameters was fixed empirically to avoid additional computations. A finer selection could lead to further improvements of the results presented here."
Detection of Alzheimer's disease by displacement field and machine learning,"Aim. Alzheimer’s disease (AD) is a chronic neurodegenerative disease. Recently, computer scientists have developed various methods for early detection based on computer vision and machine learning techniques.

Method. In this study, we proposed a novel AD detection method by displacement field (DF) estimation between a normal brain and an AD brain. The DF was treated as the AD-related features, reduced by principal component analysis (PCA), and finally fed into three classifiers: support vector machine (SVM), generalized eigenvalue proximal SVM (GEPSVM), and twin SVM (TSVM). The 10-fold cross validation repeated 50 times.

Results. The results showed the “DF + PCA + TSVM” achieved the accuracy of 92.75 ± 1.77, sensitivity of 90.56 ± 1.15, specificity of 93.37 ± 2.05, and precision of 79.61 ± 2.21. This result is better than or comparable with not only the other proposed two methods, but also ten state-of-the-art methods. Besides, our method discovers the AD is related to following brain regions disclosed in recent publications: Angular Gyrus, Anterior Cingulate, Cingulate Gyrus, Culmen, Cuneus, Fusiform Gyrus, Inferior Frontal Gyrus, Inferior Occipital Gyrus, Inferior Parietal Lobule, Inferior Semi-Lunar Lobule, Inferior Temporal Gyrus, Insula, Lateral Ventricle, Lingual Gyrus, Medial Frontal Gyrus, Middle Frontal Gyrus, Middle Occipital Gyrus, Middle Temporal Gyrus, Paracentral Lobule, Parahippocampal Gyrus, Postcentral Gyrus, Posterior Cingulate, Precentral Gyrus, Precuneus, Sub-Gyral, Superior Parietal Lobule, Superior Temporal Gyrus, Supramarginal Gyrus, and Uncus.

Conclusion. The displacement filed is effective in detection of AD and related brain-regions.","The results in Table 5 compare the proposed three classifiers (SVM, GEPSVM, and TSVM) with ten state-of-the-art methods. Plant’s results (Task 1 in Table 3 in Plant et al., 2010) presented the means together with 95% confidence intervals. Wang’s results (Table 7 in Wang et al., 2014) were obtained through a single K-fold cross validation analysis. Savio’s (Table 5 in Savio & Grana, 2013) and Dong’s results (Table 9 in Dong et al., 2015b) gave the means with SD.
Among the proposed methods, the proposed “DF + PCA + TSVM” yields the accuracy of 92.75 ± 1.77, sensitivity of 90.56 ± 1.15, specificity of 93.37 ± 2.05, and precision of 79.61 ± 2.21. Additional to it, the proposed “DF + PCA + GEPSVM” offers the accuracy of 91.52 ± 1.63, sensitivity of 88.93 ± 1.80, specificity of 92.27 ± 1.79, and precision of 76.66 ± 2.33. The proposed “DF + PCA + SVM” obtains the accuracy of 88.27 ± 1.89, sensitivity of 84.93 ± 1.21, specificity of 89.21 ± 1.63, and precision of 69.30 ± 1.91.
In terms of average accuracy, the proposed “DF + PCA + TSVM” result is as large as 92.75%, better than nine approaches of AD prediction, e.g., BRC + IG + SVM of 90.00% (Plant et al., 2010), BRC + IG + Bayes of 92.00% (Plant et al., 2010), BRC + IG + VFI of 78.00% (Plant et al., 2010), MGM + PEC + SVM of 92.07% (Savio & Grana, 2013), GEODAN + BD + SVM of 92.09% (Savio & Grana, 2013), US + SVD-PCA + SVM-DT of 90% (Wang et al., 2014), EB + WTT + SVM of 91.47% (Dong et al., 2015b), EB + WTT + RBF-KSVM of 86.71% (Dong et al., 2015b), and EB + WTT + POL-KSVM of 92.36% (Dong et al., 2015b). Nevertheless, the average accuracy of the method “DF + PCA + TSVM” is only less than one approach of “TJM + WTT + SVM” of 92.83% (Savio & Grana, 2013).
Why TSVM is better? There are two reasons. First, the non-parallel support vector machines provide more flexible and complicated hyperplanes than standard support vector machine. Second, the twin support vector machines formulate each of the two QP problems as a standard support vector machine, which makes it superior to generalized eigenvalue proximal support vector machine.
There were many other methods (Arbizu et al., 2013; Chaves et al., 2013; Cohen & Klunk, 2014; Dukart et al., 2013; Gray et al., 2012) proposed for detecting AD from NC, however, they dealt with images produced by other types of modalities: PET, SPECT, DTI, etc. Hence, it is inappropriate to compare the proposed methods with them. We will test our methods on SPECT and PET images in the future.
Notwithstanding, some regions reported to be associated with AD are not interpreted by displacement field. Those areas include caudate nucleus (Montagne et al., 2015), claustrum (Pirone et al., 2015), lentiform nucleus (Dong et al., 2015b), subcallosal gyrus (Dong et al., 2015b), and subthalamic nucleus (De Reuck et al., 2014). Why are those areas not detected by our DF method? The reasons are tri-fold: (1) We only preserved the displacement field with magnitude longer than 5. Reducing the value of 5 may include more potential regions and noises. A feasible solution is to reduce from 5 to 3 and meanwhile develop robust anti-noise method. (2) Some literature used advanced imaging modalities, such as MRSI and fMRI for metabolism detection and function analysis. Therefore, we may also include these imaging-modality techniques. (3) The key-slice selection procedure may miss important regions. Hence, we will try to reduce the slice separation, although it will increase the computation burden.
The advantages of displacement-field are two-fold. On one hand, it reaches excellent classification performance, which was comparable to latest approaches (Dong et al., 2015b; Plant et al., 2010; Savio & Grana, 2013; Wang et al., 2014). On the other hand, it can directly locate AD-related regions. The disadvantages of displacement field are: (i) The accuracy of displacement field estimation relies on the accuracy of rigid registration, which is a necessary pre-processing (see Fig. 3). (ii) The estimated DF may exist in the background (see Fig. 8A), which needs to be removed by masking technique."
Machine learning-based method for personalized and cost-effective detection of Alzheimer's disease,"Diagnosis of Alzheimer's disease (AD) is often difficult, especially early in the disease process at the stage of mild cognitive impairment (MCI). Yet, it is at this stage that treatment is most likely to be effective, so there would be great advantages in improving the diagnosis process. We describe and test a machine learning approach for personalized and cost-effective diagnosis of AD. It uses locally weighted learning to tailor a classifier model to each patient and computes the sequence of biomarkers most informative or cost-effective to diagnose patients. Using ADNI data, we classified AD versus controls and MCI patients who progressed to AD within a year, against those who did not. The approach performed similarly to considering all data at once, while significantly reducing the number (and cost) of the biomarkers needed to achieve a confident diagnosis for each patient. Thus, it may contribute to a personalized and effective detection of AD, and may prove useful in clinical settings.","We tested a machine learning approach for personalized and cost-effective detection of AD based on: 1) locally weighted learning [1], [15] and 2) a sequential selection of biomarkers to reduce their number or cost for confident diagnosis [4]. Two classification tasks were addressed: CN–AD and cMCI–nMCI. The approach is closer to the clinical setting, where not all biomarkers are available at once. It also considers which previous cases are more relevant for the patient.

The personalized classifiers tried to minimize the number or cost of the biomarkers included in the process. In both modes, the classifications were considerably more cost-effective than those based on all variables as there were important reductions in the diagnosis cost. Minimizing the number of biomarkers led to classifiers with fewer but more expensive features. Fig. 2 suggests that expensive, but perhaps more informative, biomarkers tended to be selected in the first iterations of the process that minimized the number of tests. On the other hand, the system optimizing the cost tended to select inexpensive biomarkers first and only if these were not conclusive were more expensive tests chosen. The overall classification performance was better when the system tried to minimize the number of biomarkers. This improvement came with relatively modest additional cost. Nonetheless, both strategies still need a more detailed cost-effectiveness analysis.

We also carried out a preliminary inspection of the evolution of the criteria for biomarker selection with the number of iterations (results not shown due to space constraints). The results suggested that both accuracy and AUC are appropriate metrics to select the features since the improvement in performance tended to decrease monotonically from the first to the last iterations.

Our results are comparable to those computed in other cross-validated studies. Schemes that used MRI, biochemistry, and PET data simultaneously reached accuracies of about 0.93 and 0.75 for the tasks CN–AD and nMCI–cMCI, in that order [5], [6]. Another study classified progression from MCI to AD over a period of two years using MRI, biochemistry, and cognitive scores simultaneously with accuracy of 0.67 and AUC of 0.80 [7]. Yet, the results cannot be directly compared due to differences in the datasets and biomarkers. As a proof of concept, we considered only one marker from the MRI and PET [10], [11], but the inclusion of more advanced brain imaging features may improve the performance.

The Pool of known cases is a key part of the method. Interim analyses (not shown due to space constraints) indicate that the classification performance decreases when smaller subsamples of the Pool are considered. Thus, it is essential to include a large enough number of subjects in it. More extensive tests are needed to determine how many cases should be included in the Pool. In any case, in clinical practice, the Pool should be populated with local data, which are more likely to reflect local life-style and environmental factors that might affect the disease.

According to current recommendations [13], the clinical diagnosis of AD and MCI should rely only on the patient's cognitive and behavioral symptoms. However, the biomarkers can increase or decrease the certainty that such symptoms are due to a pathological process of AD [8], [9], [13]. We conjecture that the personalized approach may also contribute to the diagnostic process, conveying supporting evidence that the patient's data match the profile of AD. This information is given as a probability, which summarizes the patient's data abnormalities, and shown to the clinician at every iteration, thus allowing him or her to monitor how the patient's data fit with known cases of the disease. The system assumes that not all biomarkers are available at once and it could be modified to account for other criteria than just cost when selecting the biomarkers (e.g., their level of invasiveness or risk of side effects). This could be done by deriving appropriate “modified costs” accounting for the relative effects of such factors (e.g., twice as much risk of side effects would lead to twice as much “modified cost”). The “modified costs” could also incorporate the clinician's prior expertise to guide the system towards specific biomarker combinations. We acknowledge that the classification performance of the system is not high enough to replace clinical diagnosis and that it is sometimes lower than that obtained considering all variables at once. However, this is not an inherent limitation of the method because its aim is to support, and not replace, the clinician, who must always make the final decision on clinical diagnosis.

Some limitations will be considered in future work. First, other classifiers [1] can be tested as base learners. Second, “modified cost” can be developed to account for additional factors in the selection of biomarkers. Finally, an independent validation set should be used to optimize α considering which values are clinically acceptable. Yet, the small number of ADNI subjects with all variables available limits our results and our ability to address such issues in this letter.

To sum up, the results are promising and might be used to support personalized diagnosis processes, while reducing the number or cost of the biomarkers needed for diagnosis. Future study is still needed but the framework presented in this letter could be readily extended to other biomarkers and diseases."
Classification of Alzheimer's disease and Parkinson's disease by using machine learning and neural network methods,"Data mining is a fast evolving technology, is being adopted in biomedical sciences and research. Data mining in medicine is an emerging field of high importance for providing prognosis and a deeper understanding of the classification of neurodegenerative diseases. Given a data set of consists of 487 patients records collected from ADRC, USA. Around eight hundred and ninety patients were recruited to ADRC and diagnosed for AD (65%) and PD (40%), according to the established criteria. In our study we concentrated particularly on the major risk factors which are responsible for Alzheimer's disease and Parkinson's disease. This paper proposes a new model for the classification of Alzheimer's disease and Parkinson's disease by considering the most influencing risk factors. The main focus was on the selection of most influencing risk factors for both AD and PD using various attribute evaluation scheme with ranker search method. Different models for the classification of AD and PD using various classification techniques such as Neural Networks (NN) and Machine Learning (ML) methods were also developed. It was found that some specific genetic factors, diabetes, age and smoking were the strongest risk factors for Alzheimer's disease. Similarly, for the classification of Parkinson's disease, the risk factors such as stroke, diabetes, genes and age were the vital factors.","In longitudinal analyses of 487 subjects we found that there was a strong association between the risk of AD and PD. The classification model was developed employing major risk factors such as age, diabetes mellitus, heart disease, hypertension, smoking, LDL, alcohol, BMI and genetical factors. We sought to identify those risk factors that are common for both the disease and then classifying the diseases based on those risk factors. There are some common risk factors in both AD and PD which play a vital role in the classification of AD and PD. The etiology of AD increases with the number of above risk factors. It was also found that different combinations of risk factors were associated with a high risk of AD. Age, genes, diabetes, smoking and stroke were the strongest risk factors for both the diseases. The role of stroke and Diabetes in Parkinson's disease seems more clear."
Alzheimer's Disease Early Detection Using Machine Learning Techniques,"Alzheimer's is the main reason for dementia, that affects frequently older adults. This disease is costly especially, in terms of treatment. In addition, Alzheimer's is one of the deaths causes in the old-age citizens. Early Alzheimer's detection helps medical staffs in this disease diagnosis, which will certainly decrease the risk of death. This made the early Alzheimer's disease detection a crucial problem in the healthcare industry. The objective of this research study is to introduce a computer-aided diagnosis system for Alzheimer's disease detection using machine learning techniques. We employed data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and the Open Access Series of Imaging Studies (OASIS) brain datasets. Common supervised machine learning techniques have been applied for automatic Alzheimer’s disease detection such as: logistic regression, support vector machine, random forest, linear discriminant analysis, etc. The best accuracy values provided by the machine learning classifiers are 99.43% and 99.10% given by respectively, logistic regression and support vector machine using ADNI dataset, whereas for the OASIS dataset, we obtained 84.33% and 83.92% given by respectively logistic regression and random forest.","This section presents and discusses the experimental results performed using the ADNI and OASIS datasets. We compare the results provided by the common machine learning models.
4.1 Experimental Results
We have performed several experiments with different parameters. In fact, after the pre-processing phase, the conversion of all the variables into numerical features and after keeping the pertinent features to be used by the machine learning models, we can now split the data into training and test sets. For this purpose, we used 5-fold cross-validation model. Finally, we evaluate the ability of using machine learning models in the Alzheimer’s disease detection using the accuracy, precision, recall, and F-measure metrics given by, respectively Eq. 1, Eq. 2, Eq. 3, and Eq. 4. The references and introduced labels in the evaluation are given in Table 4.
In Table 5, we compare the results given by the machine learning models using the ADNI dataset. As it is provided in this Table, the best values given by the machine learning models have been provided by the logistic regression and the support vector machine models with 99.43% and 99.10%, respectively. The lowest accuracy value has been given by the naïve bayes classifier with 87.07%.
The same learning-based models have been also used with the OASIS dataset. In Table 6, we compare the results provided by the selected machine learning models using OASIS dataset. As it is illustrated in this table, the best accuracy values have been provided by the logistic regression classifier and random forest with respectively, 84.33% and 83.92%. Whereas, the lowest accuracy value has been given by the naïve bayes classifier with 71.91%.
In this section, we compare the results given in this paper to those provided in the related work section. We selected the research studies that used the same ADNI database (cf., (Albright et al., 2019), (Alickovic & Subasi, 2020), and (Shahbaz et al., 2019)) or the OASIS dataset (cf., (Islam & Zhang, 2018)).
Table 7 provides the results of the comparison between our obtained results with the state-of-the-art models. As it is illustrated in this table, we obtained better results than the state-of-the-art models. For instance, compared to the results obtained by (Alickovic & Subasi, 2020), that used random forest, k-nearest neighbours, and support vector machine, we still achieve better performances for the k-nearest neighbours with 97.55% compared to 84.27%, for the support vector machine with 99.10% compared to 83.15%, and for the random forest classifier with 99.43% compared to 85.77%. Finally, we compared our obtained results with those given by (Shahbaz et al., 2019), that used random forest, k-nearest neighbours, and decision tree. We achieved better performance for the k-nearest neighbours with 97.55% compared to 43.26%, for the random forest with 98.89% compared to 69.69%, and for the decision tree with 97.53% compared to 74.22%.
"
Optimizing machine learning methods to improve predictive models of Alzheimer's disease,"Background:Predicting clinical course of cognitive decline can boost clinical trials’ power and improve our clinical decision-making. Machine learning (ML) algorithms are specifically designed for the purpose of prediction; however. identifying optimal features or algorithms is still a challenge. Objective:To investigate the accuracy of different ML methods and different features to classify cognitively normal (CN) individuals from Alzheimer’s disease (AD) and to predict longitudinal outcome in participants with mild cognitive impairment (MCI). Methods:A total of 1,329 participants from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) were included: 424 CN, 656 MCI, and 249 AD individuals. Four feature-sets at baseline (hippocampal volume and volume of 47 cortical and subcortical regions with and without demographics and APOE4) and six machine learning methods (decision trees, support vector machines, K-nearest neighbor, ensemble linear discriminant, boosted trees, and random forests) were used to classify participants with normal cognition from participants with AD. Subsequently the model with best classification performance was used for predicting clinical outcome of MCI participants. Results:Ensemble linear discriminant models using demographics and all volumetric magnetic resonance imaging measures as feature-set showed the best performance in classification of CN versus AD participants (accuracy = 92.8%, sensitivity = 95.8%, and specificity = 88.3%). Prediction accuracy of future conversion from MCI to AD for this ensemble linear discriminant at 6, 12, 24, 36, and 48 months was 63.8% (sensitivity = 74.4, specificity = 63.1), 68.9% (sensitivity = 75.9, specificity = 67.8), 74.9% (sensitivity = 71.5, specificity = 76.3), 75.3%, (sensitivity = 65.2, specificity = 79.7), and 77.0% (sensitivity = 59.6, specificity = 86.1), respectively. Conclusions:Machine learning models trained for classification of CN versus AD can improve our prediction ability of MCI conversion to AD.",
Early-stage Alzheimer's disease prediction using machine learning models,"Alzheimer's disease (AD) is the leading cause of dementia in older adults. There is currently a lot of interest in applying machine learning to find out metabolic diseases like Alzheimer's and Diabetes that affect a large population of people around the world. Their incidence rates are increasing at an alarming rate every year. In Alzheimer's disease, the brain is affected by neurodegenerative changes. As our aging population increases, more and more individuals, their families, and healthcare will experience diseases that affect memory and functioning. These effects will be profound on the social, financial, and economic fronts. In its early stages, Alzheimer's disease is hard to predict. A treatment given at an early stage of AD is more effective, and it causes fewer minor damage than a treatment done at a later stage. Several techniques such as Decision Tree, Random Forest, Support Vector Machine, Gradient Boosting, and Voting classifiers have been employed to identify the best parameters for Alzheimer's disease prediction. Predictions of Alzheimer's disease are based on Open Access Series of Imaging Studies (OASIS) data, and performance is measured with parameters like Precision, Recall, Accuracy, and F1-score for ML models. The proposed classification scheme can be used by clinicians to make diagnoses of these diseases. It is highly beneficial to lower annual mortality rates of Alzheimer's disease in early diagnosis with these ML algorithms. The proposed work shows better results with the best validation average accuracy of 83% on the test data of AD. This test accuracy score is significantly higher in comparison with existing works.","We evaluate various performance metrics like accuracy, precision, recall and F1 score. To determine the best parameters for each model, we perform 5-fold cross-validation: Decision Tree, SVM, Random Forests, XGBoost and Voting. Finally, we compare accuracy of each model. Several metrics and techniques were used to identify overfitting and parameter tuning issues after the models were developed. Performance evaluations can either be binary or multiclass and are described using the confusion matrix. A learning model was developed to distinguish true Alzheimer's disease affected people from a given population and a novel Machine Learning classifier was developed and validated to predict and separate true Alzheimer's disease affected people. The following evaluation measures were calculated using these components: precision, recall, accuracy, and F-score. Based on this study, recall (sensitivity) is the proportion of people accurately classified as having Alzheimer's. The precision of Alzheimer's diagnosis is the rate of people correctly classified as not having the disease. Alternatively, F1 represents the weighted average of recall and precision, while accuracy represents the proportion of people correctly classified. According to the results, the patient receives a report that tells him or her what stage of Alzheimer's Disease he or she is currently in. It is very important to detect the stages because the stages are based on the responses of the patients. In addition, knowing the stage helps doctors better understand how the Disease is affecting them. This research used these environments, tools, and libraries to conduct its experiments and analysis:
a) Environments Used: Python 3
b) Scikit-learn libraries for machine learning
The Figure 3 indicates that men are more likely than women to have dementia. Figure 4 that the non-demented group had much higher MMSE (Mini-Mental State Examination) scores than those with dementia.
The Figures 5A–C shows the analyzed value of ASF, eTIV and nWBV for Demented and Non-demented group of people. As indicated by the graph in Figure 5, the Non-demented group has a higher brain volume ratio than the Demented group. The reason for this is that the diseases influence the brain tissues causing them to shrink. Figure 6 shows the analyzed results of EDUC for Demented and Non-demented people.


Figure 7 shows the analysis on age attribute to find the percentage of people affected based on the demented and non-demented group. It is observed that a higher percentage of Demented patients are 70-80 years old than non-demented patients. It is likely that people with that kind of Disease have a low survival rate. Only a few people are over 90 years old.


From the above all analysis on the attributes, the following are the summary on intermediate results.
1. It is more likely for men to have demented, or Alzheimer's Disease, than for women.
2. In terms of years of education, demented patients were less educated.
3. Brain volume in non-demented groups is greater than in demented groups.
4. Among the demented group there is a higher concentration of 70-80-year-olds than in the non-demented patients.
Table 4 shows the performance comparison of accuracy, precision, recall, and F1 score for different ML models. The performance measures are defined as,
Accuracy: It is the measure of finding the proportion of correctly classified result from the total instances.
The most common metrics are the conversions of the True Positive (TP), the False Positive (FP), the True Negative (TN), and the False Negative (FN) metrics. Figures 8–13 shows the confusion matrix for Decision tree, Random Forest, SVM, XG boost, Soft, and Hard Voting classifier ML models.
A comparison of training and testing accuracy has been conducted for each model to eliminate overfitting. For each model, precision, recall, accuracy, and F1-score are shown in Table 3. Based on the analysis showed in the Table 3, the results approved that the best and ideal techniques, which have a good performance, are random forest, and XGBoost. The accuracy value of Voting classifier model is also closer to the random forest, and XGBoost models. All the experimental results (the average accuracy, precision, recall, and F measure of each model) were collected for extra analysis. The comparative analyses among all the Machine Learning models in terms of accuracy, precision, recall, and F1 score are presented graphically in Figures 14–17 respectively.


"
Machine learning and novel biomarkers for the diagnosis of Alzheimer's disease,"Background: Alzheimer’s disease (AD) is a complex and severe neurodegenerative disease that still lacks effective methods of diagnosis. The current diagnostic methods of AD rely on cognitive tests, imaging techniques and cerebrospinal fluid (CSF) levels of amyloid-β1-42 (Aβ42), total tau protein and hyperphosphorylated tau (p-tau). However, the available methods are expensive and relatively invasive. Artificial intelligence techniques like machine learning tools have being increasingly used in precision diagnosis. Methods: We conducted a meta-analysis to investigate the machine learning and novel biomarkers for the diagnosis of AD. Methods: We searched PubMed, the Cochrane Central Register of Controlled Trials, and the Cochrane Database of Systematic Reviews for reviews and trials that investigated the machine learning and novel biomarkers in diagnosis of AD. Results: In additional to Aβ and tau-related biomarkers, biomarkers according to other mechanisms of AD pathology have been investigated. Neuronal injury biomarker includes neurofiliament light (NFL). Biomarkers about synaptic dysfunction and/or loss includes neurogranin, BACE1, synaptotagmin, SNAP-25, GAP-43, synaptophysin. Biomarkers about neuroinflammation includes sTREM2, and YKL-40. Besides, d-glutamate is one of coagonists at the NMDARs. Several machine learning algorithms including support vector machine, logistic regression, random forest, and naïve Bayes) to build an optimal predictive model to distinguish patients with AD from healthy controls. Conclusions: Our results revealed machine learning with novel biomarkers and multiple variables may increase the sensitivity and specificity in diagnosis of AD. Rapid and cost-effective HPLC for biomarkers and machine learning algorithms may assist physicians in diagnosing AD in outpatient clinics.","AD is human diseases. In addition to biomarkers in brain, CSF or blood, multiple risk factors with several markers are under study. In addition to medications, multidomain interventions, targeting multiple risk factors simultaneously, could be effective dementia prevention strategies. However, multidomain interventions may be burdensome and not universally acceptable. Coley and his colleague [64] investigated adherence rates and predictors for all intervention components separately and simultaneously in the Finnish Geriatric Intervention Study to Prevent Cognitive Impairment and Disability (FINGER) and Multidomain Alzheimer Preventive Trial (MAPT). FINGER participants received a 2-year multidomain lifestyle intervention (physical training, cognitive training, nutritional counseling, and cardiovascular monitoring). MAPT participants received a 3-year multidomain lifestyle intervention (cognitive training, physical activity counseling, and nutritional counseling) with either an omega-3 supplement or placebo. Adherence decreased with increasing intervention complexity and intensity. It was highest for cardiovascular monitoring, nutritional counseling, and the omega-3 supplement, and lowest for unsupervised computer-based cognitive training. The most consistent baseline predictors of adherence were smoking and depressive symptoms. Reducing participant burden, maintaining in-person contacts, and taking into account participant characteristics may increase adherence in future trials with large sample size.
Increasing studies are investigating machine learning techniques with novel biomarkers as promising approaches to predict AD. Some studies have shown promising results. However, some lack large sample sizes and the appropriate power, or not hypothesis-driven. This review examined novel biomarkers including amyloid, tau protein, NMDAR-mediated biomarkers, and metabolites biomarkers. Because many machine learning models have no standard settings and guidelines, a robust comparison of these trials remains incomplete. However, brain image-based biomarkers with machine learning models especially deep learning such as CNN are promising. Moreover, machine learning combined with NMDAR-mediated biomarkers appear to be a new approach to predict the long-term cognitive outcome. Therefore, although the field of machine learning is relatively immature, such techniques, especially deep learning, warrant further study for their diagnostic and therapeutic implications on patients with AD."
Review on alzheimer disease detection methods: Automatic pipelines and machine learning techniques,"Alzheimer’s Disease (AD) is becoming increasingly prevalent across the globe, and various diagnostic and detection methods have been developed in recent years. Several techniques are available, including Automatic Pipeline Methods and Machine Learning Methods that utilize Biomarker Methods, Fusion, and Registration for multimodality, to pre-process medical scans. The use of automated pipelines and machine learning systems has proven beneficial in accurately identifying AD and its stages, with a success rate of over 95% for single and binary class classifications. However, there are still challenges in multi-class classification, such as distinguishing between AD and MCI, as well as sub-stages of MCI. The research also emphasizes the significance of using multi-modality approaches for effective validation in detecting AD and its stages.","Identification of structural differences in the brains of patients with neurological conditions versus healthy brains, neuroimaging is the most effective tool. The technique of analyzing biomarkers can provide both 2D and 3D structural information, which is particularly helpful in understanding the dimensions of cortical regions affected by AD. Biomarkers such as Diffusion Tensor Imaging (DTI), which measures the amount of water molecules and fibers in various areas of the brain at different stages of AD, are useful for assessing brain conditions. Other biomarkers like PET modality, CSF analysis, and genetic biomarkers can also be used to differentiate between healthy brains and those with AD with good precision. However, MRI and PET biomarkers are the most commonly used and have an acceptable level of accuracy in detecting AD. Classifications are usually done at either a binary or multi-class level, with binary class accuracy being greater than 95%, but multi-class accuracy being less than 85%. While the accuracy of accessing from AD to CN and MCI to CN is acceptable, going deeper into stages shows unacceptable accuracy at MCI, AD to MCI.
After the Analysis of biomarker level we organized the systematic review in Automatic pipelines for detection of AD and there stages. Various studies used automated pipelines to classify patients with AD from healthy controls (HC) or mild cognitive impairment (MCI). The studies used different datasets, software, and classification schemes. Most studies reported high accuracy in binary classification, but there is a need for improvement in multiclass classification. Several studies reported significant differences in brain regions or biomarkers between AD and HC/MCI. Some studies used non-conventional methods or proprietary software, which may limit reproducibility. Overall, the studies highlight the potential of automated pipelines for AD classification and biomarker discovery but also indicate the challenges and limitations of this approach. Automated pipeline approaches like Free Surfer and FSL (FMRIB SOFTWARE LIBRARY) are additionally popular for the detection of AD and its stages. Generally, these techniques are more effective when the data sample is small. In general, the AD dataset is not very large for a particular population. These strategies employ multiple preprocessing techniques that are arranged in a pipeline for classifying subjects. The three or four classes of classification or the subgroups of MCI, require more improvement than the other categories.
Various studies that have used fusion techniques to improve the accuracy of AD diagnosis. The review utilized different data sets, including ADNI and private data, and different modalities such as MRI, PET, CT, and SPECT. Some analysis used decision-level fusion, where the results of different classifiers were combined, while others used feature-level fusion, where different features from different modalities were combined. There were also review that used both feature-level and decision-level fusion. The results showed promising accuracy rates ranging from 80.9% to 98%, depending on the fusion technique used. The combination of multiple modalities or features can significantly improve the accuracy of diagnosis, as demonstrated by the studies in the table. As more advanced deep learning techniques continue to be developed, it is likely that further improvements in diagnosis accuracy will be achieved in the future. However, it is important to note that more research is needed to validate the results of these studies and ensure that they can be applied in clinical practice. The result of fusion and registrations validation are totally depend upon the ml and dl methods. The details study of ML and DL methods where made. Various analysis conducted on the application of machine learning techniques for the diagnosis of AD using different imaging modalities such as MRI, PET, SPECT, and fMRI. The review also use different methods such as ROI, VBM, SVM, CNN, PCA, and deep learning networks for feature extraction and classification. The accuracy of the classification varies between them, with some achieving high accuracy rates above 90%, while others have lower accuracy rates. The fusion of multiple modalities and the use of deep learning networks, such as CNN and RNN, have shown promising results in achieving higher accuracy rates. Most of the studies have also applied the classification models to both binary and multi-class classification tasks. This review highlight the potential of using machine learning techniques for the early and accurate diagnosis of AD."
Classification of Alzheimer's Disease Using RF Signals and Machine Learning,"Objectives: Alzheimer's disease is one of the most fastest growing and costly diseases in the world today. It affects the livelihood of not just patients, but those who take care of them, including care givers, nurses, and close family members. Current progression monitoring techniques are based on MRI and PET scans which are inconvenient for patients to use. In addition, more intelligent and efficient methods are needed to predict what the current stage of the disease is and strategies on how to slow down its progress over time. Technology or Method: In this paper, machine learning was used with S-parameter data obtained from 6 antennas that were placed around the head to noninvasively capture changes in the brain in the presence of Alzheimer's disease pathology. Measurements were conducted for 9 different human models that varied in head sizes. The data was processed in several machine learning algorithms. Each algorithm's prediction and accuracy score were generated, and the results were compared to determine which machine learning algorithm could be used to efficiently classify different stages of Alzheimer's disease. Results: Results from the study showed that overall, the logistic regression model had the best accuracy of 98.97% and efficiency in differentiating between 4 different stages of Alzheimer's disease. Clinical or Biological Impact: The results obtained here provide a transformative approach to clinics and monitoring systems where machine learning can be integrated with noninvasive microwave medical sensors and systems to intelligently predict the stage of Alzheimer's disease in the brain.","Table VIII shows the comparison of the LR model's accuracy in this paper along with the other RF-ML models that were used for microwave head imaging as described in [15]–​[17]. It can be seen that the LR model used in this paper has a high accuracy score, but a much lower number of observations compared to other studies. This is due to the limited number of voxel models available in CST Microwave Studio Suite. However, these voxel models can be easily modified such that they can represent more cases. As a result, we plan to take advantage of this in order to create more cases and observations. In comparison, the SVM and LDA methods reported in [16] had the best accuracy and large number of observations. However, the methods used in [16] were applied for stroke detection, which contains a larger difference in dielectric properties as compared to the differences found in AD. As a result, the captured S11 measurements would reflect changes in the tissues more easily, and therefore allow the ML models to be trained easily. All the models had fast learning and classification times that could train their models and generate results in under a minute."
Detection of subjects and brain regions related to Alzheimer's disease using 3D MRI scans based on eigenbrain and machine learning,"Purpose: Early diagnosis or detection of Alzheimer's disease (AD) from the normal elder control (NC) is very important. However, the computer-aided diagnosis (CAD) was not widely used, and the classification performance did not reach the standard of practical use. We proposed a novel CAD system for MR brain images based on eigenbrains and machine learning with two goals: accurate detection of both AD subjects and AD-related brain regions.

Method: First, we used maximum inter-class variance (ICV) to select key slices from 3D volumetric data. Second, we generated an eigenbrain set for each subject. Third, the most important eigenbrain (MIE) was obtained by Welch's t-test (WTT). Finally, kernel support-vector-machines with different kernels that were trained by particle swarm optimization, were used to make an accurate prediction of AD subjects. Coefficients of MIE with values higher than 0.98 quantile were highlighted to obtain the discriminant regions that distinguish AD from NC.

Results: The experiments showed that the proposed method can predict AD subjects with a competitive performance with existing methods, especially the accuracy of the polynomial kernel (92.36 ± 0.94) was better than the linear kernel of 91.47 ± 1.02 and the radial basis function (RBF) kernel of 86.71 ± 1.93. The proposed eigenbrain-based CAD system detected 30 AD-related brain regions (Anterior Cingulate, Caudate Nucleus, Cerebellum, Cingulate Gyrus, Claustrum, Inferior Frontal Gyrus, Inferior Parietal Lobule, Insula, Lateral Ventricle, Lentiform Nucleus, Lingual Gyrus, Medial Frontal Gyrus, Middle Frontal Gyrus, Middle Occipital Gyrus, Middle Temporal Gyrus, Paracentral Lobule, Parahippocampal Gyrus, Postcentral Gyrus, Posterial Cingulate, Precentral Gyrus, Precuneus, Subcallosal Gyrus, Sub-Gyral, Superior Frontal Gyrus, Superior Parietal Lobule, Superior Temporal Gyrus, Supramarginal Gyrus, Thalamus, Transverse Temporal Gyrus, and Uncus). The results were coherent with existing literatures.

Conclusion: The eigenbrain method was effective in AD subject prediction and discriminant brain-region detection in MRI scanning.","It is clearly observed in Table 6 that the selected coronal slices are significant in detecting AD from NC. In particular, the AD subjects show the cerebrospinal fluid (CSF) in the areas occupied by brain matter in the NC subjects. We conclude that 10× is reasonable because of following three reasons: (1) The 10× key-slice undersampling (i.e., select only one slice from 10 consecutive slices) yields a coarser brain while still capturing most tissues in the brain (Compare Table 6 with Figure 1). (2) It is very hard to define a fitness (optimization) function to find the optimal undersampling factor. (3) The classification system has a good accuracy in distinguishing AD from NC, and it detects correct AD-related brain regions (See Tables 9, 11). As there are spatial redundancy for neighboring coronal slices, the undersampling could reduce this redundancy to a rather small degree.

Overall, the eigenbrains in Table 7 capture both similarities and differences of structural features between AD and NC. The first eigenbrain capture the significant feature of AD from NC, and the second and following eigenbrains capture general brain structure. Revisiting the hippocampus part in the first eigenbrain of all key-slices, it is easily perceived that the body lateral ventricles area of AD are highlighted, which is indeed a distinct attribute between AD and NC. Our experiment extends the eigenbrain on SPECT images by Alvarez et al. (2009a) and Lopez et al. (2009) and shows that eigenbrain is also suitable for MRI scans.

The p-values in Table 8 show that the first eigenvalue λ1 are all less than 0.05 for all key-slices. It indicates that mean values of λ1 of AD and NC are significantly different. Hence, the most dominating eigenvalue characterizing AD and NC is the one corresponding to the first eigenbrain. For other eigenvalues, merely 1 of 10 p-values is less than 0.05, which indicates that those eigenbrains are not dominating features indicative of AD from NC. Therefore, the first eigenvalue is MIE and was selected.

Classification results in Table 9 compare the proposed three classifiers with state-of-the-art methods, in which Zhang's results (Table 7 in Zhang et al., 2014) are calculated through a single K-fold CV experiment. Plant's results (Task 1 in Table 3 Plant et al., 2010) offer the means together with 95% confidence intervals. Savio's results (Table 5 Savio and Grana, 2013) give the means with SD. For the proposed methods, it is unexpected that the POL-KSVM produces better classification accuracy of 92.36 ± 0.94 than linear SVM of 91.47 ± 1.02 and RBF-KSVM of 86.71 ± 1.93, because RBF was reported as the most widely used kernel. Our results are better than or comparable to other approaches to AD prediction from MR brain images of NC, e.g., US + SVD-PCA + SVM-DT of 90% (Zhang et al., 2014), BRC + IG + SVM of 90% (Plant et al., 2010), BRC + IG + Bayes of 92% (Plant et al., 2010), MGM + PEC + SVM of 92.07% (Savio and Grana, 2013), GEODAN + BD + SVM of 92.09% (Savio and Grana, 2013), and TJM + WTT + SVM of 92.83% (Savio and Grana, 2013). There were many other methods (Gray et al., 2012; Arbizu et al., 2013; Chaves et al., 2013; Dukart et al., 2013; Cohen and Klunk, 2014) proposed for detecting AD from NC, however, they treated images from other modalities (such as SPECT and PET). Therefore, it is not appropriate to compare the proposed methods with them. We will test our methods on SPECT and PET images in the future.

Table 11 shows that eigenbrains interpret the discriminant voxels involving the following regions reported in existing literatures: Anterior Cingulate (BA-24, BA-32) (Schultz et al., 2014), Caudate Nucleus (Head, body, and tail) (Möller et al., 2015), Cerebellum (Colloby et al., 2014), Cingulate Gyrus (BA-23, BA-24, BA-31) (Yu et al., 2014), Claustrum (De Reuck et al., 2014), Inferior Frontal Gyrus (BA-47) (Eliasova et al., 2014), Inferior Parietal Lobule (BA-40) (Wang et al., 2015), Insula (BA-13) (He et al., 2015), Lateral Ventricle (Voevodskaya et al., 2014), Lentiform Nucleus (Möller et al., 2015), Lingual gyrus (Lehmann et al., 2013), Medial Frontal Gyrus (BA-10, BA-11, BA-25, BA-6) (Kang et al., 2013), Middle Frontal Gyrus (BA-11) (Schultz et al., 2014), Middle Occipital Gyrus (Lehmann et al., 2013), Middle Temporal Gyrus (Aubry et al., 2015), Paracentral Lobule (BA-3, BA-4, BA-5, BA-6, BA-7) (Kang et al., 2013), Parahippocampal Gyrus (Amygdala, BA-28, BA-35, Hippocampus) (Eskildsen et al., 2015), Postcentral Gyrus (BA-5) (Kang et al., 2013), Posterior Cingulate (Shinohara et al., 2014), Precentral Gyrus (BA-4) (Kang et al., 2013), Precuneus (BA-7, BA-31) (Kang et al., 2013), Subcallosal Gyrus (BA-25, BA-34, BA-47) (Paakki et al., 2010), Sub-Gyral (BA-40, Corpus Callosum, Hippocampus) (Streitburger et al., 2012), Superior Frontal Gyrus (Chen et al., 2014), Superior Parietal Lobule (Quiroz et al., 2013), Superior Temporal Gyrus (BA-38) (Paakki et al., 2010), Supramarginal Gyrus (Quiroz et al., 2013), Thalamus (Medial Geniculum Body, Pulvinar, Ventral Lateral Nucleus) (He et al., 2015), Transverse Temporal Gyrus (BA-41) (Kim et al., 2012), and Uncus (BA-28) (Bangen et al., 2014).

Nevertheless, some regions reported to be associated with AD are not interpreted by Eigenbrain, such as subthalamic nucleus (De Reuck et al., 2014). The reason may lie in three aspects. First, the quantile of our method is assigned with a value of 0.98, which is considered high. Reducing the quantile value may include more regions. Second, some literature used other advanced imaging modalities, such as MRSI and fMRI for metabolism detection and function analysis. Third, the key-slice selection procedure may miss important regions.

From another point of view, Table 11 demonstrates the power of the eigenbrain. Our study uses only one feature (eigenbrain) on 10 key-slices of a simple 3D structural MR image, nevertheless, our findings cover 30 related regions reported by over twenty literatures, which used various feature extraction methods and advanced imaging technologies.

The contributions of the paper fall within the following five aspects: (i) We generalize the Eigenbrain to MR images, and prove its effectiveness; (ii) We propose a hybrid eigenbrain-based CAD system that can not only detect AD from NC, but also detect brain regions that related to AD. (iii) We prove the proposed method has a classification accuracy comparable to state-of-the-art methods, and the detected brain regions are in line with 16 existing literatures. (iv) We use ICV and WTT to reduce redundant data; (v) we find POL kernel is better than linear and RBF kernel for this study.

In conclusion, the advantages of eigenbrain are three-fold: (i) it reaches very high classification accuracy, which was better than or competitive with state-of-the-art methods (Plant et al., 2010; Savio and Grana, 2013; Zhang et al., 2014); (ii) it can directly find discriminant voxels/regions within the whole brain; (iii) it can be combined with other features, in order to increase the classification performance. On the other hand, the disadvantages of eigenbrain also exist: (i) it is essentially two-dimensional, which does not reduce the redundancy along the slice direction; (ii) it needs preprocessing of spatial registration, which costs large amount of computation resources.

To the policy-makers, this study suggests the eigenbrain technique can achieve comparable results to traditional methods. It may offer a ray of hope for AD diagnosis with unconventional means with the combination of eigenbrain and machine learning. This preclinical study suggests that hospitals and medical laboratories enroll more computer scientists and engineers, with the aim of developing efficient AD diagnosis and region detection systems.
"
Machine learning identifies candidates for drug repurposing in Alzheimer's disease,"Clinical trials of novel therapeutics for Alzheimer’s Disease (AD) have consumed a large amount of time and resources with largely negative results. Repurposing drugs already approved by the Food and Drug Administration (FDA) for another indication is a more rapid and less expensive option. We present DRIAD (Drug Repurposing In AD), a machine learning framework that quantifies potential associations between the pathology of AD severity (the Braak stage) and molecular mechanisms as encoded in lists of gene names. DRIAD is applied to lists of genes arising from perturbations in differentiated human neural cell cultures by 80 FDA-approved and clinically tested drugs, producing a ranked list of possible repurposing candidates. Top-scoring drugs are inspected for common trends among their targets. We propose that the DRIAD method can be used to nominate drugs that, after additional validation and identification of relevant pharmacodynamic biomarker(s), could be readily evaluated in a clinical trial.","In this paper, we described the development of DRIAD, a machine learning framework for evaluating potential relationships between a disease and any biological process that can be described by a list of genes. We used DRIAD to look for associations between the pathological stage of AD and genes that are differentially expressed when a small molecule drug is applied to a culture of terminally differentiated neuronal progenitor cells, which comprises a mix of neurons, astrocytes, and oligodendrocytes. DRIAD is distinct from the traditional approaches in which a model is constructed over the entire gene space and subsequently interrogated for feature importance scores and the enrichment of predefined gene sets, which then serve as a candidate list for further functional studies70. Traditional approaches are well-suited for predictors that exhibit high accuracy, because they establish a strong association between input features and the predicted phenotype. As predictor accuracy decreases, however, it becomes difficult to distinguish whether a high enrichment score is a true association between the corresponding mechanism and disease, or an artifact in a model that does not accurately predict the phenotype of interest (here, disease stage as defined by Braak score). DRIAD effectively decouples gene set enrichment and predictor performance by filtering the transcriptomic space for genes associated with drugs prior to model training and predictor evaluation. Pre-filtering to a limited set of features also addresses issues with overfitting that often arise when constructing computational models from disease databases where the number of cases is much less than the number of ‘omic features. Thus, DRIAD enables a direct, unbiased quantification of the association between the effects of a drug and AD progression.

The 80 compounds that we profiled in vitro were predominantly kinase inhibitors with anti-cancer activity since kinase inhibitors are the largest class of targeted drugs currently available, both as approved and pre-clinical compounds27, with extensive target information and diversity in chemical structure (Supplementary Fig. 5). An observation of possible significance is that there exists an inverse relationship between incidence of cancer and incidence of AD71,72. Among the 80 compounds tested, 33 were FDA-approved (Supplementary Data 3) and can potentially be used for repurposing directly. The remaining set consisted of 33 pre-clinical and 14 investigational compounds, which allowed us to explore a wider spectrum of mechanisms. Targets of pre-clinical and investigational compounds that were scored highly by DRIAD could potentially be used for selection of FDA-approved compounds in future screens or for the development of NMEs.

We ranked all compounds by how well their MOAs (as represented by a list of gene names) were able to predict disease severity based on gene expression in AMP-AD datasets. We found several drugs whose primary targets are JAK kinases to be among the top performers. We also explored connections between drug and their primary and secondary targets. This revealed that top-ranking drugs modulate pathways related to interferon signaling, autophagy, and microtubule formation and function. Kinases from JAK, ULK, and NEK families were found to be among the most consistent targets of top-scoring drugs. Future investigation will include experimental validation of these targets in cell-based and animal models using CRISPRa and/or CRISPRi, or other gene editing techniques, to evaluate whether a drug “hit” from DRIAD has an impact on AD-associated pathophysiology.

DRIAD has the potential to identify drugs that both mimic (or accelerate) disease and those that inhibit it. From the perspective of actual drug repurposing, only the later compounds are useful. In other studies, gene expression changes have demonstrated increased interferon signaling in AD73 and in ALS74 brains. Recently, we have found that cytoplasmic dsRNA, a known activator of Type I interferon, is present in ALS brains with TDP-43 pathology. Similarly, cytoplasmic dsRNA, which has been linked to increased Type I interferon signaling, was found to accumulate in glia in AD brains75. Activation of interferon signaling in this context promotes neuronal cell death. Thus, it seems probable that the inhibitors of JAK-STAT signaling identified in this study will potentially be useful in blocking neuroinflammation and cell death in the context of AD. Further studies to investigate the role of JAK-STAT signaling in aging and in AD brains are therefore warranted.

DRIAD allows for unbiased assessment of biological processes or drug candidates even when disease mechanisms are not explicitly known. This is valuable from a neuropathological perspective because it is increasingly clear that in addition to the classical AD hallmarks of amyloid plaques, neurofibrillary tangles, and neuronal loss, most patients with a clinical diagnosis of AD dementia have distinct patterns of co-occurring pathologies including TD-P43 inclusions, Lewy bodies, vascular changes, astrocyte and microglial activation, and probably other unrecognized alterations30,76,77,78,79. By working directly with the mRNA expression data from postmortem brain specimens and a priori knowledge of which genes encapsulate a proposed mechanism or co-expression module, or which genes are perturbed by a set of drugs, DRIAD can score mechanisms, co-expression modules or drugs without explicit knowledge of co-existing pathologies, such as the presence of Lewy Bodies or TDP-43 inclusions, in individual patients. Thus, DRIAD is capable of evaluating diverse hypotheses, including those associated with repurposing, without a high level of prior knowledge.

The AMP-AD Knowledge Portal is the most comprehensive database of gene expression profiles from AD brains, combining data from multiple large-scale studies. The gene expression profiles from autopsied brains are associated with Braak pathologic stage, making it possible to compare patients with no or mild AD symptoms at the time of death to patients who were demented. However, this anchoring on Braak staging also includes some cases in which symptoms did not correlate with pathological stage. A follow-on computational approach to deconvolve the correlative signals observed between top-performing drug signatures and AD expression profiles would help inform subsequent mechanistic studies. One direction for follow-up is to rerun the DRIAD pipeline on patient subgroups as defined by more detailed clinical or pathologic phenotypes, motivated by the notion of personalized treatment: different molecular pathways of AD arising in different patients will likely require different interventions to rescue neuronal death. Additional directions for follow-up include applying DRIAD to other studies that include age-matched individuals with no evidence of AD pathology as controls to build a predictor for the risk of AD pathology, and studies that include additional molecular modalities (e.g., mass spec proteomics, genome-wide association studies, etc.) and clinical co-variates (e.g., cognitive decline, comorbidities).

Our study has identified associations of gene perturbations by a subset of FDA-approved drugs and investigational compounds—largely kinase inhibitors—in human neural cells with gene perturbations in AD brain regions, but it has a number of limitations. We will extend this approach to small molecule drugs with a greater diversity of mechanisms of actions, e.g. GPRC inhibitors, as well as consider other types of drugs that target key AD genes. Our results require validation in relevant in vitro and in vivo AD model systems with amyloid plaques, neurofibrillary tangles, and neuronal death80 to examine impacts on key pathologic features or through emulated clinical trials in electronic health records81. Another limitation is knowing if drugs cross the blood-brain barrier (BBB), which is important for their use in brain diseases. These compounds were not developed to treat diseases of the brain, and we find inconsistencies in the methodology and animal models used across studies to assess BBB penetration for each compound. Theoretical approaches have been developed that aim to predict the penetration of drugs into the brain based on physical characteristics and chemical structure; however, the predictability is not generalizable across all compounds82. Therefore, the only way to know if a drug crosses the BBB is through empirical studies in patients with the disease of interest by quantifying free and total drug levels in cerebral spinal fluid by performing lumbar punctures in patients in a relevant age range who are already taking an FDA-approved drug of interest or as a pharmacokinetic outcome of a pilot clinical trial for compounds of interest with unknown BBB penetration."
A machine learning model to predict the onset of alzheimer disease using potential cerebrospinal fluid (csf) biomarkers,"Abstract—Clinical studies in the past have shown that the
pathology of Alzheimer’s disease (AD) initiates, 10 to 15 years
before the visible clinical symptoms of cognitive impairment
starts to appear in AD diagnosed patients. Therefore, early
diagnosis of the AD using potential early stage cerebrospinal
fluid (CSF) biomarkers will be valuable in designing a clinical
trial and proper care of AD patients. Therefore, the goal of our
study was to generate a classification model to predict earlier
stages of the AD using specific early-stage CSF biomarkers
obtained from a clinical Alzheimer dataset. The dataset was
segmented into variable sizes and classification models based on
three machine learning (ML) algorithms, such as Sequential
Minimal Optimization (SMO), Naïve Bayes (NB), and J48 were
generated. The efficacy of the models to accurately predict the
cognitive impairment status was evaluated and compared using
various model performance parameters available in Weka
software tool. The current findings show that J48 based
classification model can be effectively employed for classifying
cognitive impaired Alzheimer patient from normal healthy
individuals with an accuracy of 98.82%, area under curve (AUC)
value of 0.992 and sensitivity & specificity of 99.19% and
97.87%, respectively. The sample size (60% training and 40%
independent test data) showed significant improvement in T-test
with J48 algorithm when compared with other classifiers tested
on Alzheimer dataset.","As shown in Fig. 4 and 5 the gain or lift for J48 based
model tested on top 10%, 20% and 30% of the 40% test data
show a lift or gain of 3.5, 3.5 and 3.0, respectively. Similar
gains were obtained for SMO for top 10 and 20% of the test
instances. However, for SMO based model the lift values for
the remaining instances (i.e., 30% to 100%) of data were
comparatively lower than J48 based model. Moreover, the gain
or lift values for NB based model were far inferior when
compared to SMO and J48 base predictive model. These gain
or lift values obtained from J48 classification model show that
an enrichment of more than a fold of TP’s can be attained
using the J48 model as compared to any other random
screening protocols. Since in the present study, the J48 model
was found have better gain or lift (i.e., the ratio of TP obtained
with and without the model) values as compared to NB and
SMO based classification model. Therefore, the J48 classifier
based model is recommended as a reliable model to
discriminate and screen cognitively impaired individuals
from a given Alzheimer dataset.
The statistical significance of the J48 classifier based
classification model over SMO and NB classifier based model
was evaluated using paired sample t-test. The accuracy
obtained by J48 based classifier when tested on 20, 30 and 40
% independent test data was compared with SMO and NB
based model. The mean, standard deviation, standard error and
significance value obtained by comparing the accuracy results
of J48 & NB and J48 & SMO when tested on various test data
are tabulated in Tables II and III, respectively. The significance
value of 0.026 and 0.035 was obtained when the results of
20%, 30% and 40% independent test data of J48 was compared
with NB and SMO, respectively. The significance value
obtained show that the accuracy results obtained by the J48
based classification model over SMO and NB are statistically
significant as the generated significance values are lower than
0.05.
Even though, neuroimaging data are widely used to classify
subjects with early stages of the AD, the novelty in our
approach is to adequately apply low-cost CSF biomarker to
detect AD in its initial stages. Therefore the present study
provides a novel CSF biomarker-based classification tool to
efficiently classify a subject with an early stage of cognitive
impairment from healthy subjects with higher accuracy and
sensitivity."
"Harnessing the power of machine learning in dementia informatics research: Issues, opportunities, and challenges","Dementia is a chronic and degenerative
condition affecting millions globally. The care of patients
with dementia presents an ever-continuing challenge
to healthcare systems in the 21st century. Medical and
health sciences have generated unprecedented volumes
of data related to health and wellbeing for patients with
dementia due to advances in information technology, such
as genetics, neuroimaging, cognitive assessment, free
texts, routine electronic health records, etc. Making the
best use of these diverse and strategic resources will lead
to high-quality care of patients with dementia. As such,
machine learning becomes a crucial factor in achieving
this objective. The aim of this paper is to provide a stateof-the-art review of machine learning methods applied
to health informatics for dementia care. We collate and
review the existing scientific methodologies and identify
the relevant issues and challenges when faced with big
health data. Machine learning has demonstrated promising
applications to neuroimaging data analysis for dementia
care, while relatively less effort has been made to make use
of integrated heterogeneous data via advanced machine
learning approaches. We further indicate future potential
and research directions in applying advanced machine
learning, such as deep learning, to dementia informatics.
Index Terms—Alzheimer’s disease, cognitive assessment, deep learning, dementia, electronic medical records,
health informatics, machine learning, neuroimaging, NLP.","Whilst the general field of Big Data analytics continues to
mature, the current state of ML in dementia diagnosis remains
behind current state of the art methodologies. Nonetheless many
studies have proposed applications able to deliver promising dementia biomarkers or propose diagnostic procedures in collaboration with ML methods able to outperform current procedure.
Several avenues of research still remain relatively unexplored
in addition to advances in fields of ML opening previously unavailable avenues.
A. Potential Unexplored Machine Learning
Methodologies
Within the fields of NLP, computer vision, and time-series
based analysis, the incorporation of modern ML applications
have enabled the potential for complex and novel modellings of
various problem spaces.
1) Natural Language Processing: Current literature involving the application of NLP algorithms have shown promise
in the diagnosis or screening of SD through analysis of semantic fluency over continuous periods of SS. Being non-intrusive
and passive whilst maintaining reported performance metrics
comparable to current established screening practice, such applications show great potential as complementary tools in conjunction to current procedures within social or primary care
establishments.
NLP still remains in it’s infancy however, with current stateof-the-art being described as lagging behind the pace of other
growing and innovating technologies [87]. Cambria et al. [87]
justifies this viewpoint through their envisioned evolution of
NLP research indicating the three paradigms of current and future research direction. Of which, state-of-the-art NLP remains
within the first category of syntactic analysis, the use of keyword, punctuation and co-occurrence frequency in analysing
language. While the next major step in NLP technology is predicted to be the incorporation of word and phrase semantics and
sentics into MLP applications.
The advancement of NLP technologies provide far reaching
effects considering the continuing ubiquity of NLP applications
such as voice assistants within smart phones and household appliances. Current dementia diagnosis applications using NLP,
rely on bag of words or word2vec [88] methodologies for word
encoding, following the first paradigm of NLP research as mentioned previously. Incorporation of word and phrase semantic
encoding directly applies to applications analysing patient syntactic complexity and semantic fluency in spoken vocabulary.
The reduced semantic fluency of patients suffering from SD
can be directly attributed to semantic encoding complexity as
compared to those not suffering from SD.
Further research directions include time-series based analysis of sequential words within sentences. With word order
of appearance currently being largely ignored. Lopez lightly
touches upon this concept with his use of sliding window emotional temperature analysis [4], however the flattening of said
time-series features eliminates the potential for word order analysis. Such time-series based analysis techniques allow for the
incorporation of the changing conditions shown by dementia
subjects during the interview process. Several papers observe
a noticeable drain on energy and patience in dementia subjects
as interviews continue generally resulting in early termination
of interviews. Analysis of such features coupled with Lopez’s
emotional temperature and time-series based modelling provide
a possible novel approach to diagnosis.
2) Computer Vision: A major challenge currently in neuroimaging based diagnosis consists of image normalisation prior
to classification. The need for consistent features across various
MRI hardware and brain compositions, current feature encoding focuses greatly on regional GM densities, ROI dimensions,
or flattening of images into non-spatial vectors. Applications
such as CNNs which reference spatial relationships between
neighbouring pixels enable the potential identification of ROIs
simultaneously to classification training.
Common applications within the field of computer vision include the recently established methodology of transfer learning
Authorized licensed use limited to: The University of Toronto. Downloaded on December 01,2023 at 02:30:25 UTC from IEEE Xplore. Restrictions apply. 
TSANG et al.: HARNESSING THE POWER OF MACHINE LEARNING IN DEMENTIA INFORMATICS RESEARCH 125
TABLE VI
SEVERAL LARGE SCALE EHR AND DATA LINKAGE DATABASES
pre-trained large scale image classification models for use in
various alternative ML tasks using limited observation datasets
[89]. Fields of research within neuroimaging provide an excellent and novel area in which such methodologies can be
leveraged for improved ROI identification and even Alzheimer’s
diagnosis. Additionally, CNN medical image segmentation continues to advance rapidly such as the recently proposed 3D volumetric medical image segmentation application by Milletari
et al. [90] advancing segmentation of volumetric medical image
data, such as MRI, from the previous 2D segmentation slice and
merge methodologies to a simultaneous 3D segmentation whilst
improving processing time.
Such applications, however, are potentially limited by the
black box nature of NN methodologies. The interpretability of
such methodologies remain an ongoing research avenue within
various fields [91], health informatics being one such potential
field.
Mesh representation feature encoding of ROIs generate irregular domains such as in Electroencephalogram (EEG). As
such, the use of regular domain ML methodologies which respect regional features such as CNNs produce inefficient sparse
representations of said domain. Irregular domain ML applications such as graph based CNNs [92] would provide a better fit
by respecting the irregular domain interactions between nodes.
With such a heavy reliance on edge measurements of various ROI, improved segmentation applications are also open for
further research.
B. Health Data Linkage
A major prerequisite for any Big Data based complex modelling and applications is data availability. With the majority
of research currently relying on small patient groups with observations in the hundreds to occasional thousands, various organisations have devoted immense effort into the creation of
large-scale datasets appropriate for research. The ADNI dataset
for MRI scans and patient demographics was mentioned previously along with the CERAD database for neuropsychological
assessments. Several other databases exist, as shown in Table VI
, for generic EHR which provide extremely large, full featured
datasets of patient history. Opening a new avenue into dementia
prognosis based on the chronic and degenerative nature of dementia providing continual data on individuals. Coupled with
time-series modeling, EHR enable the exploration of dementia
prognosis.
EHRs however, provide multiple challenges which limit potential applications. The predominant challenge being the wideranging and non-specific patient information recorded in such
datasets. The resulting patient data presented are generally
sparse and highly dimensional, compounded by a lack of prior
knowledge in what constitutes as relevant data utilised in specific domains such as dementia diagnosis. The use of sparse,
high dimensional EHR data within health informatics presents
two major challenges: human interpretability, requiring the employment of sparse optimised feature selection, dimensionality
reduction, or representation learning for effective biomarker and
risk factor identification; and adequate data coverage producing
meaningless artefacts and bias termed sparse data bias, potential solutions of which exist [98].
In regards to EHR encoding, various avenues of research exist
which address this challenge: representation learning technologies remain a constantly evolving field [99] with which to adapt
into the field of EHR health informatics, whereas methodologies
from alternative ML fields with similar data structures allow for
potential adaptation into EHR encoding such as word representation methodologies within NLP, of which, methodology such
as word2vec by Mikolov et al. remains highly popular [88].
With no single de facto methodology for EHR encoding, there
remains great potential in the proposal of novel tailor-made encoding methodologies for EHRs.
Finally, relatively little research has focused on evaluation or
diagnosis across simultaneously multiple data types. With health
data linkage continuing to provide the possibility for full, structured and detailed records for individual care, the use of detailed
assessments such as MRI, EEG, and cognitive assessments can
be coupled with long term patient histories from EHRs allowing
for the creation of fully fledged and thorough diagnostic support
systems. While such work has been attempted using statistical
methods [2], and through ML methods [5], little research has
continued within such research avenue.
C. Prognosis
Within reviewed literature, the difficulty of classifying MCI
versus dementia patients remains a continual observation. With
reported evaluation accuracies indicating a consistent significant
drop in comparison to control versus MCI or full dementia
classification. The use of neuroimaging, cognitive assessment
and discourse analysis have been unable to classify continued
degeneration effectively whilst other approaches such as EHR
remain unexplored.
In retrospect, little research has also gone into actual prediction of MCI and dementia conversion based on historical
patient history. Such research applications would provide great
potential into identification of risk factors and biomarkers indicating rates of cognitive decline. Several clinical studies have
attempted equating cognitive decline to cognitive assessment
scores statistically [100] however, the use of modern ML techniques may provide novel indications.
Continued cognitive decline provides a sequential time-line
of discrete events indicating the gradual worsening of dementia symptoms, such time-series based data serves as a perfect
Authorized licensed use limited to: The University of Toronto. Downloaded on December 01,2023 at 02:30:25 UTC from IEEE Xplore. Restrictions apply. 
126 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. 13, 2020
example for modelling on time-series based methodologies. As
mentioned in Section IV-G, RNNs allow for short term memory
of past events hence, ideal for applications modelling dementia
conversion. Such applications can potentially provide improved
predictions of cognitive decline allowing for personalized tailored medical care or identify future at-risk individuals for close
monitoring. However, DL technologies such as RNN remain a
concern in human interpretability and validation. Consequently,
following on from the example of at-risk identification, indications to the reasoning behind predicting an individual as at-risk
remain unknown. Such issues, remain an ongoing research challenge.
The degenerative nature of dementia results in an increase in
comorbidities [24], institutionalisation [25], and fall rate [26].
Several studies have proven the statistical significance of dementia as a risk factor to hospitalisation [24] and [101]–[103].
There remains untapped potential in prediction of institutionalization risk and hospitalisation outcomes for dementia patients
using ML applications.
D. Deep Learning
A highly advanced and influential field of ML, DL architectures such as deep NNs, RNNs and deep belief networks
are at the heart of various high profile applications such as
IBM Watson [63] and Google Translate [64] within the field of
NLP or AlphaGo beating the then current human European Go
champion [65]. In computer vision, deep CNNs revolutionised
image classification during the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with SuperVision outperforming other entries by a margin of 11% image classification error [6]. Subsequent challenges have thus been dominated
by various adaptations of deep CNNs [104]. DL has seen use
within medical fields such as drug discovery [105], patient categorisation [106], imaging [107], biomedical text mining [108]
and EHRs [109]. With DL proving capability in representing
complex functions within other fields, the merger of such DL
architectures with Dementia healthcare applications have potential for novel approaches to current research.
There is, however, criticism on various aspects of DL methodologies which limit widespread use within fields such as
medicine. Criticisms which originate from the general NN
model that architectures such as CNNs, RNNs and deep belief networks stem from. Issues of transparency or the “black
box” nature of models limit validation of trained models to
purely empirical evidence formed from unseen test data. As a
result, the reliance on a model to correctly represent relevant
aspects of the dataset are not always guaranteed as shown by
Ribeiro et al. [110]. However, the limiting of such potential
issues of dataset bias, indicated by Ribeiro, remain a primary
concern within health informatics with great emphasis on minimising confounding bias in population demographics made on
the majority of reviewed literature.
While attempts have been made to improve acceptance of
ML among medical experts through the integration of existing
medical knowledge into RF models for diagnosis [111], there
remains potential for adapting DL architectures for improved
acceptance within medicine. With acceptance, comes the possibility of adapting the proven capabilities of DL within other
fields into the mainstream of health informatics."
Studying depression using imaging and machine learning methods,"Depression is a complex clinical entity that can pose challenges for clinicians regarding both accurate diagnosis and effective timely treatment. These challenges have prompted the development of multiple machine learning methods to help improve the management of this disease. These methods utilize anatomical and physiological data acquired from neuroimaging to create models that can identify depressed patients vs. non-depressed patients and predict treatment outcomes. This article (1) presents a background on depression, imaging, and machine learning methodologies; (2) reviews methodologies of past studies that have used imaging and machine learning to study depression; and (3) suggests directions for future depression-related studies.","
To the best of our knowledge the number of articles that use machine learning methods for studying depression is limited. Additionally, at a quick glance of the past studies presented in Table 1, Table 2, the methods used across these articles vary enough to make it difficult to draw comparisons simply based on the results. Thus, in this section we attempt to evaluate the methods used by the past studies.

3.1. Sample size
A common limitation across all past studies is sample size. The sample size used by past studies is small compared to what is optimal for machine learning methods to minimize the variance in assessments of accuracy, sensitivity, and specificity. This is especially true for the studies attempting to predict depression treatment response. Given the difficulty of recruiting depression patients for treatment protocols within a geographical area, the limitation of a small sample size is quite understandable. With respect to fMRI research, multiple efforts to create repositories to which individual groups can contribute data have attempted to address the broader issue of small sample sizes within neuroimaging research. These include the 1000 Functional Connectomes Project, the International Neuroimaging Data-Sharing Initiative, and the OpenfMRI Project, which are supported by the National Institutes of Health and the National Science Foundation. While these repositories are an encouraging step, lack of uniformity between contributing sites with respect to imaging parameters may introduce bias that impairs the sensitivity of studies which aggregate the data. Ideally, acquisition and processing parameters will progressively become more standardized throughout the neuroimaging research community, allowing for more meaningful data pooling. Nonetheless, in the meantime, it may be valuable to continue conducting machine learning studies with available sample sizes to refine prediction models.

3.2. Features
Features used by past studies have primarily focused on extracting information from T1-weighted imaging and fMRI. The use of these features in an initial attempt to model depression diagnosis and treatment response is in accordance with past neuroimaging-based studies that have predominantly found anatomical changes and altered brain activity to be valuable biomarkers of major depression (Dunlop and Mayberg, 2014, McGrath et al., 2013).

3.3. Learning method(s)
All past studies, except one which is a regression-based study (Mwangi et al., Jan 2012), have used support vector machines or a variant method as either their primary method or as a means of comparison with their primary method. Given the literature on machine learning methods, support vector machines are a popular method as observed among depression studies. Support vector machines draw its popularity from its useful strengths – especially when working with real-world data – including a reliable theoretical foundation and its insensitivity to high-dimensional data. Nevertheless, there are other methods that may perform equally well or better depending on the nature of the data (Wu et al., 2008).

However, there is variability as to whether a linear or non-linear learning method was used among these past studies. According to support vector machine literature, if the number of samples is significantly less than the number of features – which may be the case for several of these studies – non-linear learning methods do not significantly affect the results and it may be better to simply use linear learning methods (Hsu et al., 2010, Raudys and Jain, 1991). Thus, to avoid complexity and chances of overfitting, linear learning methods may be optimal.

3.4. Feature reduction method(s)
Compared to other methods, the past studies show the greatest heterogeneity in their selection of feature reduction methods. Nevertheless, the most commonly used methods fall into the category of supervised feature selection methods. Given the small sample sizes used by the past studies and the literature indicating that the performance of feature selection improves with an increase in sample size (Jain and Zongker, 1997), performing supervised feature selection methods on the small dataset may be suboptimal. Hypothetically, a more effective approach for obtaining a model that generalizes well over unseen data, as used by Fu et al. (2008), may be to perform feature selection based on biomarkers already shown to be associated with major depression and its treatment response by larger studies. However, this requires further testing with larger test sets to gain a more objective understanding of which feature reduction methods produce an optimal model given smaller datasets.

Some past studies have also used an unsupervised feature reduction method, namely principal component analysis. Once again, it may be more effective to perform unsupervised feature reduction methods on larger datasets as they provide more information for generalizing population trends accurately (Osborne and Costello, 2004).

3.5. Cross-validation method
Most past studies have used the leave-one-out cross-validation method. This is not surprising as it is the most popular method for studies with small sample sizes (e.g. proof-of-concept studies). Compared to several other cross-validation methods, the leave-one-out cross-validation method provides the learning method with more data for training a prediction model. Thus, it is utilized when the available data is limited due to a small sample size. Nevertheless, it is associated with high variance making it unreliable for obtaining accurate estimates of a prediction model for larger-scale studies (Elisseeff and Pontil, 2002, Refaeilzadeh et al., 2009)."
Machine learning in major depression: From classification to treatment outcome prediction,"Aims
Major depression disorder (MDD) is the single greatest cause of disability and morbidity, and affects about 10% of the population worldwide. Currently, there are no clinically useful diagnostic biomarkers that are able to confirm a diagnosis of MDD from bipolar disorder (BD) in the early depressive episode. Therefore, exploring translational biomarkers of mood disorders based on machine learning is in pressing need, though it is challenging, but with great potential to improve our understanding of these disorders.

Discussions
In this study, we review popular machine-learning methods used for brain imaging classification and predictions, and provide an overview of studies, specifically for MDD, that have used magnetic resonance imaging data to either (a) classify MDDs from controls or other mood disorders or (b) investigate treatment outcome predictors for individual patients. Finally, challenges, future directions, and potential limitations related to MDD biomarker identification are also discussed, with a goal of offering a comprehensive overview that may help readers to better understand the applications of neuroimaging data mining in depression.

Conclusions
We hope such efforts may highlight the need for an urgently needed paradigm shift in treatment, to guide personalized optimal clinical care.","4.1 Multi-cross-classification for large sample size and deep learning
The selection of appropriate learning methods is very important to accurately learn a classification framework. Some researchers have successfully realized this selection of methods as applied to other brain disorders.114-116

Large sample size is of great importance for valid classification performance, but it is often not easy to collect large samples at one single site. To address this problem, multisite data sharing has been proposed which allows for cross-site classification of psychiatric disorders such as schizophrenia. In cross-site classification, the model is trained at one or several independent sites and tested at different sites. As a significant machine-learning method for large multisite data, cross-classification can tackle the problem of overfitting and further provide potential biomarkers less specific to one certain site, which will be more generalizable in clinical practice. Rozycki et al used advanced multivariate analysis tools and structural neuroimaging data of 941 participants from five sites to find neuroanatomical signature of patients with schizophrenia and establish cross-site classification with robust generalizability.114 Zeng et al proposed discriminant deep learning method using fMRI of 734 participants form seven sites to learn discriminating functional connections and achieve accurate prediction.115 Both of the studies conducted pooling classification and leave-site-out validation, and obtained promising classification results. Discriminating brain patterns were shared by all sites. Cross-site classification is still challenging, but shows promise for the future.

Another state of the art machine-learning method is deep learning, which has the ability to extract hidden information from high-dimensional data, and some researches have already show enhanced classification accuracies with neuroimaging data. For example, Kim et al adopt deep neural network (DNN) with L1-norm controlling weight sparsity in hidden layer for whole-brain resting-state FC pattern classification of schizophrenia vs HC.116 Zeng et al investigated cross-site classification with deep learning in schizophrenia for the first time.115 These methods may transfer to other neuropsychiatric disorders such as MDD to build diagnostic tools and provide better analysis about pathophysiology.

4.2 Multimodal MRI in MDD
Although fMRI and sMRI biomarkers have been found to be associated with depression, there are additional studies that have shown the relevance of DTI biomarkers.41, 47, 58, 93 Additionally, nonimaging measures have also been used in depression.117 Thus, it is important to study how multimodal MRI in conjunction with nonimaging features affects prediction models of depression.27, 50 Each MRI modality represents different view of the brain, and data fusion capitalizes on the strengths of each modality and their inter-relationships in a joint analysis to unravel the pathophysiology of brain disease.117-119 Recent advances in data fusion120-122 increase our confidence in multimodal approaches and also provide insight into both anatomical and functional information.123-125 Often multimodal studies reveal information which may be missed by methods based on a single modality.126 Some studies have already applied advanced multimodal fusion methods like multisite canonical correlation analysis with reference + joint independent component analysis (mCCA+jICA)119 and its variants117 in MDD associated analysis with promising classification performance.50 In ref. 27 jointly selected features from amplitude of low-frequency fluctuation (ALFF) and GM trained by the SVM classifier enable classifying MDD and BD at high accuracy based on the identified features (eg, dorsal lateral prefrontal cortex in GM). Data fusion methods combined with machine learning are thus a promising direction for depression classification.

4.3 Multiple classification and subtypes
Multiple classification can be performed via a pseudo multiclass strategy by applying a two-class algorithm either to separate pairs of depression subtypes or to separate different subtype from one other. An alternative approach for this problem is to apply clustering methods to label subjects as belonging to specific disease subtype clusters.22 Although abnormalities of major neuroanatomical regions and neural networks are common in one generalized category of disorders, prominently disturbed symptoms differ between subtypes, which have already been found in MDD127 and BD.128, 129 Differentiating a disorder sharing symptoms with other disorders is also one of the main challenges in psychiatry and neurology. It has been reported that such overlapping disorders include schizophrenia, bipolar, unipolar, and mood disorders. Classification with task-based fMRI has achieved good performance in distinguishing schizophrenia and bipolar disorder.130-132 Other studies also classify schizophrenia, bipolar and healthy controls with high accuracy using sMRI.59, 64, 133, 134 So, multiple classification in depression is thought to be very promising but also challenging.22

Besides, there are also some researches applying machine-learning methods to explore vulnerable biomarkers. In the field of MDD, Opel et al investigated gray matter alterations of healthy controls, MDD patients, healthy first-degree relatives of MDD and healthy individuals exposed to former childhood maltreatment using univariate analysis (t test) and pattern recognition approach (SVM) to conduct both group- and individual-level analyses. The classifier can successfully detect individuals with a risk of MDD and perform even better with the help of specific brain regions associated with MDD found by group-level analysis, showing the potential power in searching familial and environmental risk factors for MDD in the future.135

4.4 Large-scale datasets
Problems with data heterogeneity may be reduced through using large training datasets.32, 109 In the past few years, several multiple center data repositories such as PGC, ENIGMA, UK Biobank, and others have been started, and they are all collaborative confederation with many working groups including major depression group. Despite some respective MDD group of these large consortium contains relatively smaller sample size, there are also marvelous achievements. For example, MDD workgroup has been a part of the PGC since 2007, and now it covers over 100 000 people with depression. The PGC MDD group has published a paper136 confronting the notably challenges in genetic dissection of MDD and keeps increasing sample size and expanding their studies.137-140 The ENIGMA MDD working group includes brain scans of around >5000 MDD patients and >9000 controls from 35 research samples of 14 different countries worldwide. Its primary aim is to identify imaging markers that robustly discriminate MDD patients from healthy controls cross-site with standardized image processing and statistical analysis protocols.141-144 More recently, a predictive analytics competition (PAC), a major depression classification challenge with the goal of automatic classification of patients suffering from major depression, and healthy individuals based on sMRI data have been carried out (https://www.photon-ai.com/pac). The training data of PAC contain labeled sMRI data of 759 MDD patients and 1033 normal controls from three different sites which are free to public. The unlabeled testing data of totally 448 individuals from three sites are also available. The winners of 2018 PAC competition achieved 65% accuracy on classifying MDD from HC. Another ongoing project named REST-meta-MDD in China also integrated multisite meta-analysis results from thousands of resting-state fMRI data of MDD and HC. We believe that the field of MDD can benefit a lot from similar machine-learning competitions and projects.
"
"Predicting anxiety, depression and stress in modern life using machine learning algorithms","In the fast-paced modern world, psychological health issues like anxiety, depression and stress have become very common among the masses. In this paper, predictions of anxiety, depression and stress were made using machine learning algorithms. In order to apply these algorithms, data were collected from employed and unemployed individuals across different cultures and communities through the Depression, Anxiety and Stress Scale questionnaire (DASS 21). Anxiety, depression and stress were predicted as occurring on five levels of severity by five different machine learning algorithms – because these are highly accurate, they are particularly suited to predicting psychological problems. After applying the different methods, it was found that classes were imbalanced in the confusion matrix. Thus, the f1 score measure was added, which helped identify the best accuracy model among the five applied algorithms as the Random Forest classifier. Furthermore, the specificity parameter revealed that the algorithms were also especially sensitive to negative results.","Table 4 shows the accuracy, error rate, precision, recall, specificity and f1 score of each class obtained by the different algorithms. From the observation made in Table 4, the highest accuracy for all three scales ofanxiety, depression and stress were achieved by Naïve Bayes. Nevertheless, the results in Table 3 show that the classes were imbalanced, because the confusion matrices of anxiety, depression and stress produced 25, 37 and 43 instances of normal but 7, 12 and 19 instances of mild, respectively. Similarly, there were 35, 25 and 23 instances of moderate; 11, 11 and 16 instances of severe; and 27,19 and 4 instances of extremely severe for the scales of Anxiety, Depression and Stress respectively; thus, the classes here were imbalanced. In such cases, accuracy alone is not a sufficient measure, and the f1 score becomes an important measure for determining the best model. The f1 score is a harmonic mean of precision and recall, whose value is higher when both precision and recall are higher. Consequently, the best model in cases of classification imbalance is the one whose f1 score is higher even if its accuracy is lower. The f1 score of Random Forest was the highest for Stress and Naïve Bayes for depression whereas it is low for all the algorithms for anxiety. The specificity of all the algorithms was found to be around 90% or above in all three cases. Moreover, this is an important parameter in healthcare because it shows that the negative cases(patients without diseases) are also classified appropriately. All of the algorithms applied in this study produced highly accurate results for negative cases as well. Figure 5 shows the variable importance ratings for Anxiety, Depression and Stress, respectively; the higher the number, the more important the variable. That is, the variable ‘Scared_without_any_good_reason’ proved to be most important on the Anxiety scale; the variable ‘Life_was_meaningless’ was found to be important on the Depression scale whilst‘Difficult_to_relax’ was found to be important on the Stress scale."
Applications of machine learning algorithms to predict therapeutic outcomes in depression: A meta-analysis and systematic review,"Background
No previous study has comprehensively reviewed the application of machine learning algorithms in mood disorders populations. Herein, we qualitatively and quantitatively evaluate previous studies of machine learning-devised models that predict therapeutic outcomes in mood disorders populations.

Methods
We searched Ovid MEDLINE/PubMed from inception to February 8, 2018 for relevant studies that included adults with bipolar or unipolar depression; assessed therapeutic outcomes with a pharmacological, neuromodulatory, or manual-based psychotherapeutic intervention for depression; applied a machine learning algorithm; and reported predictors of therapeutic response. A random-effects meta-analysis of proportions and meta-regression analyses were conducted.

Results
We identified 639 records: 75 full-text publications were assessed for eligibility; 26 studies (
) and 20 studies (
) were included in qualitative and quantitative review, respectively. Classification algorithms were able to predict therapeutic outcomes with an overall accuracy of 0.82 (95% confidence interval [CI] of [0.77, 0.87]). Pooled estimates of classification accuracy were significantly greater (p < 0.01) in models informed by multiple data types (e.g., composite of phenomenological patient features and neuroimaging or peripheral gene expression data; pooled proportion [95% CI] = 0.93[0.86, 0.97]) when compared to models with lower-dimension data types (
).

Limitations
Most studies were retrospective; differences in machine learning algorithms and their implementation (e.g., cross-validation, hyperparameter tuning); cannot infer importance of individual variables fed into learning algorithm.

Conclusions
Machine learning algorithms provide a powerful conceptual and analytic framework capable of integrating multiple data types and sources. An integrative approach may more effectively model neurobiological components as functional modules of pathophysiology embedded within the complex, social dynamics that influence the phenomenology of mental disorders.","4.1. Evidence source
Approximately 96% of studies included in the qualitative review were published in peer-reviewed medical journals, with a recent trend towards higher impact factor journals. Results from Egger’s test and funnel plot asymmetry prior to trim and fill additionally support the presence of publication bias. Although our search included grey literature sources, studies that applied machine learning algorithms with negative results may have been less likely to be published, abstracted, or made available online. The vast majority of studies that inform this nascent field appears to have been published in the past three years, which may be due to improved access to data and computational resources in recent years. Alternatively, older studies using a machine learning algorithm may have used a different terminology that was not captured in our search or by the subject headings in present use. However, the year and the impact factor of publication did not moderate pooled estimates of overall classification accuracy in meta-regression analyses. Country of study origin also did not moderate pooled estimates of classification accuracy, although it is important to note that our search was delimited to manuscripts in the English language.

4.2. Evidence types
While studies using structural or functional neuroimaging data to inform a predictive model were grouped together in our analysis, resting-state EEG is cheaper, simpler, and thus more accessible than f/MRI. In our meta-regression analyses, there were no differences in classification accuracy proportions between studies informed by EEG or f/MRI data. Although only one study identified in our review used task-based EEG, given the better temporal span with EEG when compared to fMRI, EEG may represent a feasible and cost-effective method to investigate neural correlates of therapeutic outcomes (Bailey et al., 2018).

We identified four studies that included genetic data to inform predictive models; two studies were included in the meta-analysis, one of which combined genetic data with phenomenological data. Notwithstanding the growing interest in peripheral tissue biomarkers and their potential to inform treatment selection and to optimize therapeutic decisions (Gadad, Jha, Czysz, Furman, Mayes, Emslie, Trivedi, 2017, Köhler, Freitas, Stubbs, Maes, Solmi, Veronese, de Andrade, Morris, Fernandes, Brunoni, Herrmann, Raison, Miller, Lanctôt, Carvalho, 2017, Raison, Rutherford, Woolwine, Shuo, Schettler, Drake, Haroon, Miller, 2013, Rethorst, Toups, Greer, Nakonezny, Carmody, Grannemann, Huebinger, Barber, Trivedi, 2013, Strawbridge, Arnone, Danese, Papadopoulos, Herane Vives, Cleare, 2015), we did not identify studies that included other peripheral tissue biomarkers (e.g., serum levels of inflammatory or metabolic substrates, markers of neuroplasticity) to devise predictive models. Only one study (
) investigating a relatively small set of SNPs was included in the genetic predictor subgroup, limiting any conclusions that may be made regarding the utility of genetic data in informing predictive models of therapeutic outcomes. The neuroimaging and phenomenological subgroups respectively contributed 52.8% and 31.0% of the model weight; the genetic and combined predictor subgroups comparatively contributed fewer studies (6.0% and 10.1% of model weight, respectively). The differences in the availability of data between subgroups limit inferences that may be made about the predictive utility of different modalities of data.

It is worth noting that not all variables used to inform a model have predictive capability: machine learning algorithms select variables that improve estimates of model accuracy and generalizability and omit variables that reduce or do not affect the foregoing measures. Some machine learning techniques (e.g., neural networks) do not yield estimates of individual predictors’ importance, precluding any conclusions that may be made about an individual variable’s significance as a predictor of a given outcome.

4.3. Study design
Few studies were prospectively conducted to inform predictive models of therapeutic response; most studies were retrospective analyses of large, published, interventional clinical trials that primarily investigated antidepressant efficacy. All studies operationalized therapeutic response as baseline-to-endpoint change on a validated measure of overall depressive symptom severity, which may not correlate with psychosocial, role, or occupational functioning and other biopsychosocial moderators of patients’ functional recovery (McIntyre and Lee, 2016). Symptom rating scales are often conducted retrospectively and may be confounded by patients’ subjective reports of their symptoms and their ability to recall or recognize their symptoms.

Hyperparameter tuning describes the process of identifying and selecting the optimal constraints or parameters of a machine learning algorithm for a given dataset, as opposed to applying the default values specified in a software package or manually setting alternative values. Examples of hyperparameter tuning strategies include calculating and comparing model performance with different hyperparameter settings chosen randomly or exhaustively (e.g., grid search). Carefully selected hyperparameters can significantly influence model performance; however, hyperparameter tuning strategies are computationally expensive and remain an area of active research (Probst, Bischl, Boulesteix, van Rijn, Hutter, 2017). Eight studies in our review (40.0%) (Bailey, Hoy, Rogasch, Thomson, McQueen, Elliot, Sullivan, Fulcher, Daskalakis, Fitzgerald, 2018, Chekroud, Zotti, Shehzad, Gueorguieva, Johnson, Trivedi, Cannon, Krystal, Corlett, 2016, Guilloux, Bassi, Ding, Walsh, Turecki, Tseng, Cyranowski, Sibille, 2015, Korgaonkar, Rekshan, Gordon, Rush, Williams, Blasey, Grieve, 2015, Mumtaz, Xia, Mohd Yasin, Azhar Ali, Malik, 2017, Serretti, Smeraldi, 2004, Serretti, Zanardi, Mandelli, Smeraldi, Colombo, 2007, Wade, Joshi, Njau, Leaver, Vasavada, Woods, Gutman, Thompson, Espinoza, Narr, 2016) reported the use of a hyperparameter tuning method (e.g., grid search using ROC AUC maximisation), whereas the remaining studies did not report a hyperparameter tuning method or reported the use of default parameters for a given algorithm.

Despite the importance of hyperparameter tuning, the use of a hyperparameter tuning method did not moderate pooled estimates of classification accuracy. The observed lack of a benefit in predictive utility with a hyperparameter tuning method may be explained by differences in which tuning strategies the study authors applied and how they were implemented (e.g., hyperparameter search space, which hyperparameters were tuned, other differences in datasets and models). Future studies should compare classification accuracy with and without hyperparameter tuning using the same model and dataset to evaluate the utility of hyperparameter tuning.

4.4. Model evaluation
One approach to reduce overfitting and improve generalizability is to divide a given sample into three separate datasets for model evaluation: a training set to devise a model, a validation set to optimize the model’s parameters, and a test set to evaluate the model’s performance, providing an unbiased estimate of classification accuracy using data not “seen” by the training algorithm that devised the model. Most studies identified in our review did not assess out-of-sample estimates of model fit, thereby limiting their generalizability and likely overestimating predictive accuracy. Four (20.0%) studies evaluated model performance in a separate dataset not used in training or cross-validation. One study (5.0%) evaluated model performance using hold-out validation. External validation sets were often derived from a clinical trial with similar methodology (e.g., participant inclusion and exclusion criteria, sampling method, intervention, research site location).

A cross-validation scheme can be used to create multiple training and validation set pairs to reduce the variance of model accuracy and improve the accuracy of parameter selection. The majority of studies included in our meta-analysis reported the use of a cross-validation method with permutation testing: 7 (35.0%) studies used a non-exhaustive method (k-fold, 
; Monte–Carlo repeated random subsampling, 
) and 11 (55.0%) studies used an exhaustive leave-n-out method (with nested loops, 
; without nested loops, 
).

However, a limitation of using separate datasets to train, validate, and test a model is the reduction in sample size and statistical power, which limits the ability to detect predictors. It is additionally difficult to determine a “gold standard” or estimate a minimum sample size to optimally train and validate models with machine learning approaches, as the estimator can suffer from bias and/or variance errors with more training data and/or the training and validation scores may converge (i.e., learning curve) (Goodfellow et al., 2016). Sample sizes of studies identified in our review ranged from 10 to 7221."
An in-depth analysis of machine learning approaches to predict depression,"Among all the forms of psychological and mental disorders, depression is the most common form. Nowadays a large number of youths and adults around the world suffer from depression. Depression can cause severe problems in case of failing to detect it at an early stage or failing to ensure the timely counseling of a depressed person. It is one of the major reasons to raise suicidal cases. But ironically, our society still does not want to acknowledge depression as a mental disorder causing a significant number of depressed persons to remain unidentified and untreated. This study has investigated six different machine learning classifiers using various socio-demographic and psychosocial information to detect whether a person is depressed or not. Besides, three different feature selection methods, such as Select K-Best Features (SelectKBest), Minimum Redundancy and Maximum Relevance (mRMR), and Boruta feature selection algorithm have been used for extracting the most relevant features from the dataset. To achieve better accuracy in predicting depression, Synthetic Minority Oversampling Technique (SMOTE) has been used that reduces the class imbalance of the training data. The AdaBoost classifier with the SelectKBest feature selection technique has outperformed all other approaches with an accuracy of 92.56%. Moreover, other evaluation metrics, namely sensitivity, specificity, precision, F1-score, and area under the curve (AUC) of different models have been calculated to identify the most efficient model for predicting depression.","It is observed from Table 7 that the Bagging classifier has shown the best result with an accuracy of 89.26% without performing feature selection. Here, the least accuracy has been shown by KNN. It has attained an accuracy of 66.94%. The achieved accuracies of AdaBoost, GB, XGBoost, and Weighted Voting classifiers are 87.60%, 86.78%, 85.95%, and 86.78%, respectively.
By applying different feature selection techniques, accuracies of all of these classifiers have been increased dramatically. While using the SelectKBest feature selection technique, the AdaBoost has outperformed the other classifiers in terms of accuracy. It has achieved an accuracy of 92.56%. By applying the SelectKBest feature selection technique, the accuracies of the other classifiers namely, KNN, GB, XGBoost, Bagging, and Weighted Voting classifiers are 85.12%, 91.74%, 86.78%, 90.91%, and 91.74%, respectively.
In the case of using the mRMR feature selection technique, KNN, AdaBoost, GB, XGBoost, Bagging, and Weighted Voting classifiers have attained accuracies of 84.30%, 91.74%, 90.08%, 90.08%, 90.08%, and 90.08%, respectively.
Using the Boruta feature selection technique, AdaBoost has shown superior performance than the other classifiers in terms of accuracy. Here, the achieved accuracies of KNN, AdaBoost, GB, XGBoost, Bagging, and Weighted Voting classifiers are 85.12%, 91.74%, 90.91%, 86.78%, 90.08%, and 90.91%, respectively.
Sensitivity and specificity play a great role in evaluating the performance of a model. A model with a higher sensitivity has a higher ability to identify participants with depression, and a model with a higher specificity has a higher ability to identify participants without depression.
Without applying feature selection techniques, the Bagging classifier has achieved the highest sensitivity of 93.24%. But the attained specificity of the Bagging classifier is only 82.98%. On the other hand, the KNN classifier has shown the highest specificity of 87.23% without performing feature selection. But in terms of sensitivity, KNN has shown the worst performance. Here, the achieved sensitivity of KNN is 54.05%.
While using the SelectKBest feature selection technique, the AdaBoost classifier has obtained the highest sensitivity and specificity of 91.89% and 93.62%, respectively. AdaBoost has shown the highest specificity and sensitivity of 89.36% and 93.24%, respectively, in the case of using the Boruta feature selection technique also. While applying mRMR as a feature selection technique, GB, XGBoost, and Weighted Voting classifiers have gained the highest sensitivity of 91.89%. They have achieved specificities of 87.23%. Here, AdaBoost has gained the highest specificity of 93.62%. On the other hand, it has achieved a sensitivity of 90.54%. So, by analyzing the performance of the classifiers in terms of sensitivity and specificity, it can be ensured that by applying the above feature selection techniques, both the sensitivity and specificity of the classifiers have been improved.
Also, the Area Under Curve (AUC) values of these models have been calculated. If a model's AUC value is 1, then it is assumed to be a perfect model or classifier. When the AUC value of a model is 0.5, it cannot distinguish between the samples of different classes. So, a higher AUC value of a model is always desired. The mentioned feature selection techniques have enhanced the performance of the classifiers in terms of AUC, Precision, and F1-score also.
To demonstrate the trade-off between the sensitivity and specificity of the models, the Receiver Operator Characteristic (ROC) curve is used. It is a two-dimensional graph where the x-axis represents the False Positive Rate, and the y-axis represents the True Positive Rate. The closer the ROC curve of a classifier is to the top-left corner of the graph, the better the performance of the classifier is. Fig. 2 (a)–(d) shows the ROC curves of these classifiers using different feature selection techniques. Fig. 2 reveals that the ROC curves of the classifiers have moved closer to the graph's upper left corner after applying feature selection techniques.
Fig. 2
Reducing the number of features using different feature selection techniques have made the classifiers faster. From Fig. 3, it is revealed that different feature selection techniques have reduced the training time of the classifiers significantly.
Fig. 3 shows that for minimizing the training time of the KNN classifier, mRMR has outperformed other feature selection techniques, and for minimizing the training time of the AdaBoost, GB, and XGBoost classifiers, SelectKBest feature selection technique has shown the best performance. Boruta feature selection technique has minimized the training time of Bagging and Weighted Voting classifiers more than other feature selection techniques.
So, from the above discussions, it is found that the SelectKBest feature selection technique has outperformed other feature selection techniques in most of the cases in order to minimize the training time of the classifiers.
Comparing the results of different models, it can be concluded that the AdaBoost classifier with the SelectKBest feature selection algorithm has surpassed the other models in terms of accuracy, AUC value, ROC-curves, and other measured performance metrics. Adaboost classifier with SelectKBest feature selection technique has shown the highest accuracy of 92.56%. This model has also attained the highest AUC value of 0.96.
Boruta feature selection algorithm discards the irrelevant features based on the Maximum Z-score. In this study, this algorithm has discarded seventeen predictor variables based on the Maximum Z-score and found the remaining thirteen predictor variables to be significant. The SelectKBest algorithm has ranked the predictor variables based on the score. On the other hand, the mRMR algorithm has ranked the predictor variables based on their relevance and redundancy. This study has selected the top fifteen predictor variables from the rankings of both SelectKBest and mRMR algorithms, as in both cases using the top fifteen predictor variables as input variables for the classifiers yields the best results. In Table 4, the top fifteen features selected by the SelectKBest and the mRMR algorithms are arranged according to their rankings. Table 8 shows the accuracy of different classifiers by taking a different number of features from the rankings for both SelectKBest and mRMR algorithms.
So, from Table 8, it can be undoubtedly stated that the majority of the classifiers shows the best performance while using the top fifteen features as input variables for both SelectKBest and mRMR algorithms.
For understanding the quality of this study, it is necessary to compare the results and contribution of this study with other existing works. Most of the previous works have been performed to predict depression among people of a certain age group, occupation, or health condition. A few of them have extracted the most relevant socio-demographic and psychosocial factors that cause depression. But this study has been conducted to predict depression among people of different age ranges, professions, and socioeconomic backgrounds. This study has also identified the most significant socio-demographic and psychosocial factors that cause depression. Table 9 shows a comparative analysis of this study with other existing works."
Depression detection from social network data using machine learning techniques,"Purpose
Social networks have been developed as a great point for its users to communicate with their interested friends and share their opinions, photos, and videos reflecting their moods, feelings and sentiments. This creates an opportunity to analyze social network data for user’s feelings and sentiments to investigate their moods and attitudes when they are communicating via these online tools.

Methods
Although diagnosis of depression using social networks data has picked an established position globally, there are several dimensions that are yet to be detected. In this study, we aim to perform depression analysis on Facebook data collected from an online public source. To investigate the effect of depression detection, we propose machine learning technique as an efficient and scalable method.

Results
We report an implementation of the proposed method. We have evaluated the efficiency of our proposed method using a set of various psycholinguistic features. We show that our proposed method can significantly improve the accuracy and classification error rate. In addition, the result shows that in different experiments Decision Tree (DT) gives the highest accuracy than other ML approaches to find the depression.

Conclusions
Machine learning techniques identify high quality solutions of mental health problems among Facebook users.
","For a better understanding of the general intuition behind depression, in this paper, we applied Decision Tree, KNN, SVM and Ensemble classifier techniques for depression detection of emotional terms. We showed that all of these classification techniques based on linguistic style, emotional process, temporal process and all (Linguistic, emotional and temporal) features are able to successfully extract the depressive emotional result. Tables 5 and 6 demonstrate the results of various characterizations with various proportions of four features. It can be observed that Decision Tree gives the better outcome. We believe that the current study has laid the ground for future research on inferences and discovery of additional information based on cause-event relation, such as detection of implicit emotion or cause, as well as prediction of public opinion based on cause events, etc. Moreover, in this paper, we applied total 21 types of attributes of LIWC software for detecting depression, but we can apply more than 54 attributes. Though we achieved accuracy between 60 and 80%; there is still some room for improvement. It is important to note that this study does not identify who the sufferers are; but assess the Facebook comments for depression detection."
"Machine learning algorithms for depression: diagnosis, insights, and research directions","Over the years, stress, anxiety, and modern-day fast-paced lifestyles have had immense psychological effects on people’s minds worldwide. The global technological development in healthcare digitizes the scopious data, enabling the map of the various forms of human biology more accurately than traditional measuring techniques. Machine learning (ML) has been accredited as an efficient approach for analyzing the massive amount of data in the healthcare domain. ML methodologies are being utilized in mental health to predict the probabilities of mental disorders and, therefore, execute potential treatment outcomes. This review paper enlists different machine learning algorithms used to detect and diagnose depression. The ML-based depression detection algorithms are categorized into three classes, classification, deep learning, and ensemble. A general model for depression diagnosis involving data extraction, pre-processing, training ML classifier, detection classification, and performance evaluation is presented. Moreover, it presents an overview to identify the objectives and limitations of different research studies presented in the domain of depression detection. Furthermore, it discussed future research possibilities in the field of depression diagnosis.","We propose some possible future study directions in this part, based on the review of prior research in the preceding section.
(1)
A larger data sample is required:
The majority of prior depression detection research utilized a small sample size. A small sample size is useful for building a prediction model, while a bigger sample size is important for constructing a more accurate model that works well throughout the population. When a large sample size is used to train a model, it allows for a greater diversity of depressed patients to be included, perhaps leading to models with real therapeutic value. When a few studies use bigger datasets, the methods will most likely alter and show more developed approval metrics. The k-fold cross-validation technique, in particular, may be employed with higher k-values to allow for larger test sets on which to test prediction models and increase generalizability.
(2)
Learning method(s):
Various learning techniques give a better outcome in different situations; therefore, choosing the right one is crucial. Unlabeled data may sometimes help develop a prediction model for a large sample size with little data. As a result, the first step is to determine if the incoming data are labeled, unlabeled, or a combination of labeled and unlabeled data. As a result, employing an unsupervised, supervised, or semi-supervised learning technique will be determined. The second phase is dependent on the learning method’s objective, which must be addressed. The last stage is to identify whether the input is linear or nonlinear; linear data are helpful when the dataset is small to prevent overfitting, whereas nonlinear data are important when the dataset is big. The last step is to choose a learning technique to limit the options. The technique for picking the best learning method is to assess various factors such as complexity, flexibility, computation time, optimization ability, and so on, and then choose the best one. If you have too many learning method choices, evaluate the performance of each technique on the provided data; if you just have a few, simply change the default model to make it more appropriate for learning the given data.
(3)
Clinical application:
Long-term, creating a predictive model aims to find a method that can improve accuracy. However, such a scenario is unlikely to arise in the next few years, since SVM and a few other supervised learning algorithms are presently trustworthy and seem to be around in this area of research. Regardless, after a sufficiently strong method has been thoroughly authorized via preliminary considerations, showing its efficacy, and determining whether it will benefit patients or not, its progression to clinical preliminaries will be critical. Future clinical trials should ensure that machine learning methods efficiently identify depressed individuals who are unlikely to respond to the current specialist under investigation. Clinicians’ use of this information improves patient outcomes (for example, diminished inactivity among determination and reduction).
(4)
Collaboration of research groups:
With the significant progress among different disciplines, collaboration with other disciplines is crucial for ADE. For affective computing, relevant fields include psychology, physiology, computer science, ML, etc. Thus, researchers should borrow each other’s strengths to promote ADE’s advances. For audio-based ADE, the deep models only represent the depression scale from audios. The deep models capture patterns only from facial expressions specific to video-based ADE. Notably, physiological signals also contain significant information closely related to depression estimation. Accordingly, different researchers should study together to build multimodal-based DL approaches for clinical application.
(5)
Availability of databases:
Because of the sensitivity of depression data, it is difficult to gain various data for estimating the scale of depression. Hence, the availability of data is a major issue. First, as opposed to the facial expression recognition task, database availability is scarce up to the present day. Given the literature review, one can note that the widely used depression databases are AVEC2013, AVEC2014, and DAIC-WOZ. Notably, AVEC2014 is a subset of AVEC2013. Second, there is no multimodal (i.e., audio, video, text, physiological signals) database to learn comprehensive depression representations for ADE. The existing databases consist of two or three modalities. Though the DAIC database comprises three modalities (audiovisual and text), the organizer has not provided the original videos of DAIC, leading to a certain inconvenience for ADE. Third, the limited size of the datasets limits the research in depression prediction, especially when using DL technologies. For instance, AVEC2013 only contains 50 samples for training, development, and test set. Effective methods to augment the limited amount of annotated data are called to address this bottleneck. Fourth, the criteria for data collection should be standardized. At present, different organizers adopt a range of conditions, equipment, and configurations to collect multimodal data."
"Assessment of anxiety, depression and stress using machine learning models","Over the last few decades, psychological health issues have become very common in people worldwide. In this paper, prediction of the occurrence of psychological problems such as anxiety, depression and stress has been made by applying eight machine learning algorithms to data taken from the online DASS42 tool. Five different severity levels of anxiety, depression and stress have been predicted using eight algorithms. The algorithms are grouped into four categories: probabilistic, nearest neighbor, neural network and tree based. A hybrid classification algorithm was also applied for prediction of different severity level anxiety, depression and stress. The same methods were also applied to another dataset, DASS21 collected by authors. The prediction accuracy found by using the hybrid algorithm was greater than by using single algorithms, but the highest accuracy was found by use of the radial basis function network, which comes under the category of neural network.","The same classification techniques were also applied on a different dataset from DASS21. This data was collected
through Google forms completed by 349 participants from various parts of north India. The dataset consisted of 349
adults aged between 18 and 60 years with five severity levels of anxiety, depression and stress. The DASS21 score
was calculated in same manner as DASS42. This score needed to be multiplied by two for each condition: anxiety,
depression and stress because DASS21 consisted of only 21 items, with seven questions in each category i.e.
anxiety, depression and stress. The results of the application of eight different ML methods on DASS21 is shown in
table 3.
The results in table 3 show that MLP gives best performance for anxiety, depression and stress and RBFN gives
the best performance for depression in DASS21. The accuracy is 100 percent and the precision is one for anxiety in
random forest. These fluctuations may have arisen because there was only a small number of instances in dataset.
Otherwise RBFN outperformed for this dataset also. Figure 4 shows the accuracy of classification of different ML
methods for DASS21."
Detection of child depression using machine learning methods,"Background
Mental health problems, such as depression in children have far-reaching negative effects on child, family and society as whole. It is necessary to identify the reasons that contribute to this mental illness. Detecting the appropriate signs to anticipate mental illness as depression in children and adolescents is vital in making an early and accurate diagnosis to avoid severe consequences in the future. There has been no research employing machine learning (ML) approaches for depression detection among children and adolescents aged 4–17 years in a precisely constructed high prediction dataset, such as Young Minds Matter (YMM). As a result, our objective is to 1) create a model that can predict depression in children and adolescents aged 4–17 years old, 2) evaluate the results of ML algorithms to determine which one outperforms the others and 3) associate with the related issues of family activities and socioeconomic difficulties that contribute to depression.

Methods
The YMM, the second Australian Child and Adolescent Survey of Mental Health and Wellbeing 2013–14 has been used as data source in this research. The variables of yes/no value of low correlation with the target variable (depression status) have been eliminated. The Boruta algorithm has been utilized in association with a Random Forest (RF) classifier to extract the most important features for depression detection among the high correlated variables with target variable. The Tree-based Pipeline Optimization Tool (TPOTclassifier) has been used to choose suitable supervised learning models. In the depression detection step, RF, XGBoost (XGB), Decision Tree (DT), and Gaussian Naive Bayes (GaussianNB) have been used.

Results
Unhappy, nothing fun, irritable mood, diminished interest, weight loss/gain, insomnia or hypersomnia, psychomotor agitation or retardation, fatigue, thinking or concentration problems or indecisiveness, suicide attempt or plan, presence of any of these five symptoms have been identified as 11 important features to detect depression among children and adolescents. Although model performance varied somewhat, RF outperformed all other algorithms in predicting depressed classes by 99% with 95% accuracy rate and 99% precision rate in 315 milliseconds (ms).

Conclusion
This RF-based prediction model is more accurate and informative in predicting child and adolescent depression that outperforms in all four confusion matrix performance measures as well as execution duration.
","It is important to understand the causative factors of depression in children and adolescents in order to detect it accurately and efficiently for their early diagnosis and development. When the parents or the involved persons in their social and academic developmental activities are unsure whether they should consult a medical expert on this issue, this research will help them make a decision to start the treatment at early stage. In this paper, we attempted to determine the specific features on the status of depression, even though the dataset was imbalanced. Class imbalance is a condition in large dataset streams that cannot be overlooked [42]. Since the number of classes fluctuate greatly, we do not want to ignore any key features that has more importance assigned to a particular class on depression analysis. Both unweighted and weighted Random Forest (RF) with Boruta has been employed in the feature selection phase, providing 8 and 11 features, respectively. These 11 features include the same 8 features that previously belonged for weighted RF. We have considered the binary answers that are the main constraints of our research, but according to the YMM questionnaire evaluation and the machine learning model prediction assessment, this feature set is clinically comparable to the major symptoms of depression [43]. As a result, using a RF model with class weight, an optimum feature set has been established for predicting depressive state. The decision path for three sample of test dataset in Table 5 shows the optimal feature set from the questionnaire using the fitted RF model to predict depression status.
The sample of test dataset in Table 5 shows how RF model predicts depression in children and adolescents. In sample 1, there are yes values in unhappy, nothing fun and irritable mood and, for these values, our model is detecting depression. Again, when there are yes values in unhappy and nothing fun, it results no depression. That means, an unhappy child who has feeling nothing fun anymore cannot be addressed as depressed. In the last test sample, if unhappy children are showing no interest in anything as well losing or gaining wights, they need to be started their depression treatment as soon as possible.
It is important for machine learning model to get training data from large dimensional dataset with defined exploratory target domain to find trends, pattern and outliers to make intelligent decisions. In this research we have provided top 62 features of 667 variables of large dimensional dataset. As well, the target variable of this dataset is confirmed by doctor, mental health professional and with the status of diagnosing depression. For this reason, our model can predict depression in children and adolescents with 95% accuracy with most important 11 features in the Australian context. Moreover, the specificity of RF is 100%. Furthermore, this is an important statistic for demonstrating that RF can accurately classify the negative occurrences in the nondepressed class. Our resulted RF model provides highly accurate results for recognising both the depressed and nondepressed classes. During the analysis of these data for depression detection, certain surrounding circumstances of depressed child or adolescent become apparent in Table 6.
Despite the fact that we have identified 11 important features for identifying child depression, the factors listed in Table 6 are noteworthy. All the children who live with their adopted or foster families are suffer from depression. One-third, 37.14% are in poor health condition. Social phobia impacted 32% of them. 27.3% of their caregivers are foster mothers. 25.56% of them do not like school. 14.3% of their residence is in a vacant block. 14.3% of them directly consume or sell drugs. Separation anxiety affects 12.1% of them. 11.76% of their another parent works in a location that is far from their home. Only 3.3% are fired from their jobs. Among all reasons of Table 6, children who have a complicated family situations as opposed to a healthy parent involvement are more likely to suffer from depression. There is evidence that children needs both parents actively involved in their life. It is extremely important in influencing a child’s learning, socialisation, and health benefits. Consequently, it can result in a more secure attachment and a sense of belonging among young children. As well, it ensures a healthy environment for a child’s upbringing."
Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from EEG signal,"Diagnosing depression in the early curable stages is very important and may even save the life of a patient. In this paper, we study nonlinear analysis of EEG signal for discriminating depression patients and normal controls. Forty-five unmedicated depressed patients and 45 normal subjects were participated in this study. Power of four EEG bands and four nonlinear features including detrended fluctuation analysis (DFA), higuchi fractal, correlation dimension and lyapunov exponent were extracted from EEG signal. For discriminating the two groups, k-nearest neighbor, linear discriminant analysis and logistic regression as the classifiers are then used. Highest classification accuracy of 83.3% is obtained by correlation dimension and LR classifier among other nonlinear features. For further improvement, all nonlinear features are combined and applied to classifiers. A classification accuracy of 90% is achieved by all nonlinear features and LR classifier. In all experiments, genetic algorithm is employed to select the most important features. The proposed technique is compared and contrasted with the other reported methods and it is demonstrated that by combining nonlinear features, the performance is enhanced. This study shows that nonlinear analysis of EEG can be a useful method for discriminating depressed patients and normal subjects. It is suggested that this analysis may be a complementary tool to help psychiatrists for diagnosing depressed patients.","In this study, we analyzed resting EEG of 45 depressed patients and 45 normal subjects by power of EEG bands and four nonlinear features. In classification based on EEG bands power, the highest accuracy achieved by alpha power, suggesting that depressed patients and normal subjects differ in alpha band more significantly than other power bands like delta, theta and beta. Furthermore, we observed that alpha power had significant difference in T3, F7, O1, P3, C3 in the left hemisphere and O2 in the right hemisphere. The mean alpha powers in these electrodes were higher in depressed patients in comparison with normal subjects. These results were similar to the results obtained in [3]. In this study, Henriques and Davidson reported that left hemisphere of depressed patients had higher alpha power than left hemisphere of normal subjects. Also, this study showed alpha power was higher in left hemisphere of depressed patients than right hemisphere of this group.

In classifying depression patients and normal healthy subjects with nonlinear features, the highest accuracy was achieved when correlation dimension used as input of classification compared with DFA, higuchi and maximum lyapunov exponent. This experiment showed in discriminating depressed and normal persons, correlation dimension was a powerful feature for analyzing EEG signals. For further improved, we combined the features and used them as one feature vector for classifying. Combination of power bands had no considerable changes in accuracy of classifiers but nonlinear features improved the accuracy of classification significantly. The highest accuracy was 90% by combination of nonlinear features and LR classifier. The number of features, which GA selected for achieving this accuracy, was 14. Most of these features are related to correlation dimension. Compared to previous researches that based on linear and nonlinear analysis of depressed patients and normal subjects EEG signals, this study can achieve considerable accuracy, according to the fact that in this study, independent test is used. The prediction accuracy obtained from the unknown set shows the performance of classification and datasets more precisely. Knott et al. [5] reported the accuracy 91.3% for classifying 70 depressed patients and 23 normal subjects using linear features such as relative power and absolute power. In [8] wavelet entropy was used for analyzing EEG signals of 26 depressed patients and normal subjects and 80% accuracy was achieved in this experiment. Lee et al. [9] applied DFA to EEG of 11 depressed patients and 11 normal subjects and they obtained that DFA of depressed patients are higher than normal subjects but classification were not used in this study. In our study DFA value of both depressed patients and normal subjects were between 0.5 and 1 similar to the results in [9] but no significant difference were found between two groups.

In this study three classifiers were used. Among this classification, LR classifier performed better compared to LDA and KNN classifiers. This study suggests that more nonlinear features should be studied for analyzing EEG of depressed patients. Also, instead of using EEG in rest condition, EEG in different conditions and tasks can be recorded and analyzed for depressed patients and normal subjects. However, future investigation should focus on finding the regions of brain that involved in depression. Finally, an increase in EEG data would make it possible to validate the reliability of these features and classifiers."
Analysis of machine learning algorithms for predicting depression,"The increasing use of technology can cause a life- style of less physical work. Also, the constant pressure on an individual can create a risk of mental disorder. These vulnerabilities include peer pressure, heart attack, depression, and many other effects. In this paper different approaches to predict depression are studied in detail. The mechanisms include collecting dataset through questionnaires asked to the person, posts on social media, text used throughout verbal communication and expressions on face. Result is derived from extracted information. Here output expected is that the person needs attention or not. In this research work different algorithms and classifiers of machine learning such as Decision Trees, SVM, Naive Bayes Classifier, Logistic Regression and KNN Classifier are analyzed to identify state of mental health in a target group. Target groups used for this identification process are public like students of high school, college students and working professionals. The paper also demonstrates an example in which Twitter scrapping tool Twint is used detect whether given Twit is depressive or not.","As a cause of disability Depression has been recognized in the whole world as it is causing more considerable cost to health care systems. Depression leads to negative thinking as well as insecure feeling. It affects physical well-being and behaviors of the depressed person. In more cases, depression is found as one of the leading causes of suicide and substance abuse. In this paper different approaches used for Predicting Depression are analyzed and a Deep Learning mechanism is proposed for automated depression detection. It allows to extract features from collected dataset thorough text on Twitter. This mechanism has potential to improve accuracy."
Depression recognition using machine learning methods with different feature generation strategies,"The diagnosis of depression almost exclusively depends on doctor-patient communication and scale analysis, which have the obvious disadvantages such as patient denial, poor sensitivity, subjective biases and inaccuracy. An objective, automated method that predicts clinical outcomes in depression is essential for increasing the accuracy of depression recognition and treatments. This paper aims at better recognizing depression using the transformation of EEG features and machine learning methods. An experiment based on emotional face stimuli task was conducted, and twenty-eight subjects’ EEG data were recorded from 128-channel HydroCel Geodesic Sensor Net (HCGSN) by Net Station software. The Mini International Neuropsychiatric Interview (MINI) was used by psychiatrists as the criterion for diagnosis of depression patients. The power spectral density and activity were respectively extracted as original features using Auto-regress model and Hjorth algorithm with different time windows. Two separate approaches processed the features: ensemble learning and deep learning. For the ensemble learning, a deep forest transformed the original features to new features that potentially improve feature engineering and a support vector machine (SVM) that was applied as classifier. For deep learning method, we added spatial information of EEG caps to both features by image conversion and adopted convolutional neural network (CNN) to recognize them. The performance of both methods was evaluated for separated and total frequency bands. As a result, the best accuracy obtained was 89.02% when we used the ensemble model and power spectral density. The best accuracy of deep learning method was 84.75% using the activity. These experimental results prove the efficiency of the proposed methods and show that EEG could be used as a reliable indicator for depression recognition, which makes it possible for EEG-based portable system design and application in auxiliary depression recognition in the future.","Brain topographic maps of the activity and power spectral density in Fig. 11, Fig. 12 exhibit interhemispheric electrophysiological asymmetry in brain regions. It is consistent with the results of previous studies [56] that interhemispheric electrophysiological asymmetry related to depression. Previous research [57,58] suggested temporal lobe was correlated with depression. In Fig. 11, Fig. 12, we can conclude that there are significant differences between normal and pathological EEG signals in the temporal lobe for alpha and theta frequency bands. The statistical results further indicate that there are significant differences between depression and normal subjects. Therefore, the activity and power spectral density can provide beneficial information in depression recognition.
In this paper, for EEG signal analysis, we focus on the role of three different aspects of EEG (i.e. temporal, spectral, and spatial information). We extracted two features: time domain feature (i.e. activity) and frequency domain feature (i.e. power spectral density) to construct feature vectors as input data. Meanwhile, two approaches were used for EEG-based depression recognition: ensemble model and deep learning. We firstly used these input vector in ensemble learning method. However, we ignored the spatial information supplied by the electrode cap. In order to utilize the spatial information, we converted these feature vectors into images. But the image didn’t match the input format of ensemble learning method. We used deep learning method to deal with this problem.
For ensemble learning method, we proposed an ensemble model which is the combination of deep forest and SVM. The major advantage of this method is that the ensemble model gives us better representations of original features which can improve the classification accuracy. We used deep forest to generate new features for following reasons. Firstly, the traditional classifiers have weak expression ability in the face of high-dimensional features, but the ensemble model of decision trees has stronger expression ability which can better find effective features and feature combinations [59]. Secondly, deep forest is self-adaptive [41]. It can automatically determine whether to continue training or not, based on if there are significant performance gains in the training process. Additionally, compared with the conventional methods where different types of feature vectors are directly concatenated together and fed to a classifier, we achieved the transformation for each original feature vector. For example, following the training strategy 1 of Fig. 7, to generate new feature (see Fig. 13), we used four feature vectors: the power spectral density of total frequency band with 0.2 s, 0.5 s, 1 s, and 2 s time window. In Fig. 13, for comparison between map (1–4) and map (5), we obviously notice that the new feature can better distinguish two types of instance. The same conclusion can also be drawn in other training strategies (see Fig. 8, Fig. 10).
For deep learning method, in comparison with the traditional machine learning methods which always consider the features of time domain and frequency domain of EEG signals, our work added the spatial information of EEG signals to the process of depression recognition. To add spatial information into EEG signals, we applied AEP method for coordinate transformation and used Clough-Tocher scheme for interpolation to generate images. Then, we obtained images contained spatial information and used CNN and dense layer as the feature extractor and classifier. The main advantage of this method is that EEG image combined with neural network model gives us a new perspective to analyze EEG signals. AEP method is a distance-based projection method. Compare with related research [60] for non-distance method based on EEG, using AEP method preserves the topology structure of electrodes, it means more information is retained in final EEG images. However, AEP method only retained the distance between the electrode and the center electrode which would affect subsequent interpolation. For non-distance method, there are no coordinate transformation and interpolation, so it has lower time cost, but reduces the related information in final EEG images.
As already noted, depression recognition is a subject-independent case. A considerable part of the research ignores this case. The results of those research are usually much better than the real results. For example, in our previous research [61], in the subject-dependent case, the best accuracy is 86.80% on the total frequency band. In the subject-independent case, the accuracy drops 10%. In this paper, a total of 28 subjects’ EEG data were used in our work. In order to avoid incorrect results which are higher than the actual result, we strictly divided the training set and testing set according to the subjects. Samples of one subject appeared either in the training set or testing set.
In the process of ensemble learning method, we applied different training strategies to get better classification accuracy. From Table 8, we can see that the best result we obtained is 89.02% (total, number of features: 1632) using the power spectral density. Most of accuracies are higher than 84% except for beta and theta frequency bands. The best accuracies of separated frequency bands are 87.87% (alpha, number of features: 256), 74.02% (beta, number of features: 256), and 69.10% (theta, number of features: 544). In comparison with the results of the baseline method, the performance of the ensemble model is better than the baseline method (about 3%˜7% better), which indicates that the ensemble model can automatically extract the effective representations and enhance the performance. Meanwhile, some researchers also used the combination of decision tree to find better feature representations in the Kaggle competition [62].
For deep learning method, we converted EEG signals into images contained spatial information. The best accuracy is 84.75% (alpha, number of features: 128) using the activity. Most of accuracies are more than 80% except for beta and theta frequency band. The best accuracies of beta and theta frequency bands are 67.73% (number of features: 128), 66.00% (number of features: 128). Compared with the results of the baseline method, the performance has not improved significantly. It may be caused by the structure of the neural network is not ideal enough or the parameters of the neural network are not accurate. As stated above, AEP preserved the distance between electrodes and central electrode but didn’t preserve any information between other electrode pairs which may be one reason as well. It requires us to conduct further exploration. Currently, there are more and more studies that use deep learning method for EEG analysis. Song et al. adopted CNN and RNN to classify grid-like frames which were encapsulated by the multi-channel EEG signals (72.06%/74.12%) [63]. Carabez et al. applied CNN model for P300 identification (86%/85.8%/84.8%). The CNN model is given a novel single trial three-dimensional (3D) representation of EEG data as the input [60]. Acharya et al. proposed a CNN model for EEG-based screening of depression and attained better accuracies which were 93.5% and 96.0% using EEG signal from the left and right hemisphere, respectively [23]. All these studies prove using deep learning method for EEG analysis is an effective method. We expect to keep finding better structures and parameters to improve its performance as future work.
From Table 8, we also find that the classification accuracies of both features (the power spectral density and the activity) are approximate. The number of features has no noteworthy effect on the performance of both methods, e.g. there is about 0.45% better (87.87%, alpha, “EMFT” vs 87.42%, alpha, “EMFF”), but the number of features differs by 288. The performance of both methods is related to the frequency band used. Previous studies have indicated there are significant differences between depression and normal controls on different frequency bands [64]. For both methods, we noticed that the accuracy of alpha frequency band is better than beta and theta frequency bands. It is consistent with the previous studies. Additionally, we also computed sensitivity, specificity, positive predictive value, and negative predictive value as performance measurements for the ""best strategy"" for each method (see Appendix A). As indicated by these results of both methods, using features which are further transformed is a good solution to recognize depression. These experimental results also show that EEG could be used as a reliable indicator for depression recognition, which makes it possible for EEG-based portable system design and application in auxiliary depression recognition in the future. Currently, the diagnosis of depression almost exclusively depends on doctor-patient communication and scale analysis. The proposed method can reduce the time of diagnosis and make the diagnosis more objective. It could be an effective way to assist in diagnosing depression patients at early stages and identify potential patients as early as possible. Thus, the patient can get an early treatment without delay. All proposed methods can run automatically and can be deployed on remote server. It meets the requirements of the portable system. Psychiatrists can connect to remote server to upload data and obtain analytical results. To deal with the problem of data collection, our team has developed the pervasive three-electrode EEG acquisition system [65,66], which runs on rechargeable battery and transmits all the EEG data through Bluetooth 2.0 wirelessly. Therefore, psychiatrists can synchronize data through mobile devices (i.e. phone, laptop, etc.)."
Using machine learning-based analysis for behavioral differentiation between anxiety and depression,"Anxiety and depression are distinct—albeit overlapping—psychiatric diseases, currently diagnosed by self-reported-symptoms. This research presents a new diagnostic methodology, which tests rigorously for differences in cognitive biases among subclinical anxious and depressed individuals. 125 participants were divided into four groups based on the levels of their anxiety and depression symptoms. A comprehensive behavioral test battery detected and quantified various cognitive–emotional biases. Advanced machine-learning tools, developed for this study, analyzed these results. These tools detect unique patterns that characterize anxiety versus depression to predict group membership. The prediction model for differentiating between symptomatic participants (i.e., high symptoms of depression, anxiety, or both) compared to the non-symptomatic control group revealed a 71.44% prediction accuracy for the former (sensitivity) and 70.78% for the latter (specificity). 68.07% and 74.18% prediction accuracy was obtained for a two-group model with high depression/anxiety, respectively. The analysis also disclosed which specific behavioral measures contributed to the prediction, pointing to key cognitive mechanisms in anxiety versus depression. These results lay the ground for improved diagnostic instruments and more effective and focused individually-based treatment.","The current study sought to differentiate between subclinical levels of anxiety and depression by detecting if a unique pattern of biased reactions to emotional stimuli exists for each disorder, based on participants’ aggregated performance in several behavioral tasks. Attention, memory, interpretation and expectancy biases were examined, as well as cognitive control ability. Data were analyzed by machine-learning tools that predicted the group membership of each participant according to his or her performance on the tasks. The analysis reached 71% and 70% sensitivity and specificity, respectively, and 68% and 74% classification accuracy of the HD and HA groups, respectively, in a two-group model. These classification accuracies were all above chance level. These results show the importance of combining behavioral measurements and machine-learning methods in the field of psychiatric diagnostics. Specifically, we suggest that machine-learning analysis tools may help reach higher confidence during the diagnostic procedure. In contrast to current diagnosis, which is based on self-reported symptoms, the cognitive tasks examine behavioral measures such as subtle differences in reaction time, which are less prone to self-report biases. Furthermore, the battery provides a comprehensive picture of the characteristic cognitive patterns, which can serve for future development of individually-tailored treatments. In the following, we elaborate and discuss the possible meaning and implications of the findings, as well as the novel implementation of machine-learning analysis tools in the context of cognitive biases in psychiatry. Further, we present the limitations of the current study and offer future research directions.

The importance of multiple measure-based analysis
Notably, measures from all the tasks included in the test battery contributed to the categorization prediction. Thus, the ability to predict group classification would have been lower without the representation of these various functions. This finding demonstrates the existence of deficiencies in different cognitive mechanisms among depressed and anxious individuals. Such deficiencies are reflected in an increase in explained variance when inserting measures from several cognitive categories. Furthermore, as seen in Figs. 5 and 6, both overlap and variance exist between the behavioral measures that contributed to the prediction as well as in the degree of their contribution in each model. Therefore, differences between anxiety and depression cannot be based on a single cognitive function, but must be looked for in a combination of performance patterns in different functions. Machine-learning analyses enable us to connect the data from different bias categories, to comprise a united pattern characterizing each group.

The contribution of automatic processing to the prediction
The results of the current study support the notion that different levels of automaticity of processing differentiate between anxious and depressed individuals. Measurements of different levels of automaticity in the same bias category were each found to contribute to the prediction (e.g., both implicit and explicit memory).

The behavioral test battery was developed in a manner that allowed for collection of data on both automatic and more elaborated reactions. The literature suggests that biases toward emotional stimuli are mostly automatic among anxious individuals, but not so among depressed individuals, whose biases are typically not automatic. Rather, their occurrence requires longer processing and further elaboration of the stimuli (18,19, see46,47,48 for various definitions of automatic processing). Several reviews have summarized findings regarding the differences in cognitive processing between anxiety and depression. They found that the automaticity levels contribute to the differentiation to some extent14.

Bagged decision tree classification results
Differentiation between high and low levels of anxiety and depression symptoms
The results of the bagged decision tree classification, in the symptomatic vs. non-symptomatic models, point to the algorithm’s high classification sensitivity as well as to the strong impact of depression and anxiety symptoms on cognitive mechanisms. Based on participants’ cognitive biases toward emotional stimuli, the algorithm displayed an excellent ability to differentiate between individuals with low levels of anxiety and depression symptoms (the LAD group) and individuals who exhibit high levels of anxiety and/or depression (the HA, HD, and HAD groups). From a clinical point of view, these results show the impact of subclinical levels of depression and anxiety symptoms on the everyday life of individuals suffering from anxiety or depression characteristics. These individuals are likely to cope with the consequences of their biased processing patterns, and may not seek psychological or psychiatric treatment since they are not clinically diagnosed.

Differentiation between depression and anxiety symptoms
The results obtained from the two-group model are of diagnostic importance. The battery of the behavioral tests that was developed for the current study, along with the machine-learning algorithm, were able to classify participants to the HD and HA groups 18–24% more accurately than chance level, based solely on their behavioral reactions. Even though the population in the current study is not clinically diagnosed, these results lay the ground for future studies. Currently, psychiatrists worldwide rely exclusively on self-reported symptoms to make a differential diagnosis of anxiety and depression49. To reach higher confidence in the diagnosis, additional aids alongside symptom-based diagnostic tools should be developed.

Limitations and future directions
The main limitation of the current study is the rather small sample, when considering each group separately. This drawback is especially relevant as the findings were not validated by an external dataset but by using an internal validation procedure on a random 20% of the data in each iteration (see, for example50 for evidence that machine-learning studies with small samples that were not validated in external datasets resulted in decreased prediction accuracy). Future studies are planned to validate the findings in external datasets. Recruiting participants with high (albeit subclinical) levels of anxiety and/or depression symptoms is a difficult task, for two reasons: First, such participants refrain from participating in experiments compared to subjects with low anxiety or depression levels. Second, the occurrence of high levels of anxiety and depression symptoms in a normal population is rather low (e.g., 5.2% and 5.8% of participants were found with severe or extremely severe levels of anxiety and/or depression symptoms, respectively, in a study among a healthy population51). Therefore, in our study, only 59 participants with high levels of anxiety and/or depression were recruited, out of more than 400 volunteers that filled out the screening questionnaire. These 59 participants were then divided into three relatively small groups. Further, a minor percentage of the data was missing due to technical errors (see “Supplementary Material”). Data were missing, however, occurred in a very small proportion of the overall data. Furthermore, missing data were evenly distributed between the groups. Therefore, it is unlikely that this missing data had a significant effect on the results.

Another limitation is that the study was performed in a subclinical sample. Therefore, conclusions regarding clinical diagnosis and treatment should be drawn with caution. The findings should be interpreted as a proof-of-concept for the combination of the cognitive battery with machine-learning classification analysis. Considering the above, future studies may examine the tools developed here in clinical populations. Such studies will determine the efficacy of the current methodology as a decision support system during psychiatric diagnosis.

Similar to other studies using machine-learning classification, another limitation lies in the fact that the cognitive battery is validated using questionnaires, which are inherently prone to self-report biases and are not 100% accurate. This bias, however, is mitigated by the fact that the participant’s classification is not evaluated solely on their answer to the questionnaires, but also on how it compares to all previous questionnaires completed by all participants and scored thus far.

In a future study and following additional validation in clinical samples that are diagnosed based on a comprehensive interview with a psychiatrist, the cognitive tasks would be evaluated against a questionnaire as well as a clinician interview. This would increase the robustness of the method.

The conclusions drawn from exploring the specific reaction patterns presented by individuals suffering from each disorder may in the future produce clinical benefits. They could be of great help, for instance, if implemented in cognitive training such as cognitive bias modification (CBM). CBM seeks to modify cognitive processes into being more adaptive to daily life7, and was found to improve psychopathological symptoms52. Shani et al.53, reviewing the cognitive training literature, surmised that its efficacy is highly affected by the intervention selection—a central approach in personalized medicine54. Intervention selection aims to optimize intervention efficacy by identifying the most beneficial type of intervention for a given individual. Machine-learning approaches may be highly suitable for such identification. They enable the selection of the items that contribute the most to a treatment, without relying on a specific theory. In a recent study55, implementation of the variables that were found to increase treatment efficacy by machine-learning algorithms indeed resulted in improved treatment.

In sum, employing a battery of cognitive–behavioral tasks analyzed by machine-learning algorithms developed for the current study, we show the importance of advanced analysis methods based on multiple measures to better characterize subclinical psychiatric disorders. The algorithm developed in the current study differentiated between participants with high levels of anxiety and/or depression symptoms and participants with low symptoms levels, as well as identified specific reaction patterns exhibited by depressed and anxious individuals. These observations pave the way for studies aimed at finding classes that stem from cognitive–behavioral data that may indicate these (and possibly other) disorders. Further, these findings lay the foundation for the development of more specific and refined diagnosis and clinical treatments, to be supported by future studies that will further expand the growing knowledge in the field of cognitive biases among psychopathological populations."
Predicting anxiety and depression in elderly patients using machine learning technology,"Anxiety and depression are two important mental health problems among the geriatric population. They are often undiagnosed and directly or indirectly responsible for various morbidities. Early and timely diagnosis has immense effect on appropriate management of anxiety and depression along with its co-morbidities. Owing to time constraint and enormous patient load, especially in developing county such as India it is hardly possible for a physician or surgeon to identify a geriatric patient suffering from anxiety and depression using any psychometric analysis tool. So, it is of utmost importance to develop a predictive model for automated diagnosis of anxiety and depression among them. This Letter aims to develop an appropriate predictive model, to diagnose anxiety and depression among older patient from socio-demographic and health-related factors, using machine learning technology. Ten classifiers were evaluated with a data set of 510 geriatric patients and tested with ten-fold cross-validation method. Highest prediction accuracy of 89% was obtained with random forest (RF) classifier. This RF model was tested with another data set from separate 110 older patients for its external validity. Its predictive accuracy was found to be 91% and false positive (FP) rate was 10%, compared with gold standard tool.","Machine learning technology is a new promise for the development of an automated disease diagnostic system. Researchers from both medical and engineering fields are trying to address this issue together. In this Letter, an endeavour has been made to predict one of the most important age-related mental changes, i.e. anxiety and depression among the older people, from socio-demographic and medical factors, using machine learning technology. A considerably large number (510) of elderly patients were interviewed and screened for anxiety and depression. This data set was used for predictive modelling. Study conducted by Suhasini et al. [37] tried to predict mental health problems of 400 psychiatric patients, not exclusively on older patients, using machine learning technology. However, this kind of study specially focusing on elderly patient is hardly available in literature. Study conducted by Bhakta and Sau [38] on prediction of depression among elderlies living in a slum at Kolkata found that among four different classifiers (BN, logistic, MLP, and SMO) BN and SMO algorithms predicted depression with an accuracy of almost 90%. In this Letter, at first relevant features were selected using attribute evaluator in WEKA. Ten features were found to be effective. Then, ten machine learning classifiers were evaluated and RF had the highest predictive accuracy with ten-fold cross-validation test. This RF model was tested on another 110 elderly patients for its external validity. Its predictive accuracy was found to be 91% and FP rate was only 10%, compared with gold standard HADS. This Letter will pave the way for future research work in search for better classifier with appropriate features especially suitable for the geriatric patients. Moreover, this can be extended to identify older persons suffering from underlying mental health problems, from general population at community level."
The application of machine learning in depression,"Because of the lack of disease awareness in depressed patients and the lack of early screening methods, most patients had developed to major depressive disorder when they were first diagnosed with depression. In order to improve the current situation, machine learning has been gradually used in some aspects of depression recent years, including early prediction, early recognition, auxiliary diagnosis, and treatment. In the application, the factors that affect the accuracy of machine learning model include the type and size of sample set, feature engineering, algorithm type, etc. In the future, machine learning should be further integrated into the health care system and mobile applications, continuously optimizing the machine learning model, fully mining patient health data to improve depression-related problems in terms of the, identification, diagnosis, treatment and so on on.","6.1 Small sample size
Using machine learning to establish a predictive model with high accuracy and strong generalization ability requires a sufficient sample size. However, current domestic related research is limited by insufficient project funds and manpower investment, incomplete hospital and community health systems, and lack of uniformity among hospitals. The influence of factors such as the standardized medical information management system led to the failure to obtain sufficient sample size during data collection. Compared with China, the U.S. health system, such as the Omaha System based on standardized nursing language, is relatively complete, and patients' health information can be shared among hospitals, making it easier to collect larger data sets and use machine learning for analysis. Data analysis ( Gao, Calhoun, & Sui, 2018 ). Therefore, in the future, our country can improve a unified and standardized patient health information database to achieve multi-site data sharing among hospitals, hospitals-communities, etc., to better mine these health data, improve the generalization ability of the established model, and enable It is suitable for a broad spectrum of people.

6.2 The prediction model needs further optimization
The prediction accuracy of the model is affected by features, algorithms, parameters, etc., so it requires continuous optimization to achieve the optimal model. The training algorithm is the core of building a model, so the algorithm based on which to train is the key. For the same problem, the same sample set, prediction models based on different algorithms will show different prediction accuracy ( Karhade et al., 2019 ; Hasanzadeh, Mohebbi, & Rostami, 2019 ), and each algorithm has different applicable scope and characteristics. . The systematic review by Lee et al. (2018) shows that for the same problem to be solved, different types of sample sets will also affect the prediction results, and the selection of predictors is also an important factor affecting the accuracy of the model. Therefore, future research needs to consider various aspects such as the data type applicable to each algorithm, the function of the algorithm, time and economy, make an accurate judgment on what algorithm is suitable for your own research, conduct training based on appropriate algorithms, and continue to explore. The best predictors are determined, and parameter tuning iterations are performed to obtain the optimal prediction model.

6.3 Lack of medical information talents
In today's information age, a large amount of health data is generated every day. Medical staff in clinical and community hospitals input patient data into electronic medical records during their daily shifts. Medical instruments, biosensors, wearable devices, and mobile devices transmit patients’ vital information in real time. The data contains a large amount of information about patients, health care trends, and nursing care. Phenomenon and other rich information also contains a lot of depression-related information that needs to be mined. Mining these data requires certain informatics technologies, but medical informatics talents are now very scarce in China ( Ma Liming, Huang Hao, 2018 ). There is an urgent need to focus on the cultivation of medical informatics talents and improve their ability to use machine learning and other technologies to process large amounts of complex data. Ability. At the same time, the development of multidisciplinary cooperation models should be strengthened to promote cooperation between talents in computer information disciplines and medical disciplines. This interdisciplinary research model will further promote the development of psychology and medicine.

6.4 Clinical practice rate is low
The use of machine learning can reveal hidden trends in patient conditions and treatment results in health data, assist medical staff in making data-driven decisions, and improve patient treatment and care. However, most current research remains on the theoretical basis of model establishment, without combining other computer technologies and hardware to generate runnable platforms or applications. Easy-to-operate applications and platforms can facilitate individuals or primary health departments to know disease risks in advance through simple data input ( Jimenez-Serrano, Tortajada, & Miguel Garcia-Gomez J, 2015 ), do a good job in disease prevention, and reduce medical costs. Assisting doctors in diagnosing depression and selecting personalized treatment plans, it also helps reduce the burden on doctors. Therefore, in the future, machine learning can be combined with Internet of Things technology, cloud computing, big data, computer software and hardware, etc., to develop a complete clinical auxiliary decision-making system.

In summary, machine learning has been gradually used in early screening, auxiliary diagnosis, and auxiliary treatment decision-making in the field of depression. However, there are still obstacles and shortcomings in application, such as small sample size, lack of medical informatics talents, and low clinical practice rate. Therefore, in the future, our country should focus on the unified storage of medical and health information, strengthen the training of medical informatics talents, promote research on machine learning in the field of depression, continuously optimize and construct depression prediction models, and gradually apply them in clinical practice."
"Fusing data mining, machine learning and traditional statistics to detect biomarkers associated with depression","Background
Atheoretical large-scale data mining techniques using machine learning algorithms have promise in the analysis of large epidemiological datasets. This study illustrates the use of a hybrid methodology for variable selection that took account of missing data and complex survey design to identify key biomarkers associated with depression from a large epidemiological study.

Methods
The study used a three-step methodology amalgamating multiple imputation, a machine learning boosted regression algorithm and logistic regression, to identify key biomarkers associated with depression in the National Health and Nutrition Examination Study (2009–2010). Depression was measured using the Patient Health Questionnaire-9 and 67 biomarkers were analysed. Covariates in this study included gender, age, race, smoking, food security, Poverty Income Ratio, Body Mass Index, physical activity, alcohol use, medical conditions and medications. The final imputed weighted multiple logistic regression model included possible confounders and moderators.

Results
After the creation of 20 imputation data sets from multiple chained regression sequences, machine learning boosted regression initially identified 21 biomarkers associated with depression. Using traditional logistic regression methods, including controlling for possible confounders and moderators, a final set of three biomarkers were selected. The final three biomarkers from the novel hybrid variable selection methodology were red cell distribution width (OR 1.15; 95% CI 1.01, 1.30), serum glucose (OR 1.01; 95% CI 1.00, 1.01) and total bilirubin (OR 0.12; 95% CI 0.05, 0.28). Significant interactions were found between total bilirubin with Mexican American/Hispanic group (p = 0.016), and current smokers (p<0.001).

Conclusion
The systematic use of a hybrid methodology for variable selection, fusing data mining techniques using a machine learning algorithm with traditional statistical modelling, accounted for missing data and complex survey sampling methodology and was demonstrated to be a useful tool for detecting three biomarkers associated with depression for future hypothesis generation: red cell distribution width, serum glucose and total bilirubin.
","The strength of this hybrid methodology over other variable selection methods is the potential to adequately handle missing data and complex survey samples using a sound and systematic multi-stepped approach to variable selection. The application of the data mining knowledge discovery process to large epidemiological studies allows researchers to include a large array of data (i.e. variables and observations) to be investigated to generate hypotheses that may have been otherwise overlooked. This is probably true for this NHANES study, particularly in regard to the total bilirubin finding.

The data mining method of splitting data files into training and validation minimises issues of overfitting that is often problematic in traditional statistical techniques with a large number of predictors. The boosted machine learning technique can accommodate different types of variables and has been found to have high predictive accuracy, with shrinkage also used to avoid over-fitting. The iterative learning nature of the algorithm provides researchers with reasonable confidence in the results with its boosted handling of residuals at each iteration. The relative importance measure produced by this technique has been demonstrated to be more effective than the traditional coefficient measures produced by lasso regularized regression and stepwise regression.

A limitation of the methodology is the potential computing power required to perform the machine learning techniques when implementing a small shrinkage parameter. In addition, this study implemented the recommended bagging and number of iterations for the machine learning boosted regression algorithm, but it may be appropriate in the future to run the algorithm on a number of different bagging percentages and number of iterations.

A limitation of this methodology is the complexity of the implementing a multi-stepped variable selection approach compared to a simpler single-stepped variable selection approach. However, unlike simpler single-stepped variable selection procedures such as stepwise regression and regularized regression this multi-stepped method can accommodate missing data using multiple imputation combined with complex survey designs.

The NHANES study used in the example for this hybrid methodology is a large cross-sectional study that contains both missing data and utilises a complex four-stage sampling methodology. The limitations of this type of data restrict the ability to infer the direction of the relationship between the key biomarkers and depression. The self-report instrument of depression, the PHQ-9, may have missed less severe cases of depression [19, 21]. It is also recognised that the imbalance of depressive symptoms in the data set may have resulted in a prediction bias towards the major classes [86].
"
Systematic misestimation of machine learning performance in neuroimaging studies of depression,"We currently observe a disconcerting phenomenon in machine learning studies in psychiatry: While we would expect larger samples to yield better results due to the availability of more data, larger machine learning studies consistently show much weaker performance than the numerous small-scale studies. Here, we systematically investigated this effect focusing on one of the most heavily studied questions in the field, namely the classification of patients suffering from Major Depressive Disorder (MDD) and healthy controls based on neuroimaging data. Drawing upon structural MRI data from a balanced sample of N = 1868 MDD patients and healthy controls from our recent international Predictive Analytics Competition (PAC), we first trained and tested a classification model on the full dataset which yielded an accuracy of 61%. Next, we mimicked the process by which researchers would draw samples of various sizes (N = 4 to N = 150) from the population and showed a strong risk of misestimation. Specifically, for small sample sizes (N = 20), we observe accuracies of up to 95%. For medium sample sizes (N = 100) accuracies up to 75% were found. Importantly, further investigation showed that sufficiently large test sets effectively protect against performance misestimation whereas larger datasets per se do not. While these results question the validity of a substantial part of the current literature, we outline the relatively low-cost remedy of larger test sets, which is readily available in most cases.","Sparked by the observation that machine learning studies drawing on larger neuroimaging samples consistently showed weaker results than studies drawing on smaller ones, we drew samples of various sizes from the PAC dataset, thereby mimicking the process by which researchers would draw samples from the population of ML studies reported in the literature. When applying a linear SVM with LOOCV, as is the most common approach in the neuroimaging literature [10], we observed a higher risk of misestimations, which may lead to artificially high performance estimates in smaller samples. Importantly, our analyses revealed that this is primarily due to a small test set, not training set size. Generally, this shows that a small test sample size may explain many of the highly optimistic results published in recent years. When considering the well-established effect of publication bias, even if underestimated results are equally as likely, they will have a significantly lower chance of being published.

Our results are the first to disentangle the effects of training and test set size effects which are typically inseparable in common cross-validation frameworks such as LOOCV. This delineation of effects enabled two important insights for biomarker discovery and outcome prediction. First, researchers need to validate their models on large, independent test sets. Our results indicate that in the PAC dataset, a test set size of N = 100 was already sufficient to lower the probability of obtaining artificially good performance (i.e., 70% or higher) to 13%. With a median N of less than 100 in many published studies [10], this may seem challenging. However, online infrastructure for independent machine learning model evaluation is available (e.g., www.photon-ai.com/repo). If researchers open-source their models, anyone—independent of technical knowledge or machine learning expertise—can evaluate them. This way, large independent test datasets can be obtained in a short time without the need for data sharing. This is not restricted to neuroimaging data, but any machine learning model. In addition, efforts from consortia will also help to mitigate this problem and should be considered by machine learning practitioners.

Second, the size of the training set alone cannot serve as an indicator of later model performance. Larger training sets are more likely to generalize to new data and broaden model scope (i.e., about which groups within a population a given model can make reasonable predictions), however, in the current analysis, the linear rule learned by an SVM on high-dimensional neuroimaging data could be approximated with only a handful of samples. From a training set size of 30 onward, we no longer observed any increase in model performance. This somewhat counterintuitive effect arises whenever a simple rule is approximated. For higher complexity models (i.e., models capable of learning more complex rules), we, of course, expect performance increases as training sample size increases. However, considering the results of the PAC competition (Supplementary appendix B), high complexity models such as Deep Learning approaches did not yield higher performance when trained with ~1000 samples. Thus, we conclude that simple models are competitive for sample sizes of up to N = 1000 for this particular classification problem. Whether more complex rules can be discovered beyond this point or whether other fundamental issues hamper biomarker discovery in psychiatry (cf. e.g., biotype identification [19] and normative modelling approaches [23]) remains an open question.

An intuitive criticism of our main analyses would be that it has merely replicated methods similar to those of previous low-quality works (for example, studies using only linear SVMs with default parameters, tested in LOOCV schemes, on samples with many more predictors than observations). Whilst this pipeline configuration was used in the current analysis to (a) hold constant properties, that if varied, may have been indistinguishably responsible for changes in accuracy misestimation, and (b) replicate the most commonly used ML pipeline configuration in our field, it was important to conduct complementary analyses to rule out these confounds. Therefore, we tested a further 48 ML pipeline configurations (see Supplementary appendix F) using both linear (a linear SVM) and non-linear (a radial basis function SVM and a Random Forest) classifiers. In addition, we conducted PCA based dimensionality reduction as well as f-test based feature selection within these classifiers to delineate whether our findings were confounded by the large size of the predictor space relative to our number of observations. Importantly, all pipeline configurations demonstrated the same pattern of systematic misestimation as that in our main analyses. The second potential criticism of the current work is that these findings may merely be modality-specific, limiting the generalizability of these findings across domains. However, the use of a dummy classifier that completely ignored the input predictor space (the voxels), and instead, classified samples based only on their prevalence in training (MDD = 50%, Control = 50%), showed the same pattern of sample size based systematic misestimation across all pipeline configurations, thus, demonstrating a generalizable statistical effect regardless of the data modality used.

Given the profound effect of the test set size on systematic misestimation, it is important to consider why an effect of overestimation may arise. Previous work by Schnack and Kahn [24] suggests that patient characteristics in smaller samples tend to be more homogenous. In the case of small N, participants may be more likely to be recruited from the same data collection site and of a similar age (for example, in the case of a university recruited convenience sample). In addition, stringent recruitment criterion may be easily met, resulting in a well-defined phenotype that is not truly representative of the parent population of interest. Whilst this explanation makes sense for samples collected in this manner, it fails to explain why we observed this phenomenon in our random sampling procedure, and more importantly, with our dummy classifiers that paid no attention to participants, their characteristics, or the inputted predictor variables. This observation suggests a mechanism for systematic misestimation that is not just sample/patient-specific or contingent on sample homogeneity, but instead, inherent in the natural variation that arises in small test samples. Indeed, this effect is known as sampling error, and as demonstrated by Combrisson et al. [25] can lead to an effect whereby we exceed a machine learning model’s chance level, purely by chance.

In addition to sample size, other issues such as data leakage [13], are likely contributing to the systematic overestimation seen in the literature. Dedicated cross-platform software to help avoid data leakage is freely available (e.g., PHOTON, www.photon-ai.com or Scikit-learn [22]). Finally, code should be made available on request or provided in an online repository (e.g., GitHub or GitLab) upon submission for review. In addition, a more elaborate evaluation framework including the analysis of model scope assessment as well as incremental utility and risk analysis is needed to move the field beyond proof-of-concept studies. The success of current translational efforts in neuroimaging and psychiatry will crucially depend on the timely adoption of guidelines and rules for medical machine learning models (for an in-depth introduction, see [15]).

In summary, our results indicate that—while many of the most highly published results might strongly overestimate true performance—evaluation on large test sets constitutes a straightforward remedy. Given that simple, low-complexity models such as linear SVMs did not gain from larger training set size, researchers should not discard their models due to low training N but seek evaluation on a large test set for any model showing good performance.

Data access and responsibility
All pIease take responsibility for the integrity of the respective study data and their components. All authors and coauthors had full access to all study data."
"Predicting the naturalistic course of depression from a wide range of clinical, psychological, and biological data: a machine learning approach","Many variables have been linked to different course trajectories of depression. These findings, however, are based on group comparisons with unknown translational value. This study evaluated the prognostic value of a wide range of clinical, psychological, and biological characteristics for predicting the course of depression and aimed to identify the best set of predictors. Eight hundred four unipolar depressed patients (major depressive disorder or dysthymia) patients were assessed on a set involving 81 demographic, clinical, psychological, and biological measures and were clinically followed-up for 2 years. Subjects were grouped according to (i) the presence of a depression diagnosis at 2-year follow-up (yes n = 397, no n = 407), and (ii) three disease course trajectory groups (rapid remission, n = 356, gradual improvement n = 273, and chronic n = 175) identified by a latent class growth analysis. A penalized logistic regression, followed by tight control over type I error, was used to predict depression course and to evaluate the prognostic value of individual variables. Based on the inventory of depressive symptomatology (IDS), we could predict a rapid remission course of depression with an AUROC of 0.69 and 62% accuracy, and the presence of an MDD diagnosis at follow-up with an AUROC of 0.66 and 66% accuracy. Other clinical, psychological, or biological variables did not significantly improve the prediction. Among the large set of variables considered, only the IDS provided predictive value for course prediction on an individual level, although this analysis represents only one possible methodological approach. However, accuracy of course prediction was moderate at best and further improvement is required for these findings to be clinically useful.","Our findings indicate that from a wide range of clinical, biological, and psychological predictors, only severity of baseline depressive symptoms (measured by the IDS self-report questionnaire) was a significant predictor of different course trajectories of depression. We were able to predict the presence or absence of a unipolar depression diagnosis after 2 years with an AUROC of 0.66, and to discriminate between three course trajectory groups with an AUROC of 0.69 for rapid REM, 0.63 for gradual IMP, and 0.66 for a CHR course of depression.

Prediction of outcome in psychiatry is notoriously hard, due to heterogeneity of disorders, broad comorbidities across disorders, and due to clinical categories defined without a priori biological validity28. The performance of our models will need to improve in order to be translatable to clinical practice, but is comparable to previous ML studies predicting the naturalistic course of depression. For example, a study by Kessler and colleagues9 observed an AUROC of 0.63 for predicting high chronicity, defined as an episode lasting most days throughout the year, and AUROC’s between 0.71 and 0.76 for predicting other measures indicative of a 10- to 12-year illness course of depression, such as high persistence of MDD, hospitalization, and disability caused by MDD, and suicide attempts. Kessler et al.9 based their prediction models on baseline clinical measures alone, including symptoms of MDD and parental history of MDD, mania–hypomania, anxiety disorders, and externalizing disorders. The contribution of each individual clinical measure to the overall prediction was not assessed, so we cannot infer whether the prediction of their outcomes was also driven by severity of symptoms as observed in the present study.

Our most accurate models achieved a slightly lower AUROC of 0.69 for predicting an illness course characterized by rapid remission compared with the AUROCs found in Kessler et al.9, which is arguably less extreme (and therefore likely harder to predict) and a more prevalent outcome than outcomes considered by Kessler and colleagues9. Highest AUROC’s were found by Kessler et al.9 for models predicting hospitalization, disability, and attempted suicide, which was reported in only 3.2–5.8% of the total sample. However in our study, the prevalence of a remitted course of depression was 44%. Therefore, despite the smaller AUROCs, the positive predictive value (PPV) of our models is higher (between 33 and 68% PPV for a given outcome definition in the present study (Supplemental Figure S2), compared with PPV between 12.5% and 18.3% in the Kessler et al. study9 for 20% of subjects with highest predicted probability of a given clinical outcome), which means that our models have a smaller probability of false-positive classifications. Previous studies aimed at predicting first onset and new onset of an MDD episode during follow-up in people with no current MDD diagnosis using prediction algorithms based on demographic and clinical characteristics have found slightly higher AUROC of 0.75–0.79 in the training sample, which dropped to 0.70–0.73 AUC in independent replication samples12,13,29.

In addition to evaluating the predictive value of a model including all clinical, biological, and psychological variables, we aimed to identify (a minimum set of) individual predictors that reliably predict the naturalistic course of depression. For this purpose, we used a novel approach by combining penalized logistic regression and a stability selection method that selects predictive variables from a multivariate model while controlling for family-wise error. Of all included measures, we only identified the IDS as a statistically significant predictor. The total IDS score was positively associated with IMP and CHR course group membership, and with the presence of a depression diagnosis after 2 years, and negatively associated with a REM course and the absence of a depression diagnosis at follow-up. Although our method provides excellent control over type I errors, it is conservative and can miss predictive variables26. However, other variables only improved prediction of CHR course by 0.05 AUROC and all other outcome groups (presence of an MDD diagnosis, REM, IMP) by 0.01 AUROC. This is roughly equivalent to a difference of 0.04 Cohen’s d30, indicating a low added value of additional variables over and above the IDS. Interestingly, a subsequent exploratory analysis that we performed to identify which individual items of the IDS contributed most to the prediction showed that only the items “Feeling sad” (for predicting the presence of an MDD diagnosis at follow-up) and “Response of your mood to good or desired events” (for predicting the three different course trajectories) were identified as significant predictors. Performance of models using only these two items was similar to a model using all IDS items.

Similar results were found by Chekroud and colleagues10 in a recent study examining the predictive value of clinical measures for remission of MDD symptoms following a randomized 12-week citalopram treatment. Their model selected 25 best predictors from 164 sociodemographic and clinical features, and was able to predict remission with AUC of 0.70. Total severity of depressive symptoms, measured with the QIDS (shortened version of the IDS) was the most important predictor of treatment response. In line with the current study, treatment response could also be predicted with models using fewer variables, e.g., with only 15 and 10 variables with AUC of 0.69 and 0.68, respectively.

These findings suggest that other clinical measures possess very little or no prognostic value for course of depression—or remission following treatment in the Chekroud et al. study10—above and beyond severity of depressive symptoms. Biological variables, including inflammatory markers, cortisone, metabolic measures, BDNF, and vitamin D were able to predict only a chronic course of depression, although performance was worse than for clinical variables. This finding is in contrast to our previous studies within the same sample that showed group-level associations between lower cortisol awakening response7 and vitamin D deficiency6 and chronicity of unipolar depression. These findings show clearly that a group-level association does not imply the ability to make predictions for new cases at the level of individual subjects. This implies that although these baseline biological parameters can be associated with outcome based on group-level approaches, the effect sizes are probably too small to possess sufficient prognostic ability for long-term outcome in individual patients. In line with the current findings, in previous studies we found no group-level associations between a chronic course of depression and BDNF31, CRP, IL6 and metabolic syndrome32, despite clear group differences between healthy controls and unipolar depression patients. This may suggest that biological markers implicated in the etiology of unipolar depression are not necessarily good prognostic markers. Nonetheless, although we found no evidence for biological variables being informative for predicting naturalistic course of depression at the level of individual patients, they may still be useful for discriminating unipolar depression patients from other patient groups, e.g., bipolar disorder33, or for predicting response to, e.g., antidepressant treatment. Moreover, our course outcome definitions were based on DSM diagnosis and severity of symptoms. Symptom-based classifications are agnostic about underlying biological mechanisms and patients whose trajectory of symptoms is caused by different biological processes may be subsumed under the same category. As a consequence, our different course trajectory groups may have consisted of a heterogeneous set of patients with a similar course in terms of symptoms but distinct underlying pathophysiological mechanisms, and, hence, the full predictive power of biological variables may become only visible when patients are first stratified according to clinically relevant biological characteristics.

We previously showed promising results using task-based functional brain imaging34. This study was conducted in a smaller subsample (n = 118) of the dataset used here, with identical LCGA course trajectory definitions. In this study, models based on neural patterns of activation in response to emotional facial expressions could discriminate chronic patients from patients with more favorable trajectories with up to 73% accuracy and outperformed models based on other neuroimaging modalities (structural magnetic resonance imaging, task-based functional magnetic resonance imaging related to executive functioning with a chance level accuracy) or clinical data (accuracy of 69%). However, since the sample in our previous study was smaller, resulting in less stable results, and more homogeneous due to additional selection criteria, no strong conclusions can be drawn regarding the added value of neuroimaging data.

Limitations
The main limitation of the current study is a lack of replication of our findings in an independent dataset. Although within-sample cross-validation is known to be an approximately unbiased estimator of population generalizability35, it may not completely account for the different characteristics of data from different samples. An important next step is to validate our findings in independent data. An additional limitation is that due to the naturalistic setting of our study treatment was not controlled and limited information was available on treatment received during the follow-up period. The advantage of our naturalistic design is that the sample is more representative of depression in the general population. However, the prediction accuracy may have been higher in a more homogeneous and controlled sample. A final limitation of the study is that we tested only a one ML algorithm without the extensive tuning of all hyper-parameters. It is possible that a different analytic pipeline or an algorithm would yield slightly different predictions. We have done this mainly for the sake of simplicity so that stability selection is performed on the same algorithm that was also used to make predictions, and to avoid overly optimistic results due to model selection bias and overfitting36. Our results can, therefore, be considered as a conservative estimate of out of sample predictive accuracy."
Detecting and measuring depression on social media using a machine learning approach: systematic review,"Background:
Detection of depression gained prominence soon after this troublesome disease emerged as a serious public health concern worldwide.

Objective:
This systematic review aims to summarize the findings of previous studies concerning applying machine learning (ML) methods to text data from social media to detect depressive symptoms and to suggest directions for future research in this area.

Methods:
A bibliographic search was conducted for the period of January 1990 to December 2020 in Google Scholar, PubMed, Medline, ERIC, PsycINFO, and BioMed. Two reviewers retrieved and independently assessed the 418 studies consisting of 322 articles identified through database searching and 96 articles identified through other sources; 17 of the studies met the criteria for inclusion.

Results:
Of the 17 studies, 10 had identified depression based on researcher-inferred mental status, 5 had identified it based on users’ own descriptions of their mental status, and 2 were identified based on community membership. The ML approaches of 13 of the 17 studies were supervised learning approaches, while 3 used unsupervised learning approaches; the remaining 1 study did not describe its ML approach. Challenges in areas such as sampling, optimization of approaches to prediction and their features, generalizability, privacy, and other ethical issues call for further research.

Conclusions:
ML approaches applied to text data from users on social media can work effectively in depression detection and could serve as complementary tools in public mental health practice.","Our review aimed to outline studies that conducted depression detection with ML approaches based on text from social media. According to studies included in this review, researchers would extract features from online text users posted on social media using text analysis strategies such as LIWC and other word-embedding models. Next, the researchers input the features into ML models to conduct depression prediction. The features among the seventeen studies were all produced based on words in the online text, such as emotional information, linguistic style, temporal process information, social network features, etc. As for ML approaches used in depression predicting, SL was adopted more than UL. According to the above-mentioned studies [22,23], ML approaches achieved good accuracies for depression detection using text from social media, such as Facebook, Twitter, mic-blog, etc. Nevertheless, some studies also presented that there were several challenges with ML approaches [14,15,21], and problems of piracy and popularization were ongoing concerns [18].

It is worth noting that there are some common patterns in the studies reviewed here. In terms of depression identification, the existing studies analyzed in this review are consistent in that the researchers, in each case, first identified depressed and nondepressed groups among their subjects, according to either researcher-inferred mental status, user-declared mental status, community membership, or clinicians’ judgments and then explored ways of classifying the subjects using ML approaches, and measured the accuracies of the models’ predictions. Furthermore, most of the experiments reviewed here used SL rather than UL models. UL is used to identify unobserved or underlying psychological dimensions and explore how to achieve optimal classification, while SL uses existing information in the feature database for higher-level analyses concerning identification. SL uses classifications established ahead of time to explore ways to forecast a specific outcome of interest, such as the presence of a psychiatric disorder (eg, depression and anxiety). UL explores phenomena such as clustering and compression within sets of unlabeled data [33]. Therefore, for scenarios where prediction of a specific variable, such as depression, is the aim, SL approaches may be more accurate and efficient than UL approaches [12].

Several limitations and challenges of the depression identification and predicting models reviewed here should be acknowledged. Firstly, for depression identification, the fact that some types of information about the individuals, such as sociodemographic characteristics, behaviors behind the scenes, psychological, social, and cultural environment are often lacking in social media data and pose challenges that may be hard to resolve [29,31]. Secondly, the quantities of individual users’ posts vary greatly, and posts containing too few of the terms designated as relevant input could lead to bias in depression identification [15]. Moreover, all of the study samples of the 17 studies reviewed were from either China, Japan, the United States, or other English-speaking countries. As a result of cultural and other differences, populations from different countries tend to differ in terms of posting frequencies and content, which may also lead to bias in depression identification. The generalizability of measurement standards for depression is also limited across countries and cultures [27]. It should also be mentioned that the studies reviewed here all explored ML approaches to detecting depression using only text data from social media, which may have limited their predictive efficacy. Given that social media data can also include videos, photos, etc, it may be that including more types of social media data in analyses could make depression identification programs more powerful.

The challenges facing ML approaches for depression detection may, however, be resolvable. For example, existing studies indicate that homophily exists among depressed users; that is to say, friends who interact with depressed users frequently are more likely to have depressive symptoms themselves. Therefore, the interactions and ties between users are significant. But the data used in such prediction models tend to be widely scattered on social media, and it is difficult to analyze the connections among individuals in such a way as to improve the accuracies of the ML approaches [21]. Moreover, only a large-scale data set could facilitate high accuracy in predictive applications. However, due to the characteristics of the data, it is hard to collect a sufficiently large mass of data to optimize the ML approaches applied. Often the studies are conducted based only on several hundred subjects [7,27].

In addition, the approaches and features selected for the analyses are crucial aspects of studies in this area. Wider ranges of possible features, such as specific depression lexicons appropriate for particular cultural populations or groups, and more complex techniques for analyzing posts should be explored with a view to ameliorating experimental processes and improving the accuracies of models [7]. The study conducted by De Choudhury, Gamon, et al [25], for example, in addition to using principal component analysis to perform feature reduction, also employed data abstraction techniques such as entropy, variance, and an average of the features which were significantly helpful in identifying the effects of the methods used in the study. Some approaches, however, tend to have deficiencies in both generalizability and variables selected for measurement. For example, there was a study that ended up not identifying depressed users, but only depressive tendencies, as revealed in posts on social media, because of the methods they applied [17]. Finally, there tends to be bias in the detection of depression when ML approaches are applied to data from social media. We know, for example, that youth and middle-aged people tend to be more active on social media than young children and older adults [32]. It’s also true that there is a digital divide between people with higher and lower incomes [34], and people in more developed and richer countries and localities use social media more than those in poor and undeveloped areas, etc. What’s more, most older adults seldom go on the internet. For example, according to the Pew Research Center, only 22% of American adults report using Twitter, and 73% of those people are under the age of 50 years [35]. Therefore, we cannot obtain data from social media that will represent all groups, leading to inherent population biases in studies based on social media.

To improve the validity and feasibility of depression detection research based on the application of ML approaches to social media data, increased efforts to reduce research bias will be needed. For depression identification, researchers should employ criteria and tools for depression diagnosis that are both accurate and suitable for different online populations. Moreover, collecting personal information such as sociodemographic characteristics and behaviors behind the scenes should also be considered, where necessary and ethical [31]. Furthermore, on methods used for predicting, first, it is important to refine the prediction results by continually exploring optimal input features, models, and ML approaches through constant training and learning with larger-scale samples. Second, studies should focus on standardizing the measures being used for depression detection with ML approaches and on developing scalable approaches for automated tracking of public psychological health in the future. Third, to avoid estimate biases caused by small sample sizes, researchers should focus on obtaining samples that are as large as possible for their analyses. Finally, discussions about the issues involved in the studies should include computer scientists, psychologists, clinicians, ethicists, lawyers, policymakers, as well as user representatives from various user groups."
Machine learning approaches for integrating clinical and imaging features in late‐life depression classification and response prediction,"Objective
Currently, depression diagnosis relies primarily on behavioral symptoms and signs, and treatment is guided by trial and error instead of evaluating associated underlying brain characteristics. Unlike past studies, we attempted to estimate accurate prediction models for late-life depression diagnosis and treatment response using multiple machine learning methods with inputs of multi-modal imaging and non-imaging whole brain and network-based features.

Methods
Late-life depression patients (medicated post-recruitment) (n = 33) and older non-depressed individuals (n = 35) were recruited. Their demographics and cognitive ability scores were recorded, and brain characteristics were acquired using multi-modal magnetic resonance imaging pretreatment. Linear and nonlinear learning methods were tested for estimating accurate prediction models.

Results
A learning method called alternating decision trees estimated the most accurate prediction models for late-life depression diagnosis (87.27% accuracy) and treatment response (89.47% accuracy). The diagnosis model included measures of age, Mini-mental state examination score, and structural imaging (e.g. whole brain atrophy and global white mater hyperintensity burden). The treatment response model included measures of structural and functional connectivity.

Conclusions
Combinations of multi-modal imaging and/or non-imaging measures may help better predict late-life depression diagnosis and treatment response. As a preliminary observation, we speculate that the results may also suggest that different underlying brain characteristics defined by multi-modal imaging measures—rather than region-based differences—are associated with depression versus depression recovery because to our knowledge this is the first depression study to accurately predict both using the same approach. These findings may help better understand late-life depression and identify preliminary steps toward personalized late-life depression treatment. Copyright © 2015 John Wiley & Sons, Ltd.","In this study, we showed how nonlinear combinations of multi-modal MRI and/or non-imaging measures can successfully estimate prediction models for diagnosis (87.27% accuracy) and treatment response (89.47% accuracy) of LLD. The optimal prediction models for both outcome variables were strikingly distinct in nature with no overlap of selected features. Additionally, the diagnosis prediction model was network-independent, whereas the treatment response prediction model depended on information from the dDMN and aSN. In the succeeding text, we evaluate our findings further using past studies for comparison.

Optimal predictors/biomarkers
Late-life depression diagnosis versus treatment response prediction models
Non-imaging (i.e. Mini-mental state examination and age) and global volume-based imaging (i.e. whole brain atrophy and global WMH burden) measures combined were found to be the optimal predictors/biomarkers of LLD diagnosis. Agreeing with past studies, poor cognitive ability (Ganguli et al., 2006; Wilkins et al., 2009; Kohler et al., 2010b;) and/or greater whole brain atrophy (Chang et al., 2011; Ribeiz et al., 2013; Sexton et al., 2013) indicated LLD. Possibly explaining the discrepancies between past studies, age (Luppa et al., 2012; Wild et al., 2012; Wu et al., 2012; Forlani et al., 2013;) and global WMH burden (Greenwald et al., 1998; Gunning-Dixon et al., 2010; Teodorczuk et al., 2010; Aizenstein et al., 2011; Firbank et al., 2012) were fully dependent on the other measures in regard to their association with LLD diagnosis. We speculate that the primary role of non-imaging measures in predicting diagnosis suggests that current neuroimaging methods cannot—yet—capture the neural complexity associated with the etiopathogenesis of LLD. The involvement of structure-related neural biomarkers (global atrophy and WM burden) in diagnosing LLD supports past studies that suggest vascular and atrophic changes trigger mood disorder in late life (Aizenstein et al., 2014).

Contrarily, for LLD treatment response, connectivity-based imaging measures were found to be the optimal biomarkers. Specifically, lower structural connectivity—supported by the more recent of the two (Taylor et al., 2008) contradicting past findings (Alexopoulos et al., 2008; Taylor et al., 2008)—and lower functional connectivity—supporting compensation theories (Stern, 2003)—indicate a greater probability of treatment remission. This dependency of LLD treatment response on global network health (i.e. communication strength between network regions) may serve as a biomarker for future personalized care studies. A potential interdependence between biomarkers may explain the contradictions in results between past studies and the heterogeneity in the pathophysiology of LLD patients suggested by Taylor et al, (2008).

Overall, the mix of features predictive of diagnosis likely reflects that LLD is heterogeneous. Our observation that these particular features were not predictive of treatment response suggests that there may be a more proximal mediator of depression recovery, and perhaps the features reflecting LLD heterogeneity lead to a set of global network changes (indexed by rs-fMRI and DTI). It is intriguing that these global network biomarkers were identified as most predictive of treatment response.

Mid-life versus late-life depression prediction models
Unlike past studies of depression in younger populations involving prediction models, this is the first study to accurately model both diagnosis and treatment response using the same approach. While past studies have used a single imaging modality and region-based approach (Fu et al., 2008; Marquand et al., 2008; Costafreda et al., 2009; Hahn et al., 2011; Nouretdinov et al., 2011; Mwangi et al., 2012a; Liu et al., 2012b; Mwangi et al., 2012b; Zeng et al., 2012), we used a multi-modal imaging with whole brain and network-based approach that also included non-imaging measures. Our results may suggest that biomarkers of disease diagnosis and remission possibly differ on the basis of brain structure and function—that is, the different representations of MRI modalities—as opposed to brain regions. It is possible that regional changes do not fully reflect the underlying neural vulnerabilities associated with LLD. This is supported by recent studies (Ajilore et al., 2014; Tadayonnejad and Ajilore, 2014) that describe associations of global brain networks alterations with LLD.

Past prediction model studies of mid-life depression diagnosis have shown accurate classifications can be obtained using functional (Fu et al., 2008; Marquand et al., 2008; Hahn et al., 2011; Nouretdinov et al., 2011; Zeng et al., 2012) or structural (Costafreda et al., 2009; Mwangi et al., 2012a; Mwangi et al., 2012b) imaging. Our study in LLD found structural volume-based measures in conjunction to non-imaging measures to be better predictors. We speculate that these differences in prediction factors may suggest that LLD diagnosis is primarily related to impaired structure (gray matter and WM), whereas midlife depression may stem from aberrant communication/activation of various brain regions. This hypothesis will require further testing.

Past prediction model studies of mid-life depression treatment response have primarily utilized T1-weighted Hi-Res structural imaging measures (Costafreda et al., 2009; Nouretdinov et al., 2011; Liu et al., 2012b). One study (Marquand et al., 2008) that attempted to use a task-based functional imaging measure did not achieve very high accuracy. Our study in LLD found structural and functional connectivity measures to be better predictors. Because connectivity-related imaging measures have not been tested for prediction models of mid-life depression treatment response, it is difficult to draw any conclusions.

Learning methods
Based on our findings, modified versions of decision tree and logistic regression are potential alternative learning methods—to the traditionally used SVM—for prediction depression, particularly in late-life diagnosis and treatment response. Modified decision tree methods with embedded feature selection capabilities, especially, may be useful for studying real-world nonlinear relationships in high-dimensional (i.e. large number of features) data.

Limitations and future work
Limitations to this study include small subsample size for treatment response prediction (nevertheless, the results were cross checked using four different learning methods), varying sample sizes for the different network analysis (this prevents us from accurately analyzing network-based effects and may be causing feature set 13 results to vary across networks despite its network-independent features), higher percentage of women (reflecting the naturalistic gender distribution in LLD (Luppa et al., 2012)), and lack of other potential cognitive ability measures and covariates (due to limited available data). Another limitation is the heterogeneous treatment. However, this may not have affected our LLD treatment response prediction results because all administered antidepressants are either selective serotonin reuptake inhibitors or serotonin-norepinephrine reuptake inhibitors, and the efficacy difference between the two is still a matter of debate (Taylor and Doraiswamy, 2004; Taylor et al., 2006; Papakostas et al., 2007; Thase et al., 2011). Future work includes extensive studies verifying, improving as necessary, and testing the real-world applicability of the optimal prediction models found in our study. It would also be beneficial to test other imaging and non-imaging (e.g. cognitive ability, medical comorbidities, and covariates) measures as potential biomarkers. Potential clinical applications may include using machine learning and imaging to predict treatment efficacy and recommend personalized treatment for LLD. While diagnosis of LLD is not a potential application, the prediction model for LLD diagnosis can help us gain a better understanding of LLD and consequently lead to a better model for predicting treatment response."
Predicting depression in community dwellers using a machine learning algorithm,"Depression is one of the leading causes of disability worldwide. Given the socioeconomic burden of depression, appropriate depression screening for community dwellers is necessary. We used data from the 2014 and 2016 Korea National Health and Nutrition Examination Surveys. The 2014 dataset was used as a training set, whereas the 2016 dataset was used as the hold-out test set. The synthetic minority oversampling technique (SMOTE) was used to control for class imbalances between the depression and non-depression groups in the 2014 dataset. The least absolute shrinkage and selection operator (LASSO) was used for feature reduction and classifiers in the final model. Data obtained from 9488 participants were used for the machine learning process. The depression group had poorer socioeconomic, health, functional, and biological measures than the non-depression group. From the initial 37 variables, 13 were selected using LASSO. All performance measures were calculated based on the raw 2016 dataset without the SMOTE. The area under the receiver operating characteristic curve and overall accuracy in the hold-out test set were 0.903 and 0.828, respectively. Perceived stress had the strongest influence on the classifying model for depression. LASSO can be practically applied for depression screening of community dwellers with a few variables. Future studies are needed to develop a more efficient and accurate classification model for depression.","We built a machine learning-based model for predicting future depression. The AUC (0.903), overall accuracy (0.828), sensitivity (0.828), and specificity (0.828) showed that this model could be practically used for screening community-dwelling individuals who may develop depression.
In the final set of variables, perceived stress was the strongest predictor of depression. Stress is generally categorized as either eustress or distress. Eustress represents positive aspects of stress, whereas distress refers to its negative aspects. Perceived stress measures distress by using questions such as “In the last month, how often have you felt nervous and stressed?” The negative effects of stress have a well-documented relationship with the pathophysiology of psychiatric disorders, such as depression [32,33]. As most screening instruments for depression do not contain the term “stress,” perceived stress should be included in screenings of community-dwelling individuals. Moreover, subjective health was ranked as the second most predictive variable for classifying depression. The concept of subjective health reflects the quality of life or well-being [34,35]. Subjective health plays an important role in the pathophysiology of depression [36]. Although depression might contribute to perceived stress and poor subjective health, these factors should be considered important for the early detection of depression.
Our study had several strengths. First, we built a model to classify depression among community dwellers. Although depression causes substantial disability, the treatment of clinical depression is difficult [13]. Hence, early screening and detection of depression among community dwellers are particularly important, and many countries have focused on screening for depression in community settings before the clinical stages of the disease [37,38]. Thus, we believe our model could be practically used in community mental health institutions for accurate and prompt screening of depression.
Second, we used various types of variables. As depression is based on a complex interaction among biopsychosocial variables [39,40,41], clinicians must utilize the possible correlates of depression to improve classification. We included peripheral biomarkers (e.g., thyroid hormone, hemoglobin, white blood cells, platelets, aspartate aminotransferase, and alanine aminotransferase), psychosocial functioning (e.g., EQ-5D), and sociodemographic variables (e.g., age, sex, marital status, educational level, and economic status) to classify depression.
Third, we used LASSO to reduce features and build a final model to classify depression. We found that a model with fewer variables resulted in a performance comparable to one with more variables. We believe that practicality is necessary for such a machine learning model, and from a practical perspective, a questionnaire with too many questions might not be suitable for use in routine screening settings. If the performance between the two models is not substantially different, one with fewer variables could be practically used with the benefits of a short screening time and effort. As we developed this model for use in community health institutions, rather than higher-level facilities, we presumed that low computing costs with fewer variables are an important point. The reasonable computing costs of LASSO facilitate its deployment in community health institutions.
Fourth, it is noteworthy to discuss why we used the 2014 dataset for the training set and the 2016 dataset for the test set, rather than randomly selecting training and test sets. First, we wanted to test whether the algorithm made with past data (i.e., the 2014 dataset) could be applied to future data (i.e., the 2016 dataset). There will be some changes in the frequency or severity of the variables by reflecting the number of times the dataset was collected. If an algorithm should be useful in the real world over time, it should be robust for future data. In addition, there were statistical differences in many of the variables between the 2014 and 2016 datasets, whereas there was no statistical difference in the severity of depression between the two datasets. We interpreted the results mainly in terms of sample size and standard deviation. Generally, as the total sample size increases, the p-value decreases [42]. As the sample size was large (n = 9488), negligible differences were statistically significant (p < 0.05). Moreover, as the standard deviation (i.e., the degree of spread) increases, the p-value increases [43]; thus, the non-significant statistical difference in the severity of depression (i.e., PHQ score) resulted from a high standard deviation. As the participants of this study were from the general population, the distribution of the PHQ score would be severely positively skewed, which is associated with a high standard deviation.
This study had several limitations. First, although we included biopsychosocial factors for depression, neuroimaging and genetic variables were not available. Neuroimaging markers, such as structural volumes and functional activity, have long been used to classify depression [44,45]. Genetic studies have also provided information for understanding and classifying depression [4]. As this study sought to create a prompt and accurate tool to classify depression, such expensive tests do not seem applicable for a screening test. Nonetheless, we should consider whether biological factors are, indeed, helpful for discriminating depression. For example, a previous study revealed that the singular use of biomarkers to predict depression prognosis resulted in a poor performance (AUC < 0.6) [46]. The small effects of biological factors were confirmed in our study; only blood urea nitrogen was included in the final model throughout LASSO. Second, due to the limited sample size, we could not subdivide the study population by age group (e.g., youth, middle-aged adults, and older adults); instead, we grouped all ages to build a machine learning model. Given the different contributors to depression across different age groups [47,48], future studies with larger sample sizes are needed. Third, the survey data may not sufficiently reflect respondents’ interpersonal relationships. For example, a recent study revealed that Facebook entries predicted future clinical depression [49]. Although the sample size was small (n = 683), and the outcome measure was only moderately predictive (AUC = 0.69 to 0.72), such an approach should be used to supplement future surveys and help construct a more comprehensive dataset.
In summary, we successfully built a model for classifying depression using the LASSO algorithm and sociodemographic, psychosocial, and laboratory data obtained from community dwellers. We believe that this model may help improve the accuracy of depression screening among community-dwelling individuals."
Predictive modeling of depression and anxiety using electronic health records and a novel machine learning approach with artificial intelligence,"Generalized anxiety disorder (GAD) and major depressive disorder (MDD) are highly prevalent and impairing problems, but frequently go undetected, leading to substantial treatment delays. Electronic health records (EHRs) collect a great deal of biometric markers and patient characteristics that could foster the detection of GAD and MDD in primary care settings. We approached the problem of predicting MDD and GAD using a novel machine learning pipeline to re-analyze data from an observational study. The pipeline constitutes an ensemble of algorithmically distinct machine learning methods, including deep learning. A sample of 4,184 undergraduate students completed the study, undergoing a general health screening and completing a psychiatric assessment for MDD and GAD. After explicitly excluding all psychiatric information, 59 biomedical and demographic features from the general health survey in addition to a set of engineered features were used for model training. We assessed the model's performance on a held-out test set and found an AUC of 0.73 (sensitivity: 0.66, specificity: 0.7) and 0.67 (sensitivity: 0.55, specificity: 0.7) for GAD, and MDD, respectively. Additionally, we used advanced techniques (SHAP values) to illuminate which features had the greatest impact on prediction for each disease. The top predictive features for MDD were being satisfied with living conditions and having public health insurance. The top predictive features for GAD were vaccinations being up to date and marijuana use. Our results indicate moderate predictive performance for the application of machine learning methods in detection of GAD and MDD based on EHR data. By identifying important predictors of GAD and MDD, these results may be used in future research to aid in the early detection of MDD and GAD.","Our objective was to evaluate the importance and effectiveness of standard clinical data on the prediction of MDD or GAD. We used state-of-the-art novel machine learning methodologies to make predictions. Additionally, SHAP values were generated to explain and clinically validate our findings. We trained our model with > 2500 participants and assessed the model's performance on a held-out test set. Although our accuracy metrics are comparable to previous studies predicting psychiatric outcomes, ours is unique in its primary reliance on routine biomedical and demographic features, rather than features with a known correlation to psychiatric outcomes. Previous studies that have looked at EHR to detect MDD have had the significant limitation of including predictive variables that would nullify the clinical utility of the model by relying on features that are directly indicative of known psychiatric illness (e.g. including psychiatric billing codes, which are based upon clinician diagnosis). Thus, this study is the first known study to predict MDD and GAD using EHR data with potential for predictive validity in detecting unknown psychiatric diagnoses.

Studies using magnetic resonance imaging (MRI) have been able to achieve slightly higher predictive performances ranging from 67 to 94%32. Nevertheless, perhaps due to the considerable expense of collecting MRI data, a common limitation of these was their small sample sizes. These studies also had considerable range in performance, and the due to their small sample sizes the results are highly inconsistent33. Moreover, using MRI to predict MDD is unrealistic when there is no other reason to justify an MRI, especially in an otherwise physically healthy college-age patient.

In addition to the complex machine learning approach and our carefully curated feature set, we are providing insights to the complex clinical appearance of MDD. Our pipeline, using SHAP values to visualize feature importance, provides not only the outcome prediction but the possible characteristics that a physician can identify when making a decision. These characteristics including mean arterial pressure, blood pressure, markers for low SES and general health markers have been shown to be previously associated with depression and anxiety34,35. Of note, despite the potential for these relationships between indicators of low SES and MDD and GAD, this study cannot rule out that this was based on biased physician diagnoses.

In further investigation of the predictors for generalized anxiety disorder, vaccination status may be reflective of overall poorer health outcomes in individuals with GAD36. Regarding the “marijuana use”, prior research demonstrates high comorbidity between anxiety disorders and substance use disorders37. With regard to the most important features driving major depressive disorder, there is research supporting overall poorer life satisfaction in individuals with MDD38, which may certainly include dissatisfaction with living conditions. Low interest and energy, DSM criteria for MDD, may contribute to difficulties maintaining satisfactory living conditions. Robust research to date indicates that individuals of lower socioeconomic status are more likely to have MDD39. “Difficulty memorizing lessons” may be related to concentration difficulties, also identified by the DSM as a clinical feature of MDD. An additional top predictive feature for both MDD and GAD is hypertension. Research to date corroborates this finding by demonstrating that individuals with either MDD or GAD are more likely to have hypertension40,41.

This information has the potential to allow health care providers to make informed recommendations for further screening regardless of whether the patient discusses or even recognizes his or her symptoms. This is important because as previously mentioned, it can take on average 6 or 14 years from onset of illness until diagnosis for MDD and GAD respectively17. Our study is one of the first of its kind to tackle this issue by not relying on previous psychiatric diagnoses or expensive imagine techniques to capture the disease in an early stage.

This study has several important limitations which deserve mention. One is that the original screening for the outcomes of MDD and GAD may not have captured all cases within the population. This, in addition to the study population, limits the generalizability of the results. Our dataset comes from French college aged students, who likely have baseline differences from other populations with psychiatric illness. Despite this limitation, our study still serves to show the predictive ability of mainly non-psychiatric variables for psychiatric illness. Such variables, further analyzed individually for their connection to psychiatric pathology, may prove the basis of further research. Another limitation of our study, which is fairly ubiquitous in mental health research is the low prevalence of anxiety and depression in our study population, as well as our sample size. Although this is a limitation in many studies of psychiatric nature, we were able to enhance our predictive power using a stacked ensemble model pipeline. Additionally, the lack of qualitative information (i.e., severity, subtype, etc.) regarding mental health diagnoses was not available to allow for a severity prediction analysis. Thus, future research should examine the potential for these important predictor to predict severity and subtype of MDD and GAD.

This research is an important step in the direction towards identifying potentially difficult to diagnose illnesses with readily available and easy to obtain information. Our tool, using an optimal sensitivity/specificity split would be able to capture two out of every three subjects with GAD and one out of two MDD cases while only incurring a 30% false positive rate. Because there are detrimental outcomes to both the patient and provider in a false positive, looking at the efficacy of case identification while requiring 70% specificity gives a reasonable idea of how many cases would be captured if this model were to be deployed in a clinical setting. These findings have shown promise on multiple fronts: Ability to use easy to obtain information to inform possible detection of MDD and GAD, further understanding of the demographic and biological characteristics associated with illness, and both the success and necessity for computational tools to inform psychological medicine. We believe, given a larger and more heterogeneous sample, this modeling technique could be used to elucidate the drivers of psychological illness and provide a tool that indicates the necessity of treatment with high precision and accuracy.
"
Prediction of depression among senior citizens using machine learning classifiers,"Depression is one of the leading causes of disability worldwide. Given the socioeconomic burden of depression, appropriate depression screening for community dwellers is necessary. We used data from the 2014 and 2016 Korea National Health and Nutrition Examination Surveys. The 2014 dataset was used as a training set, whereas the 2016 dataset was used as the hold-out test set. The synthetic minority oversampling technique (SMOTE) was used to control for class imbalances between the depression and non-depression groups in the 2014 dataset. The least absolute shrinkage and selection operator (LASSO) was used for feature reduction and classifiers in the final model. Data obtained from 9488 participants were used for the machine learning process. The depression group had poorer socioeconomic, health, functional, and biological measures than the non-depression group. From the initial 37 variables, 13 were selected using LASSO. All performance measures were calculated based on the raw 2016 dataset without the SMOTE. The area under the receiver operating characteristic curve and overall accuracy in the hold-out test set were 0.903 and 0.828, respectively. Perceived stress had the strongest influence on the classifying model for depression. LASSO can be practically applied for depression screening of community dwellers with a few variables. Future studies are needed to develop a more efficient and accurate classification model for depression.","We built a machine learning-based model for predicting future depression. The AUC (0.903), overall accuracy (0.828), sensitivity (0.828), and specificity (0.828) showed that this model could be practically used for screening community-dwelling individuals who may develop depression.
In the final set of variables, perceived stress was the strongest predictor of depression. Stress is generally categorized as either eustress or distress. Eustress represents positive aspects of stress, whereas distress refers to its negative aspects. Perceived stress measures distress by using questions such as “In the last month, how often have you felt nervous and stressed?” The negative effects of stress have a well-documented relationship with the pathophysiology of psychiatric disorders, such as depression [32,33]. As most screening instruments for depression do not contain the term “stress,” perceived stress should be included in screenings of community-dwelling individuals. Moreover, subjective health was ranked as the second most predictive variable for classifying depression. The concept of subjective health reflects the quality of life or well-being [34,35]. Subjective health plays an important role in the pathophysiology of depression [36]. Although depression might contribute to perceived stress and poor subjective health, these factors should be considered important for the early detection of depression.
Our study had several strengths. First, we built a model to classify depression among community dwellers. Although depression causes substantial disability, the treatment of clinical depression is difficult [13]. Hence, early screening and detection of depression among community dwellers are particularly important, and many countries have focused on screening for depression in community settings before the clinical stages of the disease [37,38]. Thus, we believe our model could be practically used in community mental health institutions for accurate and prompt screening of depression.
Second, we used various types of variables. As depression is based on a complex interaction among biopsychosocial variables [39,40,41], clinicians must utilize the possible correlates of depression to improve classification. We included peripheral biomarkers (e.g., thyroid hormone, hemoglobin, white blood cells, platelets, aspartate aminotransferase, and alanine aminotransferase), psychosocial functioning (e.g., EQ-5D), and sociodemographic variables (e.g., age, sex, marital status, educational level, and economic status) to classify depression.
Third, we used LASSO to reduce features and build a final model to classify depression. We found that a model with fewer variables resulted in a performance comparable to one with more variables. We believe that practicality is necessary for such a machine learning model, and from a practical perspective, a questionnaire with too many questions might not be suitable for use in routine screening settings. If the performance between the two models is not substantially different, one with fewer variables could be practically used with the benefits of a short screening time and effort. As we developed this model for use in community health institutions, rather than higher-level facilities, we presumed that low computing costs with fewer variables are an important point. The reasonable computing costs of LASSO facilitate its deployment in community health institutions.
Fourth, it is noteworthy to discuss why we used the 2014 dataset for the training set and the 2016 dataset for the test set, rather than randomly selecting training and test sets. First, we wanted to test whether the algorithm made with past data (i.e., the 2014 dataset) could be applied to future data (i.e., the 2016 dataset). There will be some changes in the frequency or severity of the variables by reflecting the number of times the dataset was collected. If an algorithm should be useful in the real world over time, it should be robust for future data. In addition, there were statistical differences in many of the variables between the 2014 and 2016 datasets, whereas there was no statistical difference in the severity of depression between the two datasets. We interpreted the results mainly in terms of sample size and standard deviation. Generally, as the total sample size increases, the p-value decreases [42]. As the sample size was large (n = 9488), negligible differences were statistically significant (p < 0.05). Moreover, as the standard deviation (i.e., the degree of spread) increases, the p-value increases [43]; thus, the non-significant statistical difference in the severity of depression (i.e., PHQ score) resulted from a high standard deviation. As the participants of this study were from the general population, the distribution of the PHQ score would be severely positively skewed, which is associated with a high standard deviation.
This study had several limitations. First, although we included biopsychosocial factors for depression, neuroimaging and genetic variables were not available. Neuroimaging markers, such as structural volumes and functional activity, have long been used to classify depression [44,45]. Genetic studies have also provided information for understanding and classifying depression [4]. As this study sought to create a prompt and accurate tool to classify depression, such expensive tests do not seem applicable for a screening test. Nonetheless, we should consider whether biological factors are, indeed, helpful for discriminating depression. For example, a previous study revealed that the singular use of biomarkers to predict depression prognosis resulted in a poor performance (AUC < 0.6) [46]. The small effects of biological factors were confirmed in our study; only blood urea nitrogen was included in the final model throughout LASSO. Second, due to the limited sample size, we could not subdivide the study population by age group (e.g., youth, middle-aged adults, and older adults); instead, we grouped all ages to build a machine learning model. Given the different contributors to depression across different age groups [47,48], future studies with larger sample sizes are needed. Third, the survey data may not sufficiently reflect respondents’ interpersonal relationships. For example, a recent study revealed that Facebook entries predicted future clinical depression [49]. Although the sample size was small (n = 683), and the outcome measure was only moderately predictive (AUC = 0.69 to 0.72), such an approach should be used to supplement future surveys and help construct a more comprehensive dataset.
In summary, we successfully built a model for classifying depression using the LASSO algorithm and sociodemographic, psychosocial, and laboratory data obtained from community dwellers. We believe that this model may help improve the accuracy of depression screening among community-dwelling individuals.
"
A textual-based featuring approach for depression detection using machine learning classifiers and social media texts,"Depression is one of the leading causes of suicide worldwide. However, a large percentage of cases of depression go undiagnosed and, thus, untreated. Previous studies have found that messages posted by individuals with major depressive disorder on social media platforms can be analysed to predict if they are suffering, or likely to suffer, from depression. This study aims to determine whether machine learning could be effectively used to detect signs of depression in social media users by analysing their social media posts—especially when those messages do not explicitly contain specific keywords such as ‘depression’ or ‘diagnosis’. To this end, we investigate several text preprocessing and textual-based featuring methods along with machine learning classifiers, including single and ensemble models, to propose a generalised approach for depression detection using social media texts. We first use two public, labelled Twitter datasets to train and test the machine learning models, and then another three non-Twitter depression-class-only datasets (sourced from Facebook, Reddit, and an electronic diary) to test the performance of our trained models against other social media sources. Experimental results indicate that the proposed approach is able to effectively detect depression via social media texts even when the training datasets do not contain specific keywords (such as ‘depression’ and ‘diagnose’), as well as when unrelated datasets are used for testing.

","Depression is the most prevalent mental disorder and the main cause behind more than two-thirds of suicides every year. Unfortunately, many cases go untreated because of failure to detect or self-denial. Several studies agreed that, given the exponential increase in social media usage, social media messages can be used as a valuable source for monitoring several mental health issues, including depression.

In this paper, we demonstrated that our generalised approach using ML methods and social media texts can be effectively used to detect signs of depression. Our ML models proved effective even when trained with texts that did not contain the words ‘depression’ or ‘diagnosis’—social media messages by those suffering from depression rarely include such words. It is important to note that the approach presented in this paper performed well even when tested on datasets that were unrelated to the training datasets. This contrasts with most studies in the literature that use portions of the same dataset for training and testing. Our results also indicated that using less strictly constructed datasets can be more beneficial than more strictly constructed datasets, especially when the models would be used for detecting depression in messages from a variety of sources.

Finally, it should be noted that the approach presented in this paper uses supervised ML classifiers, and therefore, the approach is limited to using labelled datasets for training the classifiers. Overcoming this limitation by including unsupervised classifiers is a potential future research area.

An additional observation is that, in a heavily imbalanced dataset, dynamic sampling could increase the accuracy of the less populous class but, at the same time, decrease the accuracy of the more populous class. However, this can be beneficial if the goal is to detect depression and the less populous class is the depression class (which is the case in this study)."
Prediction of poststroke depression based on the outcomes of machine learning algorithms,"Poststroke depression (PSD) is a major psychiatric disorder that develops after stroke; however, whether PSD treatment improves cognitive and functional impairments is not clearly understood. We reviewed data from 31 subjects with PSD and 34 age-matched controls without PSD; all subjects underwent neurological, cognitive, and functional assessments, including the National Institutes of Health Stroke Scale (NIHSS), the Korean version of the Mini-Mental Status Examination (K-MMSE), computerized neurocognitive test (CNT), the Korean version of the Modified Barthel Index (K-MBI), and functional independence measure (FIM) at admission to the rehabilitation unit in the subacute stage following stroke and 4 weeks after initial assessments. Machine learning methods, such as support vector machine, k-nearest neighbors, random forest, voting ensemble models, and statistical analysis using logistic regression were performed. PSD was successfully predicted using a support vector machine with a radial basis function kernel function (area under curve (AUC) = 0.711, accuracy = 0.700). PSD prognoses could be predicted using a support vector machine linear algorithm (AUC = 0.830, accuracy = 0.771). The statistical method did not have a better AUC than that of machine learning algorithms. We concluded that the occurrence and prognosis of PSD in stroke patients can be predicted effectively based on patients’ cognitive and functional statuses using machine learning algorithms.","In this study, we found that numerous cognitive and functional statuses were associated with the occurrence of PSD in stroke patients (Table 5 and Table 6). The recall and auditory attention omission error among the subitems of the MMSE and CNT, and dressing and locomotor function including ambulation among the subitems of the K-MBI and FIM were considered to be important features to predict the occurrence of PSD ((A) in Table 8). From decision-making models, we found that initial scores of visual and auditory attention were important for the prediction of PSD occurrence. A previous study revealed that the severity of depression and the decrease in visual and auditory tension tend to correlate to a weak degree in PSD patients [35], and depressive patients also showed impaired visual attention omission errors in a meta-analysis [36].
PSD is frequently seen in stroke patients, and it might worsen their cognitive and functional recovery and quality of life [37]. The prevention of PSD has been suggested in previous studies using various nonpharmacological modalities and antidepressants [38]; however, the evidence is very limited, and more clinical trials are needed to confirm effective prevention methods for PSD [39]. There is no doubt that early detection and treatment of PSD can help improve a patient’s prognosis; therefore, it is important to identify modifiable risk factors and their application to stroke patients. From previous studies, major risk factors from meta-analysis still debated according to researchers were identified as follows: previous history of mental disorders, including depression or anxiety; diabetes mellitus; cognitive impairment and functional deficits, including impairment in ADL; and other factors, such as old age, female sex, lesion location, and stroke type [40,41]. It should be noted here that risk factors, which are determined by statistical methods, cannot be used to develop a predictive model directly because risk factors contain results, course, or complications of target diseases, as well as causative factors and can be used as an explanatory model [42].
ML methods are a useful tool to overcome the limitations of statistical methods and help us to find predictive models [43]. Most previous ML studies on stroke patients have focused on the prediction of stroke occurrence or outcome [44], and only one study has revealed the relationship between stroke and mood disorders, including depression, apathy, and anxiety, using ML analysis [20]. In this study, we used various ML algorithms to predict PSD occurrence and prognosis for the first time, and the SVM linear and SVM with an RBF kernel were optimal to develop a predictive model for PSD. The RBF kernel is commonly used in SVM classification, has the advantages of the KNN algorithm, and overcomes challenges of using the RBF alone, such as the space complexity problem [45]. We also tried to apply other ML algorithms, including KNNs and RFs, which were effective in predicting stroke occurrence and prognosis in previous studies [44]. The VE algorithm is the combination of all ML models that were used in this study to improve model performance; however, its AUC and accuracy were not higher than those of the SVM linear or SVM with the RBF kernel (Table 9). The VE model uses multiple models for the analysis, which might contain pros and cons of each model, and individual algorithms, which are superior to other algorithms, can show better performance.
Whereas cognitive impairment and PSD are highly connected with each other, the effect of PSD treatment on cognitive improvement was not clear in previous studies. In one study, improved PSD patients showed greater cognitive improvement than nonimproved PSD patients showed [46], but in another study, PSD treatment only helped to improve attention [47]. Previous studies also revealed that motor recovery is obvious in treated PSD patients [48]. In our study, cognitive impairments such as visual memory visual span backward of the CNT, were closely related to improvements in patients with PSD ((B) in Table 8); however, further studies are needed to further confirm these relationships. We found that the educational period was strongly associated with recovery from PSD in the logistic regression analysis (AUC = 0.80). In previous studies, educational level might be associated with anxiety, depression [49], and poststroke cognitive impairment [50] and be a protective factor against PSD [51,52].
The statistical method of logistic regression analysis did not show a higher AUC than the optimal ML algorithm for the prediction of PSD occurrence and prognosis (Figure 3). During statistical processing of logistic regression, only a few parameters were included for the comparison of participants in the control and PSD groups or the Imp and NoImp groups, whereas ML algorithms included five parameters for the detection of AUC and accuracy. The sample size was important to reduce bias in regression coefficients [53], and a smaller sample size might have influenced the low AUC and accuracy of the regression analysis in this study. Nevertheless, ML algorithms in this study showed comparable performance to the existing statistical methods and might have been a suitable method to overcome a small sample size.
The SVM linear and SVM with RBF showed the best AUC among various ML algorithms for the prediction of PSD occurrence and prognosis; however, the specificity for the prediction of PSD occurrence and the sensitivity for the prediction of PSD prognosis was low, which means that these models might not accurately detect PSD among stroke patients or PSD patients whose symptoms improved depending on the cut-off selected. We studied 65 stroke patients, including controls without PSD, for the development of ML algorithm-based prediction models of PSD occurrence and prognosis; notably, many previous studies could not evaluate more than 60 stroke patients [16,54,55]. Small sample sizes can cause some problems, such as generalization, in the field of ML. In the medical field, the small sample size is due to an imbalance in which the number of people with the disease is smaller than the number of people without the disease. This imbalance problem can be addressed by introducing a method of oversampling relatively small data [56]. In this study, we performed a 5- and 10-fold stratified cross-validation set to compensate for the small sample size [57]. Although there are differences according to the ML algorithms, no significant difference was found between the two methods for the prediction of PSD occurrence and prognosis (Table 9). A prospective study can provide more accurate clinical information; therefore, it is possible to develop better predictive models. Pertinently, some prospective studies have been designed to predict PSD, but they were not replicable in an independent stroke group [6].
In this study, we used various ML algorithms to predict PSD occurrence and prognosis for the first time and showed better performance than the statistical method. However, further studies with larger sample sizes and longer follow-up periods are needed to ascertain applications for clinical use.
"
Predicting depression in Bangladeshi undergraduates using machine learning,"Depression is a major disorder and a growing problem that impacts a person's way of living and disrupts natural functioning. Depression is especially prevalent in the younger population of underdeveloped and developing countries. Youth in countries such as Bangladesh face difficulties with studies, jobs, relationships, drugs, family problems which are all major or minor contributors in a pathway to depression. This research besides predicting depression in university undergraduates for the purpose of recommendation to a psychiatrist focuses on gaining valuable insights as to why university students of Bangladesh, undergraduates, in particular suffer from depression. The data for this research was collected by a survey designed after consultation with psychologists, counselors and professors. The best method for predicting depression among Bangladesh undergraduates was found out after using three algorithms to train and test the dataset. Random Forest was found to be the best algorithm, closely followed by Support Vector Machine with similar accuracy and f-measure of around 75% and 60%} respectively but Random Forest giving a better precision, recall and lower false negatives. The objective of this research is to check whether depression can be successfully predicted with the help of related features. This research aims to identify depression in its early stages and ensure a fast recovery for victims so that heartbreaking incidents like suicide can be avoided.","The most prevalent mental disorder or illness, depression is a broad area of research that has a lot of implications in the medical domain and psychology. Young people are more likely to suffer from this disease especially residents of lower-middle-income countries due to various socio-demographic reasons. Depression has its roots deeply ingrained in the lifestyle, habits and behavior of people. This research sought to exploit this relation by using some social and personal data in order to predict depression in individuals. We have found that depression can be successfully predicted in this way using the Random Forest algorithm, with an accuracy of 75% and a fair F-measure of 60%. Currently, we are using a larger data set and running more algorithms to see if we can get better accuracy and lower false negatives. Moreover, we are also working on finding out the optimal features and using them to predict depression faster and more accurately. These findings will be presented in another paper later on.

There has not been much work on analyzing depression among Bangladesh citizens. One of the main objectives of this study was to identify early cases of depression and ask individuals at risk of developing depression to consult a psychiatrist or consultant. Also, professionals can use our system to identify the causes of depression in specific individuals. The need for identifying depression early is dire as the disorder can worsen very quickly. The proposed model could be used by psychologists, counselors, universities to find out the depression in students so that appropriate steps can be taken to reduce the effect and impact of the disease and help young people live a healthier life, a happier life."